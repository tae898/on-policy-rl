{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d811b2d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Fundamentals\n",
    "\n",
    "## üéØ The Core RL Problem\n",
    "\n",
    "**Reinforcement Learning** is fundamentally about learning a **policy** $\\pi$ that maximizes the **expected cumulative discounted reward**:\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ...)$ is a **trajectory** (sequence of states, actions, rewards)\n",
    "- $\\gamma \\in [0,1]$ is the **discount factor** (how much we value future rewards)\n",
    "- $R_t$ is the reward at time step $t$\n",
    "- $\\pi$ is our **policy** (strategy for choosing actions)\n",
    "\n",
    "## üîÑ Markov Decision Process (MDP) Assumption\n",
    "\n",
    "To make RL tractable, we assume the environment follows the **Markov Property**:\n",
    "\n",
    "$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)$$\n",
    "\n",
    "**Translation:** *The future depends only on the present state and action, not the entire history.*\n",
    "\n",
    "### Why This Assumption Matters\n",
    "\n",
    "**Without the Markov assumption** (Partially Observable MDPs - POMDPs):\n",
    "- We need to maintain **belief states** over possible true states\n",
    "- The agent must learn to **infer hidden information** from observation history\n",
    "- **Exponentially harder** - need to track probability distributions over states\n",
    "- **Memory becomes crucial** - past observations help disambiguate current state\n",
    "- Examples: Poker (hidden cards), autonomous driving with limited sensors\n",
    "\n",
    "**With the Markov assumption** (Full MDPs):\n",
    "- Current state contains **all relevant information** for decision making\n",
    "- We can use **memoryless policies**: $\\pi(a|s)$ depends only on current state\n",
    "- **Mathematically tractable** - enables dynamic programming approaches\n",
    "- **Most RL algorithms assume this** for computational feasibility\n",
    "\n",
    "### MDP Components\n",
    "\n",
    "An MDP is formally defined as a 5-tuple: $(S, A, P, R, \\gamma)$\n",
    "\n",
    "- **$S$**: State space (all possible states)\n",
    "- **$A$**: Action space (all possible actions)  \n",
    "- **$P(s'|s,a)$**: Transition probabilities (environment dynamics)\n",
    "- **$R(s,a)$**: Reward function (immediate feedback from current state and action)\n",
    "- **$\\gamma$**: Discount factor (future reward weighting)\n",
    "\n",
    "## üé≤ Policy Types\n",
    "\n",
    "### Stochastic vs Deterministic Policies\n",
    "\n",
    "**Stochastic Policy:** $\\pi(a|s) = P(\\text{action } a | \\text{state } s)$\n",
    "- Outputs **probability distribution** over actions\n",
    "- **Natural exploration** through randomness\n",
    "- Examples: Softmax policy, Gaussian policy\n",
    "\n",
    "**Deterministic Policy:** $\\pi(s) = a$\n",
    "- Outputs **single action** for each state  \n",
    "- Requires **external exploration** mechanisms (Œµ-greedy, noise)\n",
    "- Examples: Greedy policy derived from learned functions\n",
    "\n",
    "### Parameterized Policies: The Deep RL Revolution\n",
    "\n",
    "Instead of learning a lookup table, we use **function approximation**:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\text{Neural Network}(s; \\theta)$$\n",
    "\n",
    "Where $\\theta$ are the **learnable parameters** (neural network weights).\n",
    "\n",
    "#### üìö From Classical RL to Deep RL\n",
    "\n",
    "**Classical (Tabular) RL Era** (pre-2010s):\n",
    "- **Policy Representation**: Lookup table $\\pi(s) = a$ for each state\n",
    "- **State Space Requirement**: Must enumerate ALL possible states\n",
    "- **Feasible Environments**: \n",
    "  - GridWorld (10√ó10 = 100 states)\n",
    "  - Tic-Tac-Toe (‚âà3,000 unique states)\n",
    "  - Simple card games\n",
    "- **Fatal Limitation**: **Curse of dimensionality** - exponential growth in state space\n",
    "\n",
    "**Deep RL Era** (2010s-present):\n",
    "- **Policy Representation**: Neural network $\\pi_\\theta(a|s)$ that **generalizes** across states\n",
    "- **State Space Capability**: Can handle **infinite** or astronomically large state spaces\n",
    "- **Feasible Environments**:\n",
    "  - LunarLander: Continuous 8D observation space\n",
    "  - Chess: $‚âà10^{47}$ possible board positions\n",
    "  - Go: $‚âà10^{170}$ possible board positions (more than atoms in observable universe!)\n",
    "\n",
    "#### üöÄ Why Function Approximation Changed Everything\n",
    "\n",
    "**The Generalization Miracle:**\n",
    "- A neural network trained on seeing a lander at position (0.1, 0.5) can **generalize** to position (0.11, 0.51)\n",
    "- **Never seen that exact state before**, but can interpolate from similar experiences\n",
    "- **Enables learning** from finite experience in infinite state spaces\n",
    "\n",
    "## üö´ Regularization in RL: Why Dropout Usually Fails\n",
    "\n",
    "### Dropout: Usually a Bad Idea ‚ùå\n",
    "\n",
    "In supervised learning, dropout is a powerful regularizer that prevents overfitting by randomly zeroing out neurons during training. It works by creating an ensemble of smaller networks, forcing the model to learn more robust features.\n",
    "\n",
    "In Reinforcement Learning, however, this usually backfires:\n",
    "\n",
    "**It Adds More Noise**: The RL learning signal is already incredibly noisy and high-variance. Dropout injects another layer of randomness directly into the policy or value function calculations. This makes the already difficult credit assignment problem even harder, as the agent's output for the exact same state is different every forward pass.\n",
    "\n",
    "**It Creates Inconsistency**: This is especially damaging for off-policy methods. An experience stored in the replay buffer was generated with a specific dropout \"mask\" active. When the agent later learns from that experience, it uses a different random mask. This mismatch between the policy that acted and the policy that learns adds another layer of variance and instability, compounding the existing off-policy challenges.\n",
    "\n",
    "Because RL agents are desperately trying to find a stable signal in a sea of noise, dropout often hurts performance by making the learning target even more chaotic.\n",
    "\n",
    "## üîß Normalization: A Tale of Two Techniques\n",
    "\n",
    "Normalization techniques are crucial for stabilizing deep networks, but their effectiveness in RL depends entirely on how they normalize.\n",
    "\n",
    "### Batch Normalization (BatchNorm): Also a Bad Idea ‚ùå\n",
    "\n",
    "BatchNorm normalizes a layer's inputs based on the mean and variance of the current batch. This is highly effective when data is i.i.d. (independent and identically distributed), as in supervised learning.\n",
    "\n",
    "In RL, this is problematic:\n",
    "\n",
    "- **Correlated Data**: Data within an RL episode is highly correlated, not i.i.d. The statistics of one batch can be wildly different from the next.\n",
    "- **Replay Buffer Issues**: For off-policy learning, the batch statistics depend entirely on what gets randomly sampled from the replay buffer. A batch of early-game states has a different statistical profile than a batch of late-game states. This makes the normalization itself a source of randomness, destabilizing the learning process.\n",
    "\n",
    "BatchNorm's dependence on batch statistics makes it a poor fit for the non-stationary and correlated data streams in RL.\n",
    "\n",
    "### Layer Normalization (LayerNorm): A Great Idea ‚úÖ\n",
    "\n",
    "LayerNorm, in contrast, normalizes a layer's inputs based on the mean and variance calculated across the features within a single training sample. It completely ignores the batch.\n",
    "\n",
    "This makes it perfectly suited for RL:\n",
    "\n",
    "- **Batch-Independent**: Because its calculations are confined to a single sample, it is immune to the problems of small batch sizes, correlated data, and non-stationary replay buffers.\n",
    "- **Deterministic Stability**: It provides a consistent, deterministic form of normalization that stabilizes the activations and gradients inside the network. This helps combat instability without introducing any new randomness.\n",
    "\n",
    "This is why Layer Normalization (and similar batch-independent methods like GroupNorm) is widely and successfully used in modern DRL architectures, while Dropout and BatchNorm are generally avoided.\n",
    "\n",
    "## üèóÔ∏è Our Learning Environment: LunarLander-v3\n",
    "\n",
    "For this entire learning series, we'll use **LunarLander-v3** exclusively. This environment provides the perfect balance of complexity and simplicity for RL education.\n",
    "\n",
    "**Reference**: [Gymnasium Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "### Environment Specifications\n",
    "\n",
    "#### LunarLander-v3 üöÄ\n",
    "**Observation**: 8D vector with position, velocity, angle, angular velocity, leg ground contacts\n",
    "\n",
    "**Action Spaces**:\n",
    "- **Discrete Mode**: 4 actions [do_nothing, fire_left, fire_main, fire_right]\n",
    "- **Continuous Mode**: 2D vector [main_engine, lateral_booster] with bounds [-1.0, +1.0] each\n",
    "\n",
    "**Rewards**: Distance/speed/angle penalties, +10 per leg contact, engine costs, ¬±100 for outcome\n",
    "\n",
    "### Continuous Action Space Bounds\n",
    "\n",
    "**LunarLander-v3 Continuous Actions:**\n",
    "- **Action 0 (Main Engine)**: Range [-1.0, +1.0]\n",
    "  - Negative values: No effect (engine can't push upward)\n",
    "  - 0.0: Engine off\n",
    "  - Positive values: Throttle from 0% to 100%\n",
    "- **Action 1 (Lateral Engine)**: Range [-1.0, +1.0]\n",
    "  - -1.0: Full left thruster\n",
    "  - 0.0: No lateral thrust\n",
    "  - +1.0: Full right thruster\n",
    "\n",
    "### Why LunarLander-v3 for RL Education?\n",
    "\n",
    "- **Dual Action Spaces**: Perfect for testing both discrete and continuous control algorithms\n",
    "- **Vector Observations**: Rich, interpretable 8D state representation perfect for learning\n",
    "- **Realistic Physics**: Consistent Box2D physics provides realistic dynamics\n",
    "- **Educational Value**: Classic trajectory optimization problem\n",
    "- **Fast Feedback**: Clear success/failure conditions with immediate results\n",
    "- **No Visual Complexity**: Focus on algorithms, not CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889153b",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm: Vanilla Policy Gradients\n",
    "\n",
    "## üéØ Algorithm Overview\n",
    "\n",
    "**REINFORCE** (REward Increment = Nonnegative Factor √ó Offset Reinforcement √ó Characteristic Eligibility) is the most fundamental policy gradient algorithm. It directly optimizes the policy parameters using the **policy gradient theorem**.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Pure Monte Carlo Method**: Waits for complete episodes, uses full episode returns\n",
    "- **No Bootstrapping**: Never uses estimates to update estimates (unlike future methods we'll see)\n",
    "- **On-Policy**: Uses experience only from the current policy being learned\n",
    "- **Model-Free**: Doesn't require knowledge of environment dynamics\n",
    "- **Policy-Based Only**: Directly optimizes the policy, no additional function learning\n",
    "\n",
    "## üìê Mathematical Foundation\n",
    "\n",
    "### The Log-Likelihood Trick: From RL to Gradients\n",
    "\n",
    "**üîë Key Challenge**: How do we compute gradients when our objective involves expectations over trajectories that depend on our policy parameters?\n",
    "\n",
    "**Starting Point**: Our objective is to maximize expected return:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\gamma^t r_{t+1}\\right]$$\n",
    "\n",
    "**The Problem**: The expectation is over trajectories $\\tau$ sampled from $\\pi_\\theta$, so the sampling distribution itself depends on $\\theta$!\n",
    "\n",
    "**The Log-Likelihood Trick**: We use the mathematical identity:\n",
    "$$\\nabla_\\theta \\pi_\\theta(\\tau) = \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)$$\n",
    "\n",
    "This allows us to rewrite:\n",
    "$$\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)]$$\n",
    "\n",
    "Where $G(\\tau) = \\sum_{t=0}^{T-1} \\gamma^t r_{t+1}$ is the episode return.\n",
    "\n",
    "### Complete Derivation: From Log-Likelihood to Policy Gradient\n",
    "\n",
    "Now we'll show the complete mathematical derivation step by step.\n",
    "\n",
    "#### Step 1: Expand the Expectation\n",
    "\n",
    "Starting from the log-likelihood trick:\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)]$$\n",
    "\n",
    "Using the definition of expectation:\n",
    "$$= \\nabla_\\theta \\int_{\\tau} \\pi_\\theta(\\tau) G(\\tau) d\\tau$$\n",
    "\n",
    "Applying the product rule and log-likelihood trick:\n",
    "$$= \\int_{\\tau} \\nabla_\\theta \\pi_\\theta(\\tau) G(\\tau) d\\tau$$\n",
    "$$= \\int_{\\tau} \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau) G(\\tau) d\\tau$$\n",
    "$$= \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)]$$\n",
    "\n",
    "#### Step 2: Expand the Trajectory Probability\n",
    "\n",
    "A trajectory $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_{T-1}, a_{T-1}, r_T)$ has probability:\n",
    "\n",
    "$$\\pi_\\theta(\\tau) = p(s_0) \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) P(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "Where:\n",
    "- $p(s_0)$ is the initial state distribution\n",
    "- $\\pi_\\theta(a_t|s_t)$ is our policy (the only part dependent on $\\theta$)\n",
    "- $P(s_{t+1}|s_t, a_t)$ is the environment transition probability\n",
    "\n",
    "#### Step 3: Apply the Logarithm\n",
    "\n",
    "Taking the logarithm of the trajectory probability:\n",
    "$$\\log \\pi_\\theta(\\tau) = \\log p(s_0) + \\sum_{t=0}^{T-1} \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^{T-1} \\log P(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "#### Step 4: Compute the Gradient\n",
    "\n",
    "Taking the gradient with respect to $\\theta$:\n",
    "$$\\nabla_\\theta \\log \\pi_\\theta(\\tau) = \\nabla_\\theta \\log p(s_0) + \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) + \\sum_{t=0}^{T-1} \\nabla_\\theta \\log P(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "**Key Insight**: Only the policy terms depend on $\\theta$! The initial state distribution and environment dynamics are independent of our policy parameters:\n",
    "\n",
    "- $\\nabla_\\theta \\log p(s_0) = 0$ (initial state distribution doesn't depend on policy)\n",
    "- $\\nabla_\\theta \\log P(s_{t+1}|s_t, a_t) = 0$ (environment dynamics don't depend on policy)\n",
    "\n",
    "Therefore:\n",
    "$$\\nabla_\\theta \\log \\pi_\\theta(\\tau) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$$\n",
    "\n",
    "#### Step 5: Substitute Back into the Expectation\n",
    "\n",
    "Substituting back into our expectation:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[G(\\tau) \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right]$$\n",
    "\n",
    "$$= \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)\\right]$$\n",
    "\n",
    "#### Step 6: Refine to Return-to-Go\n",
    "\n",
    "Here's a subtle but important refinement. Instead of using the full episode return $G(\\tau)$ for every time step, we can use the **return-to-go** $G_t$ from time $t$ onwards:\n",
    "\n",
    "$$G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}$$\n",
    "\n",
    "**Why this works**: The policy gradient theorem tells us that the gradient can be computed using the return from time $t$ onwards, not the full episode return. This is because:\n",
    "- Actions at time $t$ cannot affect rewards that occurred before time $t$\n",
    "- The causality structure means only future rewards matter for current actions\n",
    "\n",
    "### Policy Gradient Theorem (Final Form)\n",
    "\n",
    "This gives us the **Policy Gradient Theorem**:\n",
    "\n",
    "$$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ are the policy parameters (neural network weights)\n",
    "- $G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}$ is the **return-to-go** (cumulative future reward from time $t$)\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ is the **score function** (gradient of log-probability)\n",
    "\n",
    "#### Mathematical Intuition: Why This Makes Sense\n",
    "\n",
    "The policy gradient theorem has a beautiful interpretation:\n",
    "\n",
    "$$\\underbrace{\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)}_{\\text{Direction to increase } P(a_t|s_t)} \\cdot \\underbrace{G_t}_{\\text{How good was this action?}}$$\n",
    "\n",
    "**English Translation:** *\"Move the policy parameters in the direction that increases the probability of actions that led to high returns, and decreases the probability of actions that led to low returns.\"*\n",
    "\n",
    "**The Causality Principle**: We use $G_t$ (return from time $t$ onwards) rather than $G(\\tau)$ (full episode return) because:\n",
    "- **Actions affect future, not past**: Action $a_t$ can only influence rewards $r_{t+1}, r_{t+2}, ...$\n",
    "- **Credit assignment**: We should only credit/blame an action for consequences that it could actually influence\n",
    "- **Variance reduction**: Using $G_t$ instead of $G(\\tau)$ reduces variance in gradient estimates\n",
    "\n",
    "### üîç Key Mathematical Insights\n",
    "\n",
    "1. **Model-Free Nature**: The derivation doesn't require knowing $P(s_{t+1}|s_t, a_t)$ - environment dynamics cancel out in the gradient!\n",
    "\n",
    "2. **Score Function**: $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ is the **score function** from statistics - it points in the direction of steepest increase in log-probability.\n",
    "\n",
    "3. **Unbiased Estimator**: The policy gradient theorem gives us an **unbiased estimator** of the true gradient - on average, our sample-based estimates equal the true gradient.\n",
    "\n",
    "4. **High Variance**: While unbiased, the estimator has **high variance** because it depends on noisy episode returns $G_t$.\n",
    "\n",
    "\n",
    "## üîÑ RL vs Supervised Learning: A Striking Similarity\n",
    "\n",
    "### Gradient Structures are Nearly Identical!\n",
    "\n",
    "**Supervised Learning Gradient (Maximum Likelihood Estimation):**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(x,y) \\sim D_{\\text{train}}}\\left[\\nabla_\\theta \\log p(y|x; \\theta)\\right]$$\n",
    "\n",
    "**REINFORCE Gradient:**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "**Key Similarities:**\n",
    "- Both use **log-probability gradients** as the core update mechanism\n",
    "- Both are **expectation-based** optimization procedures  \n",
    "- Both adjust parameters to increase probability of \"good\" outcomes\n",
    "\n",
    "### Understanding the Supervised Learning Foundation\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE) in Supervised Learning:**\n",
    "- **Objective**: Maximize the likelihood of observed data: $\\max_\\theta \\prod_{i=1}^N p(y_i|x_i; \\theta)$\n",
    "- **Equivalent**: Minimize negative log-likelihood: $\\min_\\theta -\\sum_{i=1}^N \\log p(y_i|x_i; \\theta)$\n",
    "- **I.I.D. Assumption**: Training samples $(x_i, y_i)$ are **independent and identically distributed**\n",
    "- **Fixed Target**: Each sample has a known, fixed \"correct\" answer $y_i$\n",
    "\n",
    "**REINFORCE as \"Weighted MLE\":**\n",
    "- **Pseudo-Target**: Actions $a_t$ become \"correct answers\" **weighted by their return** $G_t$\n",
    "- **No Fixed Labels**: Instead of fixed $y_i$, we have actions weighted by how good they turned out to be\n",
    "- **Return as Importance**: $G_t$ tells us \"how much\" we should treat action $a_t$ as correct\n",
    "\n",
    "### The Fundamental Difference: Data Control & Distribution\n",
    "\n",
    "**Supervised Learning:**\n",
    "- Data $(x,y)$ is sampled from **fixed** dataset $D_{\\text{train}}$ \n",
    "- Model has **no control** over which samples it sees\n",
    "- Distribution is **stationary** - same data every epoch\n",
    "- **I.I.D. samples**: Each training example is independent\n",
    "\n",
    "**Reinforcement Learning:**\n",
    "- Data (trajectories) generated by **agent's own policy** $\\pi_\\theta$\n",
    "- Agent **controls** what data it collects through its actions  \n",
    "- Distribution is **non-stationary** - changes as policy improves!\n",
    "- **Sequential dependence**: Each action affects future states and available data\n",
    "\n",
    "### Why This Makes RL Much Harder\n",
    "\n",
    "**The Challenge**: As $\\theta$ changes, so does $\\pi_\\theta$, which changes the data distribution, which affects the gradient estimates! This creates:\n",
    "- **High variance** in gradient estimates\n",
    "- **Non-stationary** learning problem  \n",
    "- **Exploration-exploitation** tradeoffs\n",
    "\n",
    "## üå™Ô∏è The Interconnected Challenges of RL\n",
    "\n",
    "### The Vicious Cycle: Variance ‚Üí Instability ‚Üí Parameter Sensitivity\n",
    "\n",
    "Reinforcement Learning's major challenges are not separate issues but deeply intertwined parts of a single, difficult problem that feeds into itself:\n",
    "\n",
    "**1. High Variance (The Root Cause)**\n",
    "- Random environments, stochastic policies, and sparse rewards create incredibly noisy learning signals\n",
    "- The agent receives inconsistent and contradictory feedback from the same actions\n",
    "- Gradients vary wildly from one step to the next\n",
    "\n",
    "**2. Instability (The Direct Consequence)**  \n",
    "- High variance gradients cause erratic training behavior\n",
    "- Policy improvements oscillate wildly instead of progressing smoothly\n",
    "- Creates the \"moving target\" problem - the agent chases a constantly shifting optimal policy\n",
    "\n",
    "**3. Parameter Sensitivity (The Survival Mechanism)**\n",
    "- Extreme precision required in hyperparameters to control the chaos\n",
    "- Learning rate, discount factor, and exploration parameters become critical stabilizers\n",
    "- Slight parameter mistuning doesn't just slow learning - it causes complete divergence\n",
    "\n",
    "### Why REINFORCE Suffers from All Three\n",
    "\n",
    "**REINFORCE embodies these interconnected challenges:**\n",
    "- **Pure Monte Carlo**: Maximum variance from using full episode returns\n",
    "- **No Variance Reduction**: No baselines or bootstrapping to stabilize learning\n",
    "- **Raw Policy Gradients**: Direct exposure to all the noise in the system\n",
    "\n",
    "### The Evolution: From REINFORCE to PPO\n",
    "\n",
    "This is precisely why the field evolved from REINFORCE to sophisticated algorithms like PPO (Proximal Policy Optimization):\n",
    "\n",
    "**REINFORCE (1992)** ‚Üí **Actor-Critic (2000s)** ‚Üí **PPO (2017)**\n",
    "\n",
    "Each step addresses the interconnected challenges:\n",
    "- **Baselines** reduce variance\n",
    "- **Bootstrapping** stabilizes learning  \n",
    "- **Clipping** prevents destructive updates\n",
    "- **Multiple epochs** improve sample efficiency\n",
    "\n",
    "**Coming Up**: In our next notebooks, we'll see how each algorithm systematically tackles these fundamental RL challenges, building up to PPO's elegant solution that finally makes policy gradient methods stable and practical.\n",
    "\n",
    "## üîÑ REINFORCE Algorithm\n",
    "\n",
    "**Algorithm 1: REINFORCE**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Policy $\\pi_\\theta$ with parameters $\\theta$\n",
    "- Learning rate $\\alpha$\n",
    "- Discount factor $\\gamma$\n",
    "- Number of episodes $N$\n",
    "\n",
    "**Output:** \n",
    "- Trained policy parameters $\\theta$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** policy parameters $\\theta$ randomly\n",
    "2. **For** $i = 1, 2, ..., N$ **do:**\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;**Generate trajectory** $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_{T-1}, a_{T-1}, r_T)$ using policy $\\pi_\\theta$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;**For** $t = 0, 1, ..., T-1$ **do:**\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Calculate return-to-go:** $G_t \\leftarrow \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;**Calculate policy gradient:** $g \\leftarrow \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;**Update parameters:** $\\theta \\leftarrow \\theta + \\alpha \\cdot g$\n",
    "9. **End For**\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Implementation Detail: Gradient Ascent vs Descent\n",
    "\n",
    "**‚ö†Ô∏è PyTorch Implementation Note**: The algorithm above shows **gradient ascent** (step 8: $\\theta \\leftarrow \\theta + \\alpha \\cdot g$) because we want to **maximize** the expected return $J(\\pi_\\theta)$.\n",
    "\n",
    "However, **PyTorch optimizers perform gradient descent** (minimization) by default. To convert our maximization problem to minimization, we use a simple trick:\n",
    "\n",
    "$$\\max_\\theta J(\\pi_\\theta) \\equiv \\min_\\theta (-J(\\pi_\\theta))$$\n",
    "\n",
    "## ‚ö†Ô∏è The Variance Problem & Return Normalization\n",
    "\n",
    "### The Core Challenge: Extremely High Variance\n",
    "\n",
    "**REINFORCE's Achilles' Heel**: The biggest obstacle to successful learning is the **enormous variance** in gradient estimates. This variance makes learning unstable and sample-inefficient.\n",
    "\n",
    "**Why Variance Destroys Learning:**\n",
    "- **Inconsistent Updates**: Same state-action pairs receive wildly different gradient signals\n",
    "- **Noisy Progress**: Training curves look chaotic rather than smooth improvement\n",
    "- **Poor Sample Efficiency**: Need many more episodes to average out the noise\n",
    "- **Unstable Convergence**: Algorithm may never converge to good solutions\n",
    "\n",
    "### üìä Return Normalization: A Simple Stability Technique\n",
    "\n",
    "**The Simple Trick**: In our REINFORCE implementation, we normalize the returns before computing gradients:\n",
    "\n",
    "```python\n",
    "returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "```\n",
    "\n",
    "#### Why Return Normalization Helps\n",
    "\n",
    "**The Problem**: Raw returns in LunarLander vary dramatically:\n",
    "- **Successful Landing**: Returns ‚âà +200 to +300\n",
    "- **Crash Landing**: Returns ‚âà -100 to -200\n",
    "- **Hovering/Timeout**: Returns ‚âà -50 to +50\n",
    "\n",
    "**Without Normalization**:\n",
    "```bash\n",
    "Episode 1: G_t = [250, 240, 220, ...]  ‚Üí Large positive gradients\n",
    "Episode 2: G_t = [-150, -140, -130, ...] ‚Üí Large negative gradients\n",
    "Episode 3: G_t = [180, 170, 160, ...]  ‚Üí Medium positive gradients\n",
    "```\n",
    "\n",
    "**With Normalization**:\n",
    "```bash\n",
    "Episode 1: G_t_norm = [1.2, 0.8, 0.3, ...]   ‚Üí Consistent magnitude\n",
    "Episode 2: G_t_norm = [-1.1, -0.9, -0.6, ...] ‚Üí Consistent magnitude  \n",
    "Episode 3: G_t_norm = [0.9, 0.5, 0.1, ...]   ‚Üí Consistent magnitude\n",
    "```\n",
    "\n",
    "#### Mathematical Justification\n",
    "\n",
    "**Key Insight**: The **relative ordering** of returns matters more than their absolute values!\n",
    "\n",
    "**Before Normalization**: \n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "**After Normalization**:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot \\frac{G_t - \\mu_G}{\\sigma_G}\\right]$$\n",
    "\n",
    "Where $\\mu_G$ and $\\sigma_G$ are the mean and standard deviation of returns in the current episode.\n",
    "\n",
    "#### Benefits of Return Normalization\n",
    "\n",
    "1. **Consistent Gradient Magnitudes**: Updates have similar scales across episodes\n",
    "2. **Relative Importance Preserved**: Actions that are relatively better still get stronger positive signals\n",
    "3. **Reduced Variance**: Extreme return values are tempered\n",
    "4. **Stable Learning**: Less likely to have destructively large updates\n",
    "5. **Better Convergence**: More consistent progress toward optimal policy\n",
    "\n",
    "## üé≤ Handling Different Action Spaces\n",
    "\n",
    "For REINFORCE to work, we need to be able to compute $\\log \\pi_\\theta(a|s)$. How we do this depends on the action space.\n",
    "\n",
    "### Discrete Action Space (`continuous=False`)\n",
    "\n",
    "- **Action Space**: A finite set of actions: [do_nothing, fire_left, fire_main, fire_right]\n",
    "- **Policy Network Output**: The network outputs **logits**, one for each possible action.\n",
    "- **Probability Distribution**: We apply a **Softmax** function to the logits to create a probability distribution over actions.\n",
    "  $$\\pi_\\theta(a|s) = \\text{Softmax}(\\text{logits})_a = \\frac{\\exp(\\text{logit}_a)}{\\sum_{a'} \\exp(\\text{logit}_{a'})}$$\n",
    "- **Action Sampling**: We sample an action from this categorical distribution.\n",
    "- **Log-Probability**: The log-probability is computed directly from the categorical distribution, which is computationally straightforward.\n",
    "\n",
    "### Continuous Action Space (`continuous=True`)\n",
    "\n",
    "- **Action Space**: A 2D vector [main_engine, lateral_booster] with bounds [-1.0, +1.0] each\n",
    "- **Policy Network Output**: The network outputs the **mean** ($\\mu_\\theta(s)$) for a Gaussian distribution for each action dimension. We often use a fixed or learnable standard deviation ($\\sigma$).\n",
    "- **Probability Distribution**: We model the policy as a **Gaussian (Normal) distribution** for each action dimension.\n",
    "  $$\\pi_\\theta(a|s) = \\mathcal{N}(a; \\mu_\\theta(s), \\sigma^2)$$\n",
    "- **Action Sampling**: We sample an action vector from this multi-variate Gaussian distribution.\n",
    "- **Log-Probability**: The log-probability is the log-pdf of the Gaussian distribution evaluated at the sampled action.\n",
    "- **Action Clipping**: Since the environment has action bounds ([-1, 1] for each dimension), the sampled actions from the unbounded Gaussian distribution must be clipped before being sent to the environment.\n",
    "\n",
    "## ‚ö° Advantages and Disadvantages\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "1. **Conceptual Simplicity**: Pure policy optimization, easy to understand\n",
    "2. **Unbiased Estimates**: Uses actual returns, no approximation bias\n",
    "3. **Action Space Flexibility**: Works perfectly with LunarLander's dual action modes\n",
    "4. **Stochastic Policy**: Natural exploration through policy randomness\n",
    "5. **Convergence Guarantees**: Guaranteed to converge to local optimum (under conditions)\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "1. **üî• HUGE VARIANCE**: Monte Carlo estimates have extremely high variance\n",
    "   - **LunarLander Impact**: Episode scores range from -100 (crash) to 300+ (perfect landing)\n",
    "   - **Symptom**: Training curves look like noise, very unstable learning\n",
    "   - **Learning Difficulty**: Same action taken with slight timing differences gives vastly different returns\n",
    "2. **Sample Inefficiency**: Must complete entire episodes before any learning\n",
    "3. **No Credit Assignment**: All actions in episode get same return signal\n",
    "4. **Local Optima**: Gradient ascent only finds local maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f69376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Global random seeds set to 42 for reproducible results\n",
      "üìù Environment episodes will use seeds 42 + episode_number for varied but reproducible episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our RL utilities\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    PolicyNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 5e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"window_length\": 50,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"num_videos\": 9,  # Number of videos to record during training\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"policy_network\": {\n",
    "        \"fc_out_features\": [64, 64, 64],\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"üé≤ Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"üìù Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727e03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "    \"\"\"REINFORCE agent containing the policy and update logic.\"\"\"\n",
    "\n",
    "    def __init__(self, policy_network, config):\n",
    "        self.policy = policy_network.to(config[\"device\"])\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=config[\"lr\"])\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.window_size = config.get(\"window_length\")\n",
    "\n",
    "        # Print detailed network information including parameter count\n",
    "        print(f\"üìä POLICY NETWORK DETAILS:\")\n",
    "        self.policy.print_network_info()\n",
    "\n",
    "        # Episode-specific storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        # Gradient tracking for variance analysis\n",
    "        self.gradient_norms = []\n",
    "        self.episode_returns = []\n",
    "        self.return_variance_history = []\n",
    "\n",
    "        # Update step tracking - separate from episode numbers\n",
    "        self.update_step = 0\n",
    "        self.update_steps_history = (\n",
    "            []\n",
    "        )  # Track which update step each loss/gradient corresponds to\n",
    "\n",
    "        # Loss component tracking for future algorithms\n",
    "        self.loss_history = {\n",
    "            \"policy_loss\": [],\n",
    "            \"total_loss\": [],  # For REINFORCE, total_loss == policy_loss\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action from the policy distribution.\"\"\"\n",
    "        # Normalize observation before feeding to network\n",
    "\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        dist = self.policy(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        if self.policy.is_continuous:\n",
    "            log_prob = dist.log_prob(action).sum(-1)\n",
    "            # Use the policy network's clip_action method for proper bounds\n",
    "            action_to_env = self.policy.clip_action(action).flatten()\n",
    "        else:\n",
    "            log_prob = dist.log_prob(action)\n",
    "            action_to_env = action.item()\n",
    "\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action_to_env\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy network using the collected episode data.\"\"\"\n",
    "        if not self.log_probs:\n",
    "            return {\"policy_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        # Increment update step counter\n",
    "        self.update_step += 1\n",
    "\n",
    "        # 1. Calculate discounted returns (G_t)\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Store episode return for variance tracking\n",
    "        episode_return = returns[0].item() if len(returns) > 0 else 0.0\n",
    "        self.episode_returns.append(episode_return)\n",
    "\n",
    "        # Track return variance over recent episodes - only store when we have enough data\n",
    "        if len(self.episode_returns) >= self.window_size:\n",
    "            recent_returns = self.episode_returns[-self.window_size :]\n",
    "            return_variance = np.var(recent_returns)\n",
    "            self.return_variance_history.append(return_variance)\n",
    "        # Don't append 0.0 when we don't have enough data - just skip\n",
    "\n",
    "        # 2. Normalize returns for stability (a common trick for REINFORCE)\n",
    "        returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        log_probs_tensor = torch.stack(self.log_probs)\n",
    "        policy_loss = -(log_probs_tensor * returns_normalized).mean()\n",
    "\n",
    "        # 4. Update policy network and track gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "\n",
    "        # Calculate gradient norm before clipping/updating\n",
    "        total_grad_norm = 0.0\n",
    "        for param in self.policy.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "        total_grad_norm = total_grad_norm ** (1.0 / 2)\n",
    "        self.gradient_norms.append(total_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 5. Store loss components and corresponding update step\n",
    "        policy_loss_value = policy_loss.item()\n",
    "        total_loss_value = policy_loss_value  # For REINFORCE, total == policy\n",
    "\n",
    "        self.loss_history[\"policy_loss\"].append(policy_loss_value)\n",
    "        self.loss_history[\"total_loss\"].append(total_loss_value)\n",
    "        self.update_steps_history.append(self.update_step)\n",
    "\n",
    "        # 6. Clear episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        return {\n",
    "            \"policy_loss\": policy_loss_value,\n",
    "            \"total_loss\": total_loss_value,\n",
    "        }, total_grad_norm\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.episode_returns) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"return_mean\": 0.0,\n",
    "                \"return_std\": 0.0,\n",
    "                \"recent_return_variance\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"return_mean\": np.mean(self.episode_returns),\n",
    "            \"return_std\": np.std(self.episode_returns),\n",
    "            \"recent_return_variance\": (\n",
    "                self.return_variance_history[-1]\n",
    "                if self.return_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a824066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(is_continuous, config):\n",
    "    \"\"\"Main training loop for the REINFORCE agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REINFORCE ({action_type.upper()}) TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate video recording interval\n",
    "    video_record_interval = max(1, config[\"episodes\"] // config[\"num_videos\"])\n",
    "    print(f\"üìπ Recording {config['num_videos']} videos every {video_record_interval} episodes\")\n",
    "    \n",
    "    # Create algorithm-specific video folder\n",
    "    video_folder = f\"videos/REINFORCE_{action_type.lower()}\"\n",
    "    config_with_videos = config.copy()\n",
    "    config_with_videos[\"video_folder\"] = video_folder\n",
    "    config_with_videos[\"video_record_interval\"] = video_record_interval\n",
    "    \n",
    "    # Create Environment (this will automatically clean up existing videos)\n",
    "    env = create_env_with_wrappers(\n",
    "        config_with_videos, \n",
    "        is_continuous, \n",
    "        record_videos=True, \n",
    "        video_prefix=f\"reinforce_{action_type.lower()}\",\n",
    "        cleanup_existing=True\n",
    "    )\n",
    "    \n",
    "    # Get observation dimension and space\n",
    "    dummy_obs, _ = env.reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "    \n",
    "    # Create Policy Network and Agent\n",
    "    print(f\"\\nüèóÔ∏è CREATING {action_type.upper()} POLICY NETWORK:\")\n",
    "    policy_net = PolicyNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=env.action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"policy_network\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ INITIALIZING {action_type.upper()} REINFORCE AGENT:\")\n",
    "    agent = ReinforceAgent(policy_net, config)\n",
    "    \n",
    "    # Training Loop\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=config[\"window_length\"])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING {action_type.upper()} TRAINING...\")\n",
    "    \n",
    "    # Use tqdm for progress bar with detailed information\n",
    "    pbar = tqdm(range(1, config[\"episodes\"] + 1), desc=\"Training\", unit=\"episode\")\n",
    "    \n",
    "    for i_episode in pbar:\n",
    "        state, _ = env.reset(seed=config[\"seed\"] + i_episode)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        loss_dict, grad_norm = agent.update_policy()\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "        scores_window.append(ep_reward)\n",
    "        \n",
    "        # Update tqdm description with current statistics - use specific loss names\n",
    "        avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "        policy_loss = loss_dict['policy_loss'] if loss_dict['policy_loss'] is not None else 0.0\n",
    "        total_loss = loss_dict['total_loss'] if loss_dict['total_loss'] is not None else 0.0\n",
    "        \n",
    "        pbar.set_description(\n",
    "            f\"Ep {i_episode:4d} | \"\n",
    "            f\"Score: {ep_reward:6.1f} | \"\n",
    "            f\"Avg({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "            f\"PolicyLoss: {policy_loss:8.4f} | \"\n",
    "            f\"TotalLoss: {total_loss:8.4f} | \"\n",
    "            f\"GradNorm: {grad_norm:6.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Handle video display - show all videos collected so far\n",
    "        if i_episode % video_record_interval == 0 and config[\"record_videos\"]:\n",
    "            from rl_utils.environment import display_latest_video\n",
    "            pbar.write(f\"\\nVideo recorded at episode {i_episode}\")\n",
    "            display_latest_video(\n",
    "                config_with_videos[\"video_folder\"], \n",
    "                f\"reinforce_{action_type.lower()}\", \n",
    "                i_episode\n",
    "            )\n",
    "    \n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    # Use config window size for final performance calculation\n",
    "    final_window_size = min(config[\"window_length\"], len(scores))\n",
    "    final_performance = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    print(f\"\\n{action_type} training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Final performance: {final_performance:.2f} (last {final_window_size} episodes)\")\n",
    "    \n",
    "    return scores, agent.loss_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ Displaying 2 training videos (episodes: [110, 221]):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;\">\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 110</p>\n",
       "                <video width=\"450\" height=\"337\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/REINFORCE_discrete/reinforce_discrete-episode-110.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 221</p>\n",
       "                <video width=\"450\" height=\"337\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/REINFORCE_discrete/reinforce_discrete-episode-221.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "        </div>\n",
       "        <style>\n",
       "            video {\n",
       "                border: 2px solid #ccc;\n",
       "                border-radius: 8px;\n",
       "                box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  222 | Score: -265.0 | Avg(50): -121.9 | PolicyLoss:  -0.0145 | TotalLoss:  -0.0145 | GradNorm: 0.4360:  22%|‚ñà‚ñà‚ñè       | 222/1000 [01:20<18:38,  1.44s/episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ 2 training videos available in videos/REINFORCE_discrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  257 | Score:  -16.5 | Avg(50): -108.9 | PolicyLoss:   0.0568 | TotalLoss:   0.0568 | GradNorm: 0.6852:  26%|‚ñà‚ñà‚ñå       | 257/1000 [02:18<09:46,  1.27episode/s]"
     ]
    }
   ],
   "source": [
    "# --- DISCRETE ACTION SPACE TRAINING ---\n",
    "print(\"Starting REINFORCE training with DISCRETE actions...\")\n",
    "\n",
    "discrete_scores, discrete_losses, discrete_agent = train_reinforce(is_continuous=False, config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results with REINFORCE-specific analysis\n",
    "plot_training_results(discrete_scores, discrete_agent.loss_history, CONFIG, \"Discrete\", algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# Show variance analysis with REINFORCE-specific commentary\n",
    "plot_variance_analysis(discrete_agent, discrete_scores, \"Discrete\", CONFIG, algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_scores))\n",
    "final_avg = np.mean(discrete_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_agent.policy.get_param_count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE TRAINING ---\n",
    "print(\"Starting REINFORCE training with CONTINUOUS actions...\")\n",
    "\n",
    "continuous_scores, continuous_losses, continuous_agent = train_reinforce(is_continuous=True, config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results with REINFORCE-specific analysis\n",
    "plot_training_results(continuous_scores, continuous_agent.loss_history, CONFIG, \"Continuous\", algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# Show variance analysis with REINFORCE-specific commentary\n",
    "plot_variance_analysis(continuous_agent, continuous_scores, \"Continuous\", CONFIG, algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_scores))\n",
    "final_avg = np.mean(continuous_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_agent.policy.get_param_count():,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ca7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPARATIVE ANALYSIS ---\n",
    "from rl_utils.visualization import plot_comparison\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPARATIVE ANALYSIS: Discrete vs Continuous REINFORCE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print parameter counts for comparison\n",
    "print(f\"\\nüìä NETWORK PARAMETER COMPARISON:\")\n",
    "print(f\"Discrete Policy Network:   {discrete_agent.policy.get_param_count():,} parameters\")\n",
    "print(f\"Continuous Policy Network: {continuous_agent.policy.get_param_count():,} parameters\")\n",
    "\n",
    "# Plot comparison with REINFORCE-specific context\n",
    "plot_comparison(\n",
    "    discrete_results=(discrete_scores, discrete_agent),\n",
    "    continuous_results=(continuous_scores, continuous_agent),\n",
    "    config=CONFIG,\n",
    "    algorithm_name=\"REINFORCE\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
