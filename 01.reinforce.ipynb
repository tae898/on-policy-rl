{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d811b2d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Fundamentals\n",
    "\n",
    "## üéØ The Core RL Problem\n",
    "\n",
    "**Reinforcement Learning** is fundamentally about learning a **policy** $\\pi$ that maximizes the **expected cumulative discounted reward**:\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ...)$ is a **trajectory** (sequence of states, actions, rewards)\n",
    "- $\\gamma \\in [0,1]$ is the **discount factor** (how much we value future rewards)\n",
    "- $R_t$ is the reward at time step $t$\n",
    "- $\\pi$ is our **policy** (strategy for choosing actions)\n",
    "\n",
    "## üîÑ Markov Decision Process (MDP) Assumption\n",
    "\n",
    "To make RL tractable, we assume the environment follows the **Markov Property**:\n",
    "\n",
    "$$P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ..., s_0, a_0) = P(s_{t+1} | s_t, a_t)$$\n",
    "\n",
    "**Translation:** *The future depends only on the present state and action, not the entire history.*\n",
    "\n",
    "### Why This Assumption Matters\n",
    "\n",
    "**Without the Markov assumption** (Partially Observable MDPs - POMDPs):\n",
    "- We need to maintain **belief states** over possible true states\n",
    "- The agent must learn to **infer hidden information** from observation history\n",
    "- **Exponentially harder** - need to track probability distributions over states\n",
    "- **Memory becomes crucial** - past observations help disambiguate current state\n",
    "- Examples: Poker (hidden cards), autonomous driving with limited sensors\n",
    "\n",
    "**With the Markov assumption** (Full MDPs):\n",
    "- Current state contains **all relevant information** for decision making\n",
    "- We can use **memoryless policies**: $\\pi(a|s)$ depends only on current state\n",
    "- **Mathematically tractable** - enables dynamic programming approaches\n",
    "- **Most RL algorithms assume this** for computational feasibility\n",
    "\n",
    "### MDP Components\n",
    "\n",
    "An MDP is formally defined as a 5-tuple: $(S, A, P, R, \\gamma)$\n",
    "\n",
    "- **$S$**: State space (all possible states)\n",
    "- **$A$**: Action space (all possible actions)  \n",
    "- **$P(s'|s,a)$**: Transition probabilities (environment dynamics)\n",
    "- **$R(s,a)$**: Reward function (immediate feedback from current state and action)\n",
    "- **$\\gamma$**: Discount factor (future reward weighting)\n",
    "\n",
    "## üé≤ Policy Types\n",
    "\n",
    "### Stochastic vs Deterministic Policies\n",
    "\n",
    "**Stochastic Policy:** $\\pi(a|s) = P(\\text{action } a | \\text{state } s)$\n",
    "- Outputs **probability distribution** over actions\n",
    "- **Natural exploration** through randomness\n",
    "- Examples: Softmax policy, Gaussian policy\n",
    "\n",
    "**Deterministic Policy:** $\\pi(s) = a$\n",
    "- Outputs **single action** for each state  \n",
    "- Requires **external exploration** mechanisms (Œµ-greedy, noise)\n",
    "- Examples: Greedy policy derived from learned functions\n",
    "\n",
    "### Parameterized Policies: The Deep RL Revolution\n",
    "\n",
    "Instead of learning a lookup table, we use **function approximation**:\n",
    "\n",
    "$$\\pi_\\theta(a|s) = \\text{Neural Network}(s; \\theta)$$\n",
    "\n",
    "Where $\\theta$ are the **learnable parameters** (neural network weights).\n",
    "\n",
    "#### üìö From Classical RL to Deep RL\n",
    "\n",
    "**Classical (Tabular) RL Era** (pre-2010s):\n",
    "- **Policy Representation**: Lookup table $\\pi(s) = a$ for each state\n",
    "- **State Space Requirement**: Must enumerate ALL possible states\n",
    "- **Feasible Environments**: \n",
    "  - GridWorld (10√ó10 = 100 states)\n",
    "  - Tic-Tac-Toe (‚âà3,000 unique states)\n",
    "  - Simple card games\n",
    "- **Fatal Limitation**: **Curse of dimensionality** - exponential growth in state space\n",
    "\n",
    "**Deep RL Era** (2010s-present):\n",
    "- **Policy Representation**: Neural network $\\pi_\\theta(a|s)$ that **generalizes** across states\n",
    "- **State Space Capability**: Can handle **infinite** or astronomically large state spaces\n",
    "- **Feasible Environments**:\n",
    "  - LunarLander: Continuous 8D observation space\n",
    "  - Chess: $‚âà10^{47}$ possible board positions\n",
    "  - Go: $‚âà10^{170}$ possible board positions (more than atoms in observable universe!)\n",
    "\n",
    "#### üöÄ Why Function Approximation Changed Everything\n",
    "\n",
    "**The Generalization Miracle:**\n",
    "- A neural network trained on seeing a lander at position (0.1, 0.5) can **generalize** to position (0.11, 0.51)\n",
    "- **Never seen that exact state before**, but can interpolate from similar experiences\n",
    "- **Enables learning** from finite experience in infinite state spaces\n",
    "\n",
    "## üèóÔ∏è Our Learning Environment: LunarLander-v3\n",
    "\n",
    "For this entire learning series, we'll use **LunarLander-v3** exclusively. This environment provides the perfect balance of complexity and simplicity for RL education.\n",
    "\n",
    "**Reference**: [Gymnasium Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "### Environment Specifications\n",
    "\n",
    "#### LunarLander-v3 üöÄ\n",
    "**Observation**: 8D vector with position, velocity, angle, angular velocity, leg ground contacts\n",
    "\n",
    "**Action Spaces**:\n",
    "- **Discrete Mode**: 4 actions [do_nothing, fire_left, fire_main, fire_right]\n",
    "- **Continuous Mode**: 2D vector [main_engine, lateral_booster] with bounds [-1.0, +1.0] each\n",
    "\n",
    "**Rewards**: Distance/speed/angle penalties, +10 per leg contact, engine costs, ¬±100 for outcome\n",
    "\n",
    "### Continuous Action Space Bounds\n",
    "\n",
    "**LunarLander-v3 Continuous Actions:**\n",
    "- **Action 0 (Main Engine)**: Range [-1.0, +1.0]\n",
    "  - Negative values: No effect (engine can't push upward)\n",
    "  - 0.0: Engine off\n",
    "  - Positive values: Throttle from 0% to 100%\n",
    "- **Action 1 (Lateral Engine)**: Range [-1.0, +1.0]\n",
    "  - -1.0: Full left thruster\n",
    "  - 0.0: No lateral thrust\n",
    "  - +1.0: Full right thruster\n",
    "\n",
    "### Why LunarLander-v3 for RL Education?\n",
    "\n",
    "- **Dual Action Spaces**: Perfect for testing both discrete and continuous control algorithms\n",
    "- **Vector Observations**: Rich, interpretable 8D state representation perfect for learning\n",
    "- **Realistic Physics**: Consistent Box2D physics provides realistic dynamics\n",
    "- **Educational Value**: Classic trajectory optimization problem\n",
    "- **Fast Feedback**: Clear success/failure conditions with immediate results\n",
    "- **No Visual Complexity**: Focus on algorithms, not CNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889153b",
   "metadata": {},
   "source": [
    "# REINFORCE Algorithm: Vanilla Policy Gradients\n",
    "\n",
    "## üéØ Algorithm Overview\n",
    "\n",
    "**REINFORCE** (REward Increment = Nonnegative Factor √ó Offset Reinforcement √ó Characteristic Eligibility) is the most fundamental policy gradient algorithm. It directly optimizes the policy parameters using the **policy gradient theorem**.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Pure Monte Carlo Method**: Waits for complete episodes, uses full episode returns\n",
    "- **No Bootstrapping**: Never uses estimates to update estimates (unlike future methods we'll see)\n",
    "- **On-Policy**: Uses experience only from the current policy being learned\n",
    "- **Model-Free**: Doesn't require knowledge of environment dynamics\n",
    "- **Policy-Based Only**: Directly optimizes the policy, no additional function learning\n",
    "\n",
    "## üìê Mathematical Foundation\n",
    "\n",
    "### The Log-Likelihood Trick: From RL to Gradients\n",
    "\n",
    "**üîë Key Challenge**: How do we compute gradients when our objective involves expectations over trajectories that depend on our policy parameters?\n",
    "\n",
    "**Starting Point**: Our objective is to maximize expected return:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\gamma^t r_{t+1}\\right]$$\n",
    "\n",
    "**The Problem**: The expectation is over trajectories $\\tau$ sampled from $\\pi_\\theta$, so the sampling distribution itself depends on $\\theta$!\n",
    "\n",
    "**The Log-Likelihood Trick**: We use the mathematical identity:\n",
    "$$\\nabla_\\theta \\pi_\\theta(\\tau) = \\pi_\\theta(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)$$\n",
    "\n",
    "This allows us to rewrite:\n",
    "$$\\nabla_\\theta \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[G(\\tau) \\nabla_\\theta \\log \\pi_\\theta(\\tau)]$$\n",
    "\n",
    "Where $G(\\tau) = \\sum_{t=0}^{T-1} \\gamma^t r_{t+1}$ is the episode return.\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "Expanding the trajectory log-probability and using the Markov property:\n",
    "\n",
    "$$\\nabla_\\theta J(\\pi_\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$ are the policy parameters (neural network weights)\n",
    "- $G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}$ is the **actual return** (cumulative future reward from episode)\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ is the **score function** (gradient of log-probability)\n",
    "\n",
    "### Intuitive Interpretation\n",
    "\n",
    "$$\\underbrace{\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)}_{\\text{Direction to increase } P(a_t|s_t)} \\cdot \\underbrace{G_t}_{\\text{How good was this action?}}$$\n",
    "\n",
    "**English Translation:** *\"Increase the probability of actions that led to high returns, decrease the probability of actions that led to low returns.\"*\n",
    "\n",
    "## üîÑ RL vs Supervised Learning: A Striking Similarity\n",
    "\n",
    "### Gradient Structures are Nearly Identical!\n",
    "\n",
    "**Supervised Learning Gradient (Maximum Likelihood Estimation):**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{(x,y) \\sim D_{\\text{train}}}\\left[\\nabla_\\theta \\log p(y|x; \\theta)\\right]$$\n",
    "\n",
    "**REINFORCE Gradient:**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "**Key Similarities:**\n",
    "- Both use **log-probability gradients** as the core update mechanism\n",
    "- Both are **expectation-based** optimization procedures  \n",
    "- Both adjust parameters to increase probability of \"good\" outcomes\n",
    "\n",
    "### Understanding the Supervised Learning Foundation\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE) in Supervised Learning:**\n",
    "- **Objective**: Maximize the likelihood of observed data: $\\max_\\theta \\prod_{i=1}^N p(y_i|x_i; \\theta)$\n",
    "- **Equivalent**: Minimize negative log-likelihood: $\\min_\\theta -\\sum_{i=1}^N \\log p(y_i|x_i; \\theta)$\n",
    "- **I.I.D. Assumption**: Training samples $(x_i, y_i)$ are **independent and identically distributed**\n",
    "- **Fixed Target**: Each sample has a known, fixed \"correct\" answer $y_i$\n",
    "\n",
    "**REINFORCE as \"Weighted MLE\":**\n",
    "- **Pseudo-Target**: Actions $a_t$ become \"correct answers\" **weighted by their return** $G_t$\n",
    "- **No Fixed Labels**: Instead of fixed $y_i$, we have actions weighted by how good they turned out to be\n",
    "- **Return as Importance**: $G_t$ tells us \"how much\" we should treat action $a_t$ as correct\n",
    "\n",
    "### The Fundamental Difference: Data Control & Distribution\n",
    "\n",
    "**Supervised Learning:**\n",
    "- Data $(x,y)$ is sampled from **fixed** dataset $D_{\\text{train}}$ \n",
    "- Model has **no control** over which samples it sees\n",
    "- Distribution is **stationary** - same data every epoch\n",
    "- **I.I.D. samples**: Each training example is independent\n",
    "\n",
    "**Reinforcement Learning:**\n",
    "- Data (trajectories) generated by **agent's own policy** $\\pi_\\theta$\n",
    "- Agent **controls** what data it collects through its actions  \n",
    "- Distribution is **non-stationary** - changes as policy improves!\n",
    "- **Sequential dependence**: Each action affects future states and available data\n",
    "\n",
    "### Why This Makes RL Much Harder\n",
    "\n",
    "**The Challenge**: As $\\theta$ changes, so does $\\pi_\\theta$, which changes the data distribution, which affects the gradient estimates! This creates:\n",
    "- **High variance** in gradient estimates\n",
    "- **Non-stationary** learning problem  \n",
    "- **Exploration-exploitation** tradeoffs\n",
    "\n",
    "## üîÑ REINFORCE Algorithm\n",
    "\n",
    "**Algorithm 1: REINFORCE**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Policy $\\pi_\\theta$ with parameters $\\theta$\n",
    "- Learning rate $\\alpha$\n",
    "- Discount factor $\\gamma$\n",
    "- Number of episodes $N$\n",
    "\n",
    "**Output:** \n",
    "- Trained policy parameters $\\theta$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** policy parameters $\\theta$ randomly\n",
    "2. **For** $i = 1, 2, ..., N$ **do:**\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;**Generate trajectory** $\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_{T-1}, a_{T-1}, r_T)$ using policy $\\pi_\\theta$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;**For** $t = 0, 1, ..., T-1$ **do:**\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Calculate return-to-go:** $G_t \\leftarrow \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;**Calculate policy gradient:** $g \\leftarrow \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;**Update parameters:** $\\theta \\leftarrow \\theta + \\alpha \\cdot g$\n",
    "9. **End For**\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Implementation Detail: Gradient Ascent vs Descent\n",
    "\n",
    "**‚ö†Ô∏è PyTorch Implementation Note**: The algorithm above shows **gradient ascent** (step 8: $\\theta \\leftarrow \\theta + \\alpha \\cdot g$) because we want to **maximize** the expected return $J(\\pi_\\theta)$.\n",
    "\n",
    "However, **PyTorch optimizers perform gradient descent** (minimization) by default. To convert our maximization problem to minimization, we use a simple trick:\n",
    "\n",
    "$$\\max_\\theta J(\\pi_\\theta) \\equiv \\min_\\theta (-J(\\pi_\\theta))$$\n",
    "\n",
    "\n",
    "## ‚ö†Ô∏è The Variance Problem\n",
    "\n",
    "### High Variance Issue\n",
    "\n",
    "The biggest challenge with REINFORCE is **extremely high variance** in gradient estimates. The gradient estimate for a state-action pair can vary dramatically. In one episode, an action might be part of a successful landing with a high return (e.g., `G_t = 250`). In the next, a crash shortly after that same action results in a very low return (e.g., `G_t = -100`). This provides conflicting update signals for the same action, leading to instability.\n",
    "\n",
    "### Why This Happens\n",
    "\n",
    "1. **Episode-to-Episode Randomness**: LunarLander episodes can vary based on initial conditions and stochastic dynamics\n",
    "2. **No Averaging**: We use raw episode returns, not smoothed estimates\n",
    "3. **Full Episode Dependence**: One crash can ruin the entire episode's learning\n",
    "4. **Self-Generated Data**: The policy generates its own training data, creating instability\n",
    "\n",
    "### Pure Monte Carlo Nature\n",
    "\n",
    "üé≤ **Key Insight**: REINFORCE uses **actual episode returns** $G_t$, not estimates!\n",
    "\n",
    "- **Monte Carlo**: We sample complete episodes and use the true cumulative rewards\n",
    "- **No Bootstrapping**: We never use approximations (those come in future methods!)\n",
    "- **Episode-Based**: Must wait for episode completion before any learning\n",
    "\n",
    "### Demonstrating Variance in Our Implementation\n",
    "\n",
    "In our code implementation, we'll:\n",
    "- **Track gradient magnitudes** across episodes\n",
    "- **Visualize gradient variance** to see the instability\n",
    "- **Show episode return distributions** to understand the noise\n",
    "- **Compare raw vs smoothed learning curves**\n",
    "\n",
    "This will clearly demonstrate why more advanced methods use techniques like:\n",
    "- **Baselines** to reduce variance\n",
    "- **Bootstrapping** to use estimates instead of raw returns\n",
    "- **Actor-Critic** methods that combine policy and additional learning\n",
    "\n",
    "## üé≤ Handling Different Action Spaces\n",
    "\n",
    "For REINFORCE to work, we need to be able to compute $\\log \\pi_\\theta(a|s)$. How we do this depends on the action space.\n",
    "\n",
    "### Discrete Action Space (`continuous=False`)\n",
    "\n",
    "- **Action Space**: A finite set of actions: [do_nothing, fire_left, fire_main, fire_right]\n",
    "- **Policy Network Output**: The network outputs **logits**, one for each possible action.\n",
    "- **Probability Distribution**: We apply a **Softmax** function to the logits to create a probability distribution over actions.\n",
    "  $$\\pi_\\theta(a|s) = \\text{Softmax}(\\text{logits})_a = \\frac{\\exp(\\text{logit}_a)}{\\sum_{a'} \\exp(\\text{logit}_{a'})}$$\n",
    "- **Action Sampling**: We sample an action from this categorical distribution.\n",
    "- **Log-Probability**: The log-probability is computed directly from the categorical distribution, which is computationally straightforward.\n",
    "\n",
    "### Continuous Action Space (`continuous=True`)\n",
    "\n",
    "- **Action Space**: A 2D vector [main_engine, lateral_booster] with bounds [-1.0, +1.0] each\n",
    "- **Policy Network Output**: The network outputs the **mean** ($\\mu_\\theta(s)$) for a Gaussian distribution for each action dimension. We often use a fixed or learnable standard deviation ($\\sigma$).\n",
    "- **Probability Distribution**: We model the policy as a **Gaussian (Normal) distribution** for each action dimension.\n",
    "  $$\\pi_\\theta(a|s) = \\mathcal{N}(a; \\mu_\\theta(s), \\sigma^2)$$\n",
    "- **Action Sampling**: We sample an action vector from this multi-variate Gaussian distribution.\n",
    "- **Log-Probability**: The log-probability is the log-pdf of the Gaussian distribution evaluated at the sampled action.\n",
    "- **Action Clipping**: Since the environment has action bounds ([-1, 1] for each dimension), the sampled actions from the unbounded Gaussian distribution must be clipped before being sent to the environment.\n",
    "\n",
    "## ‚ö° Advantages and Disadvantages\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "1. **Conceptual Simplicity**: Pure policy optimization, easy to understand\n",
    "2. **Unbiased Estimates**: Uses actual returns, no approximation bias\n",
    "3. **Action Space Flexibility**: Works perfectly with LunarLander's dual action modes\n",
    "4. **Stochastic Policy**: Natural exploration through policy randomness\n",
    "5. **Convergence Guarantees**: Guaranteed to converge to local optimum (under conditions)\n",
    "\n",
    "### ‚ùå Disadvantages\n",
    "\n",
    "1. **üî• HUGE VARIANCE**: Monte Carlo estimates have extremely high variance\n",
    "   - **LunarLander Impact**: Episode scores range from -100 (crash) to 300+ (perfect landing)\n",
    "   - **Symptom**: Training curves look like noise, very unstable learning\n",
    "   - **Learning Difficulty**: Same action taken with slight timing differences gives vastly different returns\n",
    "2. **Sample Inefficiency**: Must complete entire episodes before any learning\n",
    "3. **No Credit Assignment**: All actions in episode get same return signal\n",
    "4. **Local Optima**: Gradient ascent only finds local maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f69376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Global random seeds set to 42 for reproducible results\n",
      "üìù Environment episodes will use seeds 42 + episode_number for varied but reproducible episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our RL utilities\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    PolicyNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    preprocess_state,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 5e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"scores_window_size\": 10,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"video_record_interval\": 111,\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"policy_network\": {\n",
    "        \"fc_out_features\": [64, 32, 16],\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.1,\n",
    "    },\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"üé≤ Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"üìù Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "727e03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceAgent:\n",
    "    \"\"\"REINFORCE agent containing the policy and update logic.\"\"\"\n",
    "\n",
    "    def __init__(self, policy_network, config):\n",
    "        self.policy = policy_network.to(config[\"device\"])\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(), lr=config[\"lr\"])\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.window_size = config.get(\"scores_window_size\")\n",
    "\n",
    "        # Print detailed network information including parameter count\n",
    "        print(f\"üìä POLICY NETWORK DETAILS:\")\n",
    "        self.policy.print_network_info()\n",
    "\n",
    "        # Episode-specific storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        # Gradient tracking for variance analysis\n",
    "        self.gradient_norms = []\n",
    "        self.episode_returns = []\n",
    "        self.return_variance_history = []\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action from the policy distribution.\"\"\"\n",
    "        state = state.to(self.device)\n",
    "        dist = self.policy(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        if self.policy.is_continuous:\n",
    "            log_prob = dist.log_prob(action).sum(-1)\n",
    "            # Use the policy network's clip_action method for proper bounds\n",
    "            action_to_env = self.policy.clip_action(action).flatten()\n",
    "        else:\n",
    "            log_prob = dist.log_prob(action)\n",
    "            action_to_env = action.item()\n",
    "\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action_to_env\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy network using the collected episode data.\"\"\"\n",
    "        if not self.log_probs:\n",
    "            return 0.0, 0.0\n",
    "\n",
    "        # 1. Calculate discounted returns (G_t)\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Store episode return for variance tracking\n",
    "        episode_return = returns[0].item() if len(returns) > 0 else 0.0\n",
    "        self.episode_returns.append(episode_return)\n",
    "\n",
    "        # Track return variance over recent episodes\n",
    "        if len(self.episode_returns) >= self.window_size:\n",
    "            recent_returns = self.episode_returns[-self.window_size:]\n",
    "            return_variance = np.var(recent_returns)\n",
    "            self.return_variance_history.append(return_variance)\n",
    "        else:\n",
    "            self.return_variance_history.append(0.0)\n",
    "\n",
    "        # 2. Normalize returns for stability (a common trick for REINFORCE)\n",
    "        returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        # 3. Calculate policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(self.log_probs, returns_normalized):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        # 4. Update policy network and track gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "\n",
    "        # Calculate gradient norm before clipping/updating\n",
    "        total_grad_norm = 0.0\n",
    "        for param in self.policy.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "        total_grad_norm = total_grad_norm ** (1.0 / 2)\n",
    "        self.gradient_norms.append(total_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 5. Clear episode storage\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "        return policy_loss.item(), total_grad_norm\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.episode_returns) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"return_mean\": 0.0,\n",
    "                \"return_std\": 0.0,\n",
    "                \"recent_return_variance\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"return_mean\": np.mean(self.episode_returns),\n",
    "            \"return_std\": np.std(self.episode_returns),\n",
    "            \"recent_return_variance\": (\n",
    "                self.return_variance_history[-1]\n",
    "                if self.return_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a824066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(is_continuous, config):\n",
    "    \"\"\"Main training loop for the REINFORCE agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"REINFORCE ({action_type.upper()}) TRAINING\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create algorithm-specific video folder\n",
    "    video_folder = f\"videos/REINFORCE_{action_type.lower()}\"\n",
    "    config_with_videos = config.copy()\n",
    "    config_with_videos[\"video_folder\"] = video_folder\n",
    "    \n",
    "    # Create Environment (this will automatically clean up existing videos)\n",
    "    env = create_env_with_wrappers(\n",
    "        config_with_videos, \n",
    "        is_continuous, \n",
    "        record_videos=True, \n",
    "        video_prefix=f\"reinforce_{action_type.lower()}\",\n",
    "        cleanup_existing=True\n",
    "    )\n",
    "    \n",
    "    # Get observation dimension\n",
    "    dummy_obs, _ = env.reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "    \n",
    "    # Create Policy Network and Agent\n",
    "    print(f\"\\nüèóÔ∏è CREATING {action_type.upper()} POLICY NETWORK:\")\n",
    "    policy_net = PolicyNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=env.action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"policy_network\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ INITIALIZING {action_type.upper()} REINFORCE AGENT:\")\n",
    "    agent = ReinforceAgent(policy_net, config)\n",
    "    \n",
    "    # Training Loop\n",
    "    scores = []\n",
    "    losses = []\n",
    "    scores_window = deque(maxlen=config[\"scores_window_size\"])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING {action_type.upper()} TRAINING...\")\n",
    "    \n",
    "    # Use tqdm for progress bar with detailed information\n",
    "    pbar = tqdm(range(1, config[\"episodes\"] + 1), desc=\"Training\", unit=\"episode\")\n",
    "    \n",
    "    for i_episode in pbar:\n",
    "        state, _ = env.reset(seed=config[\"seed\"] + i_episode)\n",
    "        preprocessed = preprocess_state(state)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            current_state = preprocessed.unsqueeze(0)\n",
    "            action = agent.select_action(current_state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            \n",
    "            preprocessed = preprocess_state(next_state)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        loss, grad_norm = agent.update_policy()\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "        losses.append(loss)\n",
    "        scores_window.append(ep_reward)\n",
    "        \n",
    "        # Update tqdm description with current statistics - use config window size consistently\n",
    "        avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "        current_loss = loss if loss is not None else 0.0\n",
    "        \n",
    "        pbar.set_description(\n",
    "            f\"Ep {i_episode:4d} | \"\n",
    "            f\"Score: {ep_reward:6.1f} | \"\n",
    "            f\"Avg({config['scores_window_size']}): {avg_score_window:6.1f} | \"\n",
    "            f\"Loss: {current_loss:8.4f} | \"\n",
    "            f\"GradNorm: {grad_norm:6.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Handle video display - show all videos collected so far\n",
    "        if i_episode % config[\"video_record_interval\"] == 0 and config[\"record_videos\"]:\n",
    "            from rl_utils.environment import display_latest_video\n",
    "            pbar.write(f\"\\nVideo recorded at episode {i_episode}\")\n",
    "            display_latest_video(\n",
    "                config_with_videos[\"video_folder\"], \n",
    "                f\"reinforce_{action_type.lower()}\", \n",
    "                i_episode\n",
    "            )\n",
    "    \n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    # Use config window size for final performance calculation\n",
    "    final_window_size = min(config[\"scores_window_size\"], len(scores))\n",
    "    final_performance = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    print(f\"\\n{action_type} training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Final performance: {final_performance:.2f} (last {final_window_size} episodes)\")\n",
    "    \n",
    "    return scores, losses, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ Displaying 3 training videos (episodes: [110, 221, 332]):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;\">\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 110</p>\n",
       "                <video width=\"300\" height=\"225\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/REINFORCE_discrete/reinforce_discrete-episode-110.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 221</p>\n",
       "                <video width=\"300\" height=\"225\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/REINFORCE_discrete/reinforce_discrete-episode-221.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 332</p>\n",
       "                <video width=\"300\" height=\"225\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/REINFORCE_discrete/reinforce_discrete-episode-332.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "        </div>\n",
       "        <style>\n",
       "            video {\n",
       "                border: 2px solid #ccc;\n",
       "                border-radius: 8px;\n",
       "                box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  333 | Score:  -45.4 | Avg(10):  -60.0 | Loss:   4.9434 | GradNorm: 81.7906:  33%|‚ñà‚ñà‚ñà‚ñé      | 333/1000 [01:16<05:16,  2.11episode/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ 3 training videos available in videos/REINFORCE_discrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  437 | Score:  -91.4 | Avg(10):  -92.1 | Loss:   6.0595 | GradNorm: 99.8287:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 437/1000 [01:44<02:29,  3.76episode/s] "
     ]
    }
   ],
   "source": [
    "# --- DISCRETE ACTION SPACE TRAINING ---\n",
    "print(\"Starting REINFORCE training with DISCRETE actions...\")\n",
    "\n",
    "discrete_scores, discrete_losses, discrete_agent = train_reinforce(is_continuous=False, config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f3277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results with REINFORCE-specific analysis\n",
    "plot_training_results(discrete_scores, discrete_losses, CONFIG, \"Discrete\", algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# Show variance analysis with REINFORCE-specific commentary\n",
    "plot_variance_analysis(discrete_agent, discrete_scores, \"Discrete\", CONFIG, algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# REINFORCE-specific analysis\n",
    "print(f\"\\nüîç REINFORCE DISCRETE ACTION ANALYSIS:\")\n",
    "print(f\"- High variance is expected due to Monte Carlo nature\")\n",
    "print(f\"- Gradient fluctuations show the fundamental REINFORCE challenge\")\n",
    "print(f\"- This demonstrates why variance reduction techniques were developed\")\n",
    "print(f\"- Network parameters: {discrete_agent.policy.get_param_count():,}\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"scores_window_size\"], len(discrete_scores))\n",
    "final_avg = np.mean(discrete_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Training videos saved to: videos/REINFORCE_discrete/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE TRAINING ---\n",
    "print(\"Starting REINFORCE training with CONTINUOUS actions...\")\n",
    "\n",
    "continuous_scores, continuous_losses, continuous_agent = train_reinforce(is_continuous=True, config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results with REINFORCE-specific analysis\n",
    "plot_training_results(continuous_scores, continuous_losses, CONFIG, \"Continuous\", algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# Show variance analysis with REINFORCE-specific commentary\n",
    "plot_variance_analysis(continuous_agent, continuous_scores, \"Continuous\", CONFIG, algorithm_name=\"REINFORCE\")\n",
    "\n",
    "# REINFORCE-specific analysis\n",
    "print(f\"\\nüîç REINFORCE CONTINUOUS ACTION ANALYSIS:\")\n",
    "print(f\"- Gaussian policy creates additional exploration through action noise\")\n",
    "print(f\"- Continuous control often shows different variance patterns than discrete\")\n",
    "print(f\"- Action clipping may introduce bias but maintains environment constraints\")\n",
    "print(f\"- Network parameters: {continuous_agent.policy.get_param_count():,}\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"scores_window_size\"], len(continuous_scores))\n",
    "final_avg = np.mean(continuous_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Training videos saved to: videos/REINFORCE_continuous/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ca7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPARATIVE ANALYSIS ---\n",
    "from rl_utils.visualization import plot_comparison\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPARATIVE ANALYSIS: Discrete vs Continuous REINFORCE\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print parameter counts for comparison\n",
    "print(f\"\\nüìä NETWORK PARAMETER COMPARISON:\")\n",
    "print(f\"Discrete Policy Network:   {discrete_agent.policy.get_param_count():,} parameters\")\n",
    "print(f\"Continuous Policy Network: {continuous_agent.policy.get_param_count():,} parameters\")\n",
    "\n",
    "# Plot comparison with REINFORCE-specific context\n",
    "plot_comparison(\n",
    "    discrete_results=(discrete_scores, discrete_agent),\n",
    "    continuous_results=(continuous_scores, continuous_agent),\n",
    "    config=CONFIG,\n",
    "    algorithm_name=\"REINFORCE\"\n",
    ")\n",
    "\n",
    "# REINFORCE-specific insights\n",
    "print(f\"\\nüî¨ REINFORCE-Specific Insights for {CONFIG['env_id']}:\")\n",
    "print(f\"1. High variance in both action spaces demonstrates the Monte Carlo challenge\")\n",
    "print(f\"2. Gradient instability is fundamental to vanilla policy gradients\")\n",
    "print(f\"3. This variance motivates the development of:\")\n",
    "print(f\"   - Baselines (Actor-Critic methods)\")\n",
    "print(f\"   - Experience replay (off-policy methods)\")\n",
    "print(f\"   - Trust region constraints (PPO)\")\n",
    "print(f\"4. Pure policy gradients show why RL needed algorithmic innovation!\")\n",
    "\n",
    "print(f\"\\nüìÅ All Training Videos Available:\")\n",
    "print(f\"- Discrete action videos: videos/REINFORCE_discrete/\")\n",
    "print(f\"- Continuous action videos: videos/REINFORCE_continuous/\")\n",
    "print(f\"- Videos recorded every {CONFIG['video_record_interval']} episodes during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576a7259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ALGORITHM SUMMARY ---\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"REINFORCE ALGORITHM SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"\\nüéØ Key REINFORCE Characteristics:\")\n",
    "print(f\"- Pure Monte Carlo policy gradient method\")\n",
    "print(f\"- High variance but unbiased gradient estimates\")\n",
    "print(f\"- Episode-based learning (no bootstrapping)\")\n",
    "print(f\"- Works with both discrete and continuous action spaces\")\n",
    "print(f\"- Foundation for all policy gradient methods\")\n",
    "\n",
    "print(f\"\\nüìä Performance Summary:\")\n",
    "window_size = min(CONFIG[\"scores_window_size\"], len(discrete_scores), len(continuous_scores))\n",
    "discrete_final = np.mean(discrete_scores[-window_size:]) if len(discrete_scores) >= window_size else np.mean(discrete_scores)\n",
    "continuous_final = np.mean(continuous_scores[-window_size:]) if len(continuous_scores) >= window_size else np.mean(continuous_scores)\n",
    "print(f\"Final {window_size}-episode average scores:\")\n",
    "print(f\"- Discrete actions:   {discrete_final:6.2f}\")\n",
    "print(f\"- Continuous actions: {continuous_final:6.2f}\")\n",
    "print(f\"- LunarLander target: {CONFIG['target_score']:.2f}\")\n",
    "\n",
    "print(f\"\\nüî¨ Variance Analysis Results:\")\n",
    "discrete_stats = discrete_agent.get_variance_stats()\n",
    "continuous_stats = continuous_agent.get_variance_stats()\n",
    "print(f\"Return standard deviation:\")\n",
    "print(f\"- Discrete:   {discrete_stats['return_std']:6.2f}\")\n",
    "print(f\"- Continuous: {continuous_stats['return_std']:6.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
