{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c475f61b",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO): The Modern Standard\n",
    "\n",
    "## üéØ From Actor-Critic TD to PPO: The Evolution of Policy Gradient Methods\n",
    "\n",
    "Welcome to **Proximal Policy Optimization (PPO)** - the algorithm that has dominated reinforcement learning since 2017 and remains the go-to method even in 2025. PPO represents the culmination of decades of research into stable, sample-efficient policy gradient methods.\n",
    "\n",
    "## üìà The Historical Journey: REINFORCE ‚Üí Actor-Critic ‚Üí TRPO ‚Üí PPO\n",
    "\n",
    "### The Problem with Vanilla Policy Gradients\n",
    "\n",
    "From our previous notebooks, we've seen the progression:\n",
    "\n",
    "1. **REINFORCE**: High variance, simple implementation\n",
    "2. **Actor-Critic MC**: Reduced variance with baselines, but still episode-based\n",
    "3. **Actor-Critic TD**: Bootstrapping for sample efficiency, but training instability\n",
    "\n",
    "**The Core Challenge**: All these methods suffer from **destructive policy updates** - a single bad gradient step can destroy hours of learning progress.\n",
    "\n",
    "### Trust Region Policy Optimization (TRPO): The Breakthrough\n",
    "\n",
    "**TRPO (2015)** solved the destructive update problem with a brilliant insight:\n",
    "\n",
    "**Core Idea**: Constrain policy updates to stay within a \"trust region\" where our gradient estimates are reliable.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\max_\\theta \\mathbb{E}[L^{TRPO}(\\theta)] \\text{, subject to } \\mathbb{E}[KL(\\pi_{\\theta_{old}}, \\pi_\\theta)] \\leq \\delta$$\n",
    "\n",
    "Where:\n",
    "- $L^{TRPO}(\\theta)$ is the surrogate objective (importance sampling)\n",
    "- $KL(\\pi_{\\theta_{old}}, \\pi_\\theta)$ is the KL divergence between old and new policies\n",
    "- $\\delta$ is the trust region constraint\n",
    "\n",
    "**TRPO's Innovation**: \n",
    "- **Monotonic improvement**: Guaranteed to never make the policy worse\n",
    "- **Stable learning**: Prevents destructive updates through KL constraint\n",
    "- **Theoretical guarantees**: Provable convergence properties\n",
    "\n",
    "**TRPO's Fatal Flaw**: \n",
    "- **Computational complexity**: Requires second-order optimization (natural gradients)\n",
    "- **Difficult implementation**: Complex conjugate gradient and line search procedures\n",
    "- **Slow**: Expensive computation per update step\n",
    "\n",
    "### PPO: The Practical Solution\n",
    "\n",
    "**PPO (2017)** achieved TRPO's benefits with a simple, efficient implementation:\n",
    "\n",
    "**Key Insight**: Instead of constraining KL divergence, **clip the objective function** to prevent large updates.\n",
    "\n",
    "**Why PPO Won**:\n",
    "- **Simple implementation**: First-order optimization only\n",
    "- **Computational efficiency**: Fast and scalable\n",
    "- **Robust performance**: Works well across diverse environments\n",
    "- **Stable learning**: Prevents destructive updates like TRPO\n",
    "- **Sample efficiency**: Reuses data through multiple epochs\n",
    "\n",
    "**PPO's Dominance (2017-2025)**:\n",
    "- **OpenAI's choice**: Used for ChatGPT, GPT-4, and other large-scale RL applications\n",
    "- **Industry standard**: Default choice for most RL practitioners\n",
    "- **Research baseline**: Standard comparison algorithm in RL papers\n",
    "- **Continued relevance**: Still the best general-purpose RL algorithm in 2025\n",
    "\n",
    "## üîß PPO's Five Key Innovations\n",
    "\n",
    "PPO builds upon Actor-Critic TD with five crucial improvements:\n",
    "\n",
    "### 1. üéØ Clipped Surrogate Objective\n",
    "\n",
    "**Problem**: Standard policy gradients can make arbitrarily large updates, destroying learning progress.\n",
    "\n",
    "**Solution**: Clip the importance sampling ratio to prevent extreme updates.\n",
    "\n",
    "**Standard Policy Gradient**:\n",
    "$$L^{PG}(\\theta) = \\mathbb{E}[\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} A(s,a)]$$\n",
    "\n",
    "**PPO Clipped Objective**:\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}[\\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)]$$\n",
    "\n",
    "Where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $\\epsilon$ (typically 0.2) is the clipping parameter\n",
    "- $A_t$ is the advantage estimate\n",
    "\n",
    "**Intuition**: \n",
    "- If advantage is positive: prevent ratio from exceeding $1+\\epsilon$\n",
    "- If advantage is negative: prevent ratio from going below $1-\\epsilon$\n",
    "- **Result**: Conservative updates that preserve learning stability\n",
    "\n",
    "### 2. üé≤ Generalized Advantage Estimation (GAE)\n",
    "\n",
    "**Problem**: Actor-Critic TD still has high variance in advantage estimates.\n",
    "\n",
    "**Solution**: Blend multiple n-step returns with exponential weighting.\n",
    "\n",
    "**Standard N-Step Advantage**:\n",
    "$$A_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n V(s_{t+n}) - V(s_t)$$\n",
    "\n",
    "**GAE Formula**:\n",
    "$$A_t^{GAE(\\lambda)} = \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\delta_{t+k}$$\n",
    "\n",
    "Where $\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "**Effective Horizon Example**: With $\\gamma = 0.99$ and $\\lambda = 0.95$, the weights decay as $(\\gamma \\lambda)^k = 0.9405^k$. After ~15 steps, the weight drops to $0.9405^{15} \\approx 0.1$, meaning GAE effectively considers about 15 future steps while theoretically extending to infinity.\n",
    "\n",
    "**Benefits**:\n",
    "- **Bias-variance tradeoff**: $\\lambda=0$ (low variance, high bias) to $\\lambda=1$ (high variance, low bias)\n",
    "- **Flexible**: Can interpolate between TD and Monte Carlo methods\n",
    "- **Efficient**: Exponential weighting reduces computational cost\n",
    "\n",
    "### 3. üåü Entropy Bonus\n",
    "\n",
    "**Problem**: Policies can converge prematurely to suboptimal solutions.\n",
    "\n",
    "**Solution**: Add entropy regularization to encourage exploration.\n",
    "\n",
    "**Total Objective**:\n",
    "$$L^{TOTAL}(\\theta) = L^{CLIP}(\\theta) + c_1 L^{VF}(\\theta) + c_2 S[\\pi_\\theta](s_t)$$\n",
    "\n",
    "Where:\n",
    "- $S[\\pi_\\theta](s_t) = -\\sum_a \\pi_\\theta(a|s_t) \\log \\pi_\\theta(a|s_t)$ is the entropy\n",
    "- $c_2$ is the entropy coefficient (typically 0.01)\n",
    "\n",
    "**Benefits**:\n",
    "- **Exploration**: Prevents premature convergence to deterministic policies\n",
    "- **Stability**: Maintains policy diversity throughout training\n",
    "- **Adaptability**: Automatic annealing as learning progresses\n",
    "\n",
    "### 4. üìö Minibatch Epoch Updates\n",
    "\n",
    "**Problem**: Actor-Critic TD uses each transition only once, wasting valuable data.\n",
    "\n",
    "**Solution**: Reuse rollout data through multiple epochs with minibatch updates.\n",
    "\n",
    "**Data Reuse Strategy**:\n",
    "1. **Collect rollout**: Gather trajectories using current policy\n",
    "2. **Multiple epochs**: Train on the same data for K epochs (typically 4-10)\n",
    "3. **Minibatch updates**: Split data into minibatches for efficient GPU utilization\n",
    "4. **Prevent overfitting**: Clipping and KL penalties prevent over-optimization\n",
    "\n",
    "**GPU Efficiency Advantage**: Unlike previous notebooks that process entire episodes at once, PPO's minibatch approach is **GPU-friendly** - smaller batches fit better in GPU memory and enable parallel processing across multiple samples simultaneously.\n",
    "\n",
    "**Benefits**:\n",
    "- **Sample efficiency**: Better utilization of expensive environment interactions\n",
    "- **Computational efficiency**: Amortizes rollout cost over multiple updates\n",
    "- **Stable learning**: Minibatch updates provide more stable gradients\n",
    "- **Hardware optimization**: GPU parallelization across minibatch samples\n",
    "\n",
    "### 5. ‚úÇÔ∏è Clipped Value Function Loss\n",
    "\n",
    "**Problem**: Value function updates can also be destructive and unstable.\n",
    "\n",
    "**Solution**: Clip value function updates similar to policy updates.\n",
    "\n",
    "**Standard Value Loss**:\n",
    "$$L^{VF}(\\theta) = (V_\\theta(s_t) - V_t^{target})^2$$\n",
    "\n",
    "**PPO Clipped Value Loss**:\n",
    "$$L^{VF}(\\theta) = \\max\\left((V_\\theta(s_t) - V_t^{target})^2, (\\text{clip}(V_\\theta(s_t), V_{old} - \\epsilon_v, V_{old} + \\epsilon_v) - V_t^{target})^2\\right)$$\n",
    "\n",
    "**Why max() and not min()? A Conservative Approach to Clipping**\n",
    "\n",
    "The maximum operation ensures we **never underestimate the true prediction error** when clipping occurs. Here's the detailed reasoning:\n",
    "\n",
    "**Case 1: Clipping doesn't constrain the update**\n",
    "- If $V_{old} - \\epsilon_v < V_\\theta(s_t) < V_{old} + \\epsilon_v$, then clipping has no effect\n",
    "- The clipped value equals the unclipped value: $\\text{clip}(V_\\theta(s_t), ...) = V_\\theta(s_t)$\n",
    "- Both loss terms are identical: $\\max(\\text{same}, \\text{same}) = \\text{same}$\n",
    "- Result: Normal, unclipped loss computation\n",
    "\n",
    "**Case 2: Clipping constrains the update (the critical case)**\n",
    "- The new value prediction $V_\\theta(s_t)$ would move too far from $V_{old}$\n",
    "- Clipping forces: $V_{clipped} = V_{old} \\pm \\epsilon_v$ (boundary value)\n",
    "- Now we have two different loss values to choose from:\n",
    "  - **Unclipped loss**: $(V_\\theta(s_t) - V_t^{target})^2$ (true error)\n",
    "  - **Clipped loss**: $(V_{clipped} - V_t^{target})^2$ (constrained error)\n",
    "\n",
    "**The Conservative Principle**: We take the **maximum** (larger) of these two losses because:\n",
    "\n",
    "1. **Prevent Loss Hiding**: If clipping makes the prediction artificially closer to the target, we don't want to hide this by using the smaller loss\n",
    "2. **Maintain Learning Signal**: The larger loss preserves the magnitude of the error signal for gradient computation\n",
    "3. **Avoid Underfitting**: Using min() would encourage the optimizer to prefer clipped updates even when they're less accurate\n",
    "4. **Consistency Check**: Only allow clipping if it doesn't make the loss artificially small\n",
    "\n",
    "**Example Scenario**:\n",
    "- Target return: $V_t^{target} = 100$\n",
    "- Old value: $V_{old} = 50$\n",
    "- New prediction: $V_\\theta(s_t) = 90$ (moving toward target)\n",
    "- Clipping bound: $\\epsilon_v = 10$\n",
    "- Clipped value: $V_{clipped} = \\min(90, 50 + 10) = 60$\n",
    "\n",
    "Loss comparison:\n",
    "- Unclipped loss: $(90 - 100)^2 = 100$\n",
    "- Clipped loss: $(60 - 100)^2 = 1600$\n",
    "\n",
    "Using max(): We choose 1600 (the larger loss) because the clipped value is actually **further** from the target. This prevents the clipping from artificially reducing the loss signal.\n",
    "\n",
    "**Benefits**:\n",
    "- **Stable value learning**: Prevents large value function updates\n",
    "- **Conservative clipping**: Only clips when it doesn't hide true error\n",
    "- **Consistent with policy clipping**: Unified approach to stability\n",
    "- **Empirical improvement**: Better performance in practice\n",
    "\n",
    "### 6. üìä KL Divergence Monitoring (Not Constraining)\n",
    "\n",
    "**Key Distinction**: Unlike TRPO, PPO doesn't use KL divergence as a **constraint** - it uses it as a **diagnostic tool**.\n",
    "\n",
    "**What is KL Divergence?**\n",
    "The Kullback-Leibler divergence measures how much one probability distribution differs from another:\n",
    "\n",
    "$$KL(\\pi_{\\theta_{old}}, \\pi_\\theta) = \\mathbb{E}_{s \\sim \\rho} \\mathbb{E}_{a \\sim \\pi_{\\theta_{old}}} \\left[ \\log \\frac{\\pi_{\\theta_{old}}(a|s)}{\\pi_\\theta(a|s)} \\right]$$\n",
    "\n",
    "**Intuitive Meaning**:\n",
    "- **KL = 0**: New policy is identical to old policy\n",
    "- **KL > 0**: New policy differs from old policy\n",
    "- **Higher KL**: Larger policy changes\n",
    "\n",
    "**Why Monitor KL in PPO?**\n",
    "\n",
    "1. **Training Health Check**: KL divergence tells us how much the policy is changing each update\n",
    "   - **Healthy range**: 0.001 - 0.01 (modest, stable changes)\n",
    "   - **Too low**: < 0.0001 (learning stagnation)\n",
    "   - **Too high**: > 0.1 (potentially destructive updates)\n",
    "\n",
    "2. **Clipping Effectiveness**: KL helps validate that clipping is working\n",
    "   - If KL is very high despite clipping, something is wrong\n",
    "   - If KL is very low, we might be too conservative\n",
    "\n",
    "3. **Hyperparameter Tuning**: KL guides learning rate and clipping parameter adjustment\n",
    "   - High KL ‚Üí reduce learning rate or decrease clip epsilon\n",
    "   - Low KL ‚Üí increase learning rate or increase clip epsilon\n",
    "\n",
    "4. **Early Stopping**: Some implementations use KL divergence for early stopping\n",
    "   - If KL exceeds a threshold, stop training on current batch\n",
    "   - Prevents over-optimization on stale data\n",
    "\n",
    "**PPO's Approach vs TRPO's Approach**:\n",
    "\n",
    "| Aspect | TRPO | PPO |\n",
    "|--------|------|-----|\n",
    "| **KL Usage** | Hard constraint | Diagnostic monitoring |\n",
    "| **Optimization** | Constrained optimization | Unconstrained with clipping |\n",
    "| **Computational Cost** | Expensive (second-order) | Cheap (first-order) |\n",
    "| **Implementation** | Complex conjugate gradient | Simple gradient descent |\n",
    "| **Robustness** | Sensitive to KL threshold | Robust to hyperparameters |\n",
    "\n",
    "**Real-World Example**:\n",
    "In our implementation, you might see:\n",
    "- **Early training**: KL ‚âà 0.01 (policy learning quickly)\n",
    "- **Mid training**: KL ‚âà 0.005 (policy refining)\n",
    "- **Late training**: KL ‚âà 0.001 (policy converging)\n",
    "\n",
    "**Benefits of KL Monitoring**:\n",
    "- **Debugging**: Identifies training instabilities early\n",
    "- **Validation**: Confirms clipping is preventing destructive updates\n",
    "- **Optimization**: Guides hyperparameter tuning\n",
    "- **Research**: Enables comparison with TRPO and other methods\n",
    "- **Zero overhead**: Computed during normal forward pass\n",
    "\n",
    "**The Bottom Line**: PPO gets TRPO's stability benefits through clipping, but keeps KL monitoring as a \"health check\" - giving us the best of both worlds with minimal computational overhead.\n",
    "\n",
    "## üöÄ Why PPO Dominates (2017-2025)\n",
    "\n",
    "### ‚úÖ PPO's Advantages\n",
    "\n",
    "1. **Simplicity**: Easy to implement and understand\n",
    "2. **Stability**: Robust across diverse environments and hyperparameters\n",
    "3. **Sample Efficiency**: Reuses data effectively through multiple epochs\n",
    "4. **Computational Efficiency**: First-order optimization, GPU-friendly\n",
    "5. **Generality**: Works well for both discrete and continuous control\n",
    "6. **Theoretical Grounding**: Builds on solid policy gradient theory\n",
    "7. **Empirical Success**: Proven track record in complex domains\n",
    "\n",
    "### üèÜ PPO's Real-World Impact\n",
    "\n",
    "**OpenAI's Applications**:\n",
    "- **ChatGPT**: RLHF (Reinforcement Learning from Human Feedback) training\n",
    "- **GPT-4**: Large-scale language model alignment\n",
    "- **OpenAI Five**: Dota 2 championship-level performance\n",
    "- **Robotics**: Real-world robot control and manipulation\n",
    "\n",
    "**Industry Adoption**:\n",
    "- **Default choice**: Most RL practitioners start with PPO\n",
    "- **Production systems**: Widely used in recommendation systems, game AI, autonomous vehicles\n",
    "- **Research standard**: Baseline comparison in academic papers\n",
    "\n",
    "### üìä PPO vs Alternatives (2025 Perspective)\n",
    "\n",
    "**PPO vs SAC (Soft Actor-Critic)**:\n",
    "- **PPO**: Better for discrete actions, more stable, simpler\n",
    "- **SAC**: Better for continuous control, more sample efficient, more complex\n",
    "\n",
    "**PPO vs TD3 (Twin Delayed Deep Deterministic)**:\n",
    "- **PPO**: General-purpose, works with discrete actions\n",
    "- **TD3**: Continuous control only, more sample efficient in some domains\n",
    "\n",
    "**PPO vs Modern Methods**:\n",
    "- **PPO still competitive**: Remains state-of-the-art for many applications\n",
    "- **Simplicity advantage**: Easier to tune and debug than newer methods\n",
    "- **Proven reliability**: Extensive empirical validation across domains\n",
    "\n",
    "## üîÑ PPO Algorithm Overview\n",
    "\n",
    "**Algorithm: Proximal Policy Optimization (PPO)**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Actor-Critic network with parameters $\\theta$\n",
    "- Rollout length $T$\n",
    "- Minibatch size $M$\n",
    "- Number of epochs $K$\n",
    "- Clipping parameter $\\epsilon$\n",
    "- GAE parameter $\\lambda$\n",
    "- Learning rate $\\alpha$\n",
    "\n",
    "**Output:** \n",
    "- Trained policy parameters $\\theta$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** network parameters $\\theta$\n",
    "2. **For** iteration $i = 1, 2, ...$ **do:**\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;**Collect rollout** of length $T$ using policy $\\pi_\\theta$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;**Compute advantages** using GAE with parameter $\\lambda$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;**For** epoch $e = 1, 2, ..., K$ **do:**\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For** each minibatch $B$ of size $M$ **do:**\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute** clipped surrogate objective $L^{CLIP}$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute** clipped value function loss $L^{VF}$\n",
    "9. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute** entropy bonus $S[\\pi_\\theta]$\n",
    "10. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Monitor** KL divergence $KL(\\pi_{\\theta_{old}}, \\pi_\\theta)$ (diagnostic only)\n",
    "11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Total loss**: $L = L^{CLIP} + c_1 L^{VF} + c_2 S[\\pi_\\theta]$\n",
    "12. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Update** $\\theta$ using gradient ascent\n",
    "13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "14. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "15. **End For**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d955ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Global random seeds set to 42 for reproducible results\n",
      "üìù Environment episodes will use seeds 42 + episode_number for varied but reproducible episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our RL utilities including the ActorCriticNetwork\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    ActorCriticNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    ")\n",
    "from rl_utils.visualization import (\n",
    "    get_moving_average,\n",
    "    plot_ppo_training_results,\n",
    "    plot_ppo_variance_analysis,\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 3e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"window_length\": 50,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"num_videos\": 9,  # Number of videos to record during training\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"network\": {\n",
    "        \"fc_out_features\": [64],  # Shared features\n",
    "        \"actor_features\": [64],  # Actor-specific layers after shared\n",
    "        \"critic_features\": [64],  # Critic-specific layers after shared\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.0,  # No dropout for stability\n",
    "    },\n",
    "    # PPO-Specific Parameters\n",
    "    \"rollout_length\": 2048,  # Steps per rollout\n",
    "    \"minibatch_size\": 64,    # Minibatch size for updates\n",
    "    \"epochs\": 10,            # Training epochs per rollout\n",
    "    \"clip_epsilon\": 0.2,     # Clipping parameter\n",
    "    \"value_clip_epsilon\": 0.2,  # Value function clipping\n",
    "    \"critic_loss_coeff\": 0.5,   # Weight for critic loss\n",
    "    \"entropy_coeff\": 0.01,      # Weight for entropy bonus\n",
    "    \"gae_lambda\": 0.95,         # GAE parameter\n",
    "    \"max_grad_norm\": 0.5,       # Maximum gradient norm for clipping\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"üé≤ Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"üìù Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "366e3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization agent with all key innovations.\"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        \"\"\"\n",
    "        Initialize PPO agent.\n",
    "\n",
    "        Args:\n",
    "            network: ActorCriticNetwork instance\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.network = network.to(config[\"device\"])\n",
    "        self.device = config[\"device\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.rollout_length = config[\"rollout_length\"]\n",
    "        self.minibatch_size = config[\"minibatch_size\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.clip_epsilon = config[\"clip_epsilon\"]\n",
    "        self.value_clip_epsilon = config[\"value_clip_epsilon\"]\n",
    "        self.critic_loss_coeff = config[\"critic_loss_coeff\"]\n",
    "        self.entropy_coeff = config[\"entropy_coeff\"]\n",
    "        self.gae_lambda = config[\"gae_lambda\"]\n",
    "        self.max_grad_norm = config[\"max_grad_norm\"]\n",
    "        self.window_size = config.get(\"window_length\")\n",
    "\n",
    "        # Single optimizer for all network parameters\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "        )\n",
    "\n",
    "        # Print detailed network information\n",
    "        print(f\"üìä PPO NETWORK DETAILS:\")\n",
    "        self.network.print_network_info()\n",
    "        print(f\"üéØ Rollout Length: {self.rollout_length}\")\n",
    "        print(f\"üì¶ Minibatch Size: {self.minibatch_size}\")\n",
    "        print(f\"üîÅ Training Epochs: {self.epochs}\")\n",
    "        print(f\"‚úÇÔ∏è Clip Epsilon: {self.clip_epsilon}\")\n",
    "        print(f\"üåü Entropy Coefficient: {self.entropy_coeff}\")\n",
    "        print(f\"üé≤ GAE Lambda: {self.gae_lambda}\")\n",
    "        print(f\"üéì Learning Rate: {config['lr']}\")\n",
    "        print(f\"‚úÇÔ∏è Max Gradient Norm: {self.max_grad_norm}\")\n",
    "        print(f\"‚öñÔ∏è Critic Loss Coefficient: {self.critic_loss_coeff}\")\n",
    "\n",
    "        # Rollout buffer storage\n",
    "        self.reset_rollout_buffer()\n",
    "\n",
    "        # Variance and performance tracking\n",
    "        self.gradient_norms = []\n",
    "        self.episode_returns = []\n",
    "        self.return_variance_history = []\n",
    "\n",
    "        # Update step tracking\n",
    "        self.update_step = 0\n",
    "        self.update_steps_history = []\n",
    "\n",
    "        # Loss component tracking\n",
    "        self.loss_history = {\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"entropy_loss\": [],\n",
    "            \"total_loss\": [],\n",
    "        }\n",
    "\n",
    "        # PPO-specific tracking\n",
    "        self.rollout_count = 0\n",
    "        self.steps_collected = 0\n",
    "        self.policy_updates = 0\n",
    "        self.clip_fraction_history = []\n",
    "        self.kl_divergence_history = []\n",
    "\n",
    "    def reset_rollout_buffer(self):\n",
    "        \"\"\"Reset rollout buffer for new collection.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "        self.current_episode_return = 0.0\n",
    "        self.episode_step_count = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action and store necessary data.\"\"\"\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get policy distribution and value estimate\n",
    "            dist, value = self.network(state)\n",
    "            action = dist.sample()\n",
    "\n",
    "            # Store log probability and value prediction\n",
    "            if self.network.is_continuous:\n",
    "                log_prob = dist.log_prob(action).sum(-1)\n",
    "                action_to_env = self.network.clip_action(action).flatten()\n",
    "            else:\n",
    "                log_prob = dist.log_prob(action)\n",
    "                action_to_env = action.item()\n",
    "\n",
    "        # Store in rollout buffer\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "\n",
    "        self.steps_collected += 1\n",
    "        self.episode_step_count += 1\n",
    "\n",
    "        return action_to_env\n",
    "\n",
    "    def store_transition(self, reward, done):\n",
    "        \"\"\"Store reward and done flag.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.current_episode_return += reward\n",
    "\n",
    "        # If episode ended, store episode return\n",
    "        if done:\n",
    "            self.episode_returns.append(self.current_episode_return)\n",
    "            self.current_episode_return = 0.0\n",
    "            self.episode_step_count = 0\n",
    "\n",
    "    def is_rollout_complete(self):\n",
    "        \"\"\"Check if rollout is complete.\"\"\"\n",
    "        return len(self.states) >= self.rollout_length\n",
    "\n",
    "    def compute_gae_advantages(self, next_value=0.0):\n",
    "        \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
    "        # Convert to tensors\n",
    "        rewards = torch.tensor(self.rewards, dtype=torch.float32, device=self.device)\n",
    "        values = torch.stack(self.values)\n",
    "        dones = torch.tensor(self.dones, dtype=torch.float32, device=self.device)  # Convert to float\n",
    "\n",
    "        # Compute advantages using GAE\n",
    "        advantages = []\n",
    "        gae = 0.0\n",
    "        \n",
    "        # Work backwards through the episode\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                # Last step - use next_value from environment or 0 if terminal\n",
    "                next_value_t = next_value if not self.dones[t] else 0.0\n",
    "            else:\n",
    "                next_value_t = values[t + 1]\n",
    "            \n",
    "            # TD error\n",
    "            delta = rewards[t] + self.gamma * next_value_t * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            # GAE calculation\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        returns = advantages + values.detach()\n",
    "\n",
    "        return advantages, returns\n",
    "\n",
    "    def update_policy(self, next_state=None):\n",
    "        \"\"\"Update policy using PPO algorithm.\"\"\"\n",
    "        if len(self.states) < self.minibatch_size:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0, \"entropy_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        self.rollout_count += 1\n",
    "\n",
    "        # Get next state value for GAE calculation\n",
    "        if next_state is not None:\n",
    "            next_state_tensor = torch.as_tensor(next_state, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.network(next_state_tensor)\n",
    "                next_value = next_value.item()\n",
    "        else:\n",
    "            next_value = 0.0\n",
    "\n",
    "        # Compute advantages and returns using GAE\n",
    "        advantages, returns = self.compute_gae_advantages(next_value)\n",
    "\n",
    "        # Track return variance\n",
    "        if len(self.episode_returns) >= self.window_size:\n",
    "            recent_returns = self.episode_returns[-self.window_size:]\n",
    "            return_variance = np.var(recent_returns)\n",
    "            self.return_variance_history.append(return_variance)\n",
    "\n",
    "        # Convert rollout data to tensors\n",
    "        states = torch.stack(self.states)\n",
    "        actions = torch.stack(self.actions)\n",
    "        old_log_probs = torch.stack(self.log_probs)\n",
    "        old_values = torch.stack(self.values)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Store for reuse across epochs\n",
    "        rollout_data = {\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'old_log_probs': old_log_probs,\n",
    "            'old_values': old_values,\n",
    "            'advantages': advantages,\n",
    "            'returns': returns,\n",
    "        }\n",
    "\n",
    "        # Train for multiple epochs\n",
    "        total_actor_loss = 0.0\n",
    "        total_critic_loss = 0.0\n",
    "        total_entropy_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        total_grad_norm = 0.0\n",
    "        clip_fraction = 0.0\n",
    "        kl_divergence = 0.0\n",
    "        num_updates = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data for each epoch\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start_idx in range(0, len(states), self.minibatch_size):\n",
    "                end_idx = min(start_idx + self.minibatch_size, len(states))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                if len(batch_indices) < self.minibatch_size // 2:  # Skip very small batches\n",
    "                    continue\n",
    "\n",
    "                # Extract minibatch\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_old_values = old_values[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                dist, values = self.network(batch_states)\n",
    "                \n",
    "                # Compute new log probabilities\n",
    "                if self.network.is_continuous:\n",
    "                    new_log_probs = dist.log_prob(batch_actions).sum(-1)\n",
    "                else:\n",
    "                    new_log_probs = dist.log_prob(batch_actions)\n",
    "\n",
    "                # Compute probability ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "\n",
    "                # Clipped surrogate objective\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # Clipped value function loss\n",
    "                value_pred_clipped = batch_old_values + torch.clamp(\n",
    "                    values - batch_old_values, -self.value_clip_epsilon, self.value_clip_epsilon\n",
    "                )\n",
    "                value_loss_1 = (values - batch_returns).pow(2)\n",
    "                value_loss_2 = (value_pred_clipped - batch_returns).pow(2)\n",
    "                critic_loss = 0.5 * torch.max(value_loss_1, value_loss_2).mean()\n",
    "\n",
    "                # critic loss calculation without clipping. try this.\n",
    "                # critic_loss = 0.5 * (values - batch_returns).pow(2).mean()  \n",
    "\n",
    "                # Entropy bonus\n",
    "                entropy = dist.entropy()\n",
    "                if entropy.dim() > 1:\n",
    "                    entropy = entropy.sum(-1)\n",
    "                entropy_loss = -entropy.mean()\n",
    "\n",
    "                # Total loss\n",
    "                total_loss_batch = actor_loss + self.critic_loss_coeff * critic_loss + self.entropy_coeff * entropy_loss\n",
    "\n",
    "                # Update network\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss_batch.backward()\n",
    "\n",
    "                # Record gradient norm BEFORE clipping\n",
    "                grad_norm = 0.0\n",
    "                for param in self.network.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param_norm = param.grad.data.norm(2)\n",
    "                        grad_norm += param_norm.item() ** 2\n",
    "                grad_norm = grad_norm ** 0.5\n",
    "\n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Accumulate statistics\n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                total_entropy_loss += entropy_loss.item()\n",
    "                total_loss += total_loss_batch.item()\n",
    "                total_grad_norm += grad_norm\n",
    "                \n",
    "                # Track clipping and KL divergence\n",
    "                with torch.no_grad():\n",
    "                    clipped = torch.abs(ratio - 1.0) > self.clip_epsilon\n",
    "                    clip_fraction += clipped.float().mean().item()\n",
    "                    kl_divergence += (batch_old_log_probs - new_log_probs).mean().item()\n",
    "\n",
    "                num_updates += 1\n",
    "\n",
    "        # Average statistics\n",
    "        if num_updates > 0:\n",
    "            avg_actor_loss = total_actor_loss / num_updates\n",
    "            avg_critic_loss = total_critic_loss / num_updates\n",
    "            avg_entropy_loss = total_entropy_loss / num_updates\n",
    "            avg_total_loss = total_loss / num_updates\n",
    "            avg_grad_norm = total_grad_norm / num_updates\n",
    "            avg_clip_fraction = clip_fraction / num_updates\n",
    "            avg_kl_divergence = kl_divergence / num_updates\n",
    "        else:\n",
    "            avg_actor_loss = avg_critic_loss = avg_entropy_loss = avg_total_loss = avg_grad_norm = 0.0\n",
    "            avg_clip_fraction = avg_kl_divergence = 0.0\n",
    "\n",
    "        # Store statistics\n",
    "        self.update_step += 1\n",
    "        self.policy_updates += num_updates\n",
    "        self.gradient_norms.append(avg_grad_norm)\n",
    "        self.loss_history[\"actor_loss\"].append(avg_actor_loss)\n",
    "        self.loss_history[\"critic_loss\"].append(avg_critic_loss)\n",
    "        self.loss_history[\"entropy_loss\"].append(avg_entropy_loss)\n",
    "        self.loss_history[\"total_loss\"].append(avg_total_loss)\n",
    "        self.update_steps_history.append(self.update_step)\n",
    "        self.clip_fraction_history.append(avg_clip_fraction)\n",
    "        self.kl_divergence_history.append(avg_kl_divergence)\n",
    "\n",
    "        # Reset rollout buffer\n",
    "        self.reset_rollout_buffer()\n",
    "\n",
    "        return {\n",
    "            \"actor_loss\": avg_actor_loss,\n",
    "            \"critic_loss\": avg_critic_loss,\n",
    "            \"entropy_loss\": avg_entropy_loss,\n",
    "            \"total_loss\": avg_total_loss,\n",
    "        }, avg_grad_norm\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.episode_returns) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"return_mean\": 0.0,\n",
    "                \"return_std\": 0.0,\n",
    "                \"recent_return_variance\": 0.0,\n",
    "                \"clip_fraction_mean\": 0.0,\n",
    "                \"kl_divergence_mean\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"return_mean\": np.mean(self.episode_returns),\n",
    "            \"return_std\": np.std(self.episode_returns),\n",
    "            \"recent_return_variance\": (\n",
    "                self.return_variance_history[-1]\n",
    "                if self.return_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "            \"clip_fraction_mean\": np.mean(self.clip_fraction_history) if self.clip_fraction_history else 0.0,\n",
    "            \"kl_divergence_mean\": np.mean(self.kl_divergence_history) if self.kl_divergence_history else 0.0,\n",
    "        }\n",
    "\n",
    "    def get_ppo_stats(self):\n",
    "        \"\"\"Get PPO-specific statistics.\"\"\"\n",
    "        return {\n",
    "            \"rollout_count\": self.rollout_count,\n",
    "            \"steps_collected\": self.steps_collected,\n",
    "            \"policy_updates\": self.policy_updates,\n",
    "            \"updates_per_rollout\": self.policy_updates / max(1, self.rollout_count),\n",
    "            \"steps_per_update\": self.steps_collected / max(1, self.policy_updates),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afdb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(is_continuous, config):\n",
    "    \"\"\"Main training loop for the PPO agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROXIMAL POLICY OPTIMIZATION (PPO) - {action_type.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate video recording interval\n",
    "    video_record_interval = max(1, config[\"episodes\"] // config[\"num_videos\"])\n",
    "    print(f\"üìπ Recording {config['num_videos']} videos every {video_record_interval} episodes\")\n",
    "    \n",
    "    # Create algorithm-specific video folder\n",
    "    video_folder = f\"videos/PPO_{action_type.lower()}\"\n",
    "    config_with_videos = config.copy()\n",
    "    config_with_videos[\"video_folder\"] = video_folder\n",
    "    config_with_videos[\"video_record_interval\"] = video_record_interval\n",
    "    \n",
    "    # Create Environment\n",
    "    env = create_env_with_wrappers(\n",
    "        config_with_videos, \n",
    "        is_continuous, \n",
    "        record_videos=True, \n",
    "        video_prefix=f\"ppo_{action_type.lower()}\",\n",
    "        cleanup_existing=True\n",
    "    )\n",
    "    \n",
    "    # Get observation dimension and space\n",
    "    dummy_obs, _ = env.reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "    \n",
    "    # Create Actor-Critic Network and Agent\n",
    "    print(f\"\\nüèóÔ∏è CREATING {action_type.upper()} ACTOR-CRITIC NETWORK:\")\n",
    "    network = ActorCriticNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=env.action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"network\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ INITIALIZING {action_type.upper()} PPO AGENT:\")\n",
    "    agent = PPOAgent(network, config)\n",
    "    \n",
    "    # Training Loop\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=config[\"window_length\"])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING {action_type.upper()} PPO TRAINING...\")\n",
    "    \n",
    "    # Initialize environment\n",
    "    state, _ = env.reset(seed=config[\"seed\"])\n",
    "    episode_count = 0\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    pbar = tqdm(total=config[\"episodes\"], desc=\"Training\", unit=\"episode\")\n",
    "    \n",
    "    while episode_count < config[\"episodes\"]:\n",
    "        # Collect rollout\n",
    "        while not agent.is_rollout_complete():\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_transition(reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                # Episode finished\n",
    "                episode_count += 1\n",
    "                episode_reward = agent.episode_returns[-1] if agent.episode_returns else 0.0\n",
    "                scores.append(episode_reward)\n",
    "                scores_window.append(episode_reward)\n",
    "                \n",
    "                # Reset environment\n",
    "                state, _ = env.reset(seed=config[\"seed\"] + episode_count)\n",
    "                \n",
    "                # Update progress bar\n",
    "                avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "                \n",
    "                # Get latest loss values and metrics\n",
    "                actor_loss = agent.loss_history[\"actor_loss\"][-1] if agent.loss_history[\"actor_loss\"] else 0.0\n",
    "                critic_loss = agent.loss_history[\"critic_loss\"][-1] if agent.loss_history[\"critic_loss\"] else 0.0\n",
    "                entropy_loss = agent.loss_history[\"entropy_loss\"][-1] if agent.loss_history[\"entropy_loss\"] else 0.0\n",
    "                total_loss = agent.loss_history[\"total_loss\"][-1] if agent.loss_history[\"total_loss\"] else 0.0\n",
    "                grad_norm = agent.gradient_norms[-1] if agent.gradient_norms else 0.0\n",
    "                \n",
    "                # Get PPO-specific metrics\n",
    "                clip_fraction = agent.clip_fraction_history[-1] if agent.clip_fraction_history else 0.0\n",
    "                kl_divergence = agent.kl_divergence_history[-1] if agent.kl_divergence_history else 0.0\n",
    "                \n",
    "                pbar.set_description(\n",
    "                    f\"Ep {episode_count:4d} | \"\n",
    "                    f\"Score: {episode_reward:6.1f} | \"\n",
    "                    f\"Avg({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "                    f\"Rollouts: {agent.rollout_count:3d} | \"\n",
    "                    f\"Updates: {agent.policy_updates:4d} | \"\n",
    "                    f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "                    f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "                    f\"EntLoss: {entropy_loss:7.4f} | \"\n",
    "                    f\"TotalLoss: {total_loss:7.4f} | \"\n",
    "                    f\"GradNorm: {grad_norm:6.4f} | \"\n",
    "                    f\"ClipFrac: {clip_fraction:5.3f} | \"\n",
    "                    f\"KL: {kl_divergence:7.4f}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Handle video display\n",
    "                if episode_count % video_record_interval == 0 and config[\"record_videos\"]:\n",
    "                    from rl_utils.environment import display_latest_video\n",
    "                    pbar.write(f\"\\nVideo recorded at episode {episode_count}\")\n",
    "                    display_latest_video(\n",
    "                        config_with_videos[\"video_folder\"], \n",
    "                        f\"ppo_{action_type.lower()}\", \n",
    "                        episode_count\n",
    "                    )\n",
    "                \n",
    "                # Stop if we've reached the target number of episodes\n",
    "                if episode_count >= config[\"episodes\"]:\n",
    "                    break\n",
    "        \n",
    "        # Update policy when rollout is complete\n",
    "        if agent.is_rollout_complete():\n",
    "            loss_dict, grad_norm = agent.update_policy(state)\n",
    "            \n",
    "            # Update progress information with latest metrics\n",
    "            avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "            actor_loss = loss_dict.get('actor_loss', 0.0)\n",
    "            critic_loss = loss_dict.get('critic_loss', 0.0)\n",
    "            entropy_loss = loss_dict.get('entropy_loss', 0.0)\n",
    "            total_loss = loss_dict.get('total_loss', 0.0)\n",
    "            \n",
    "            # Get PPO-specific stats\n",
    "            variance_stats = agent.get_variance_stats()\n",
    "            clip_fraction = variance_stats.get('clip_fraction_mean', 0.0)\n",
    "            kl_divergence = variance_stats.get('kl_divergence_mean', 0.0)\n",
    "            \n",
    "            pbar.set_description(\n",
    "                f\"Ep {episode_count:4d} | \"\n",
    "                f\"Score: {scores[-1] if scores else 0.0:6.1f} | \"\n",
    "                f\"Avg({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "                f\"Rollouts: {agent.rollout_count:3d} | \"\n",
    "                f\"Updates: {agent.policy_updates:4d} | \"\n",
    "                f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "                f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "                f\"EntLoss: {entropy_loss:7.4f} | \"\n",
    "                f\"TotalLoss: {total_loss:7.4f} | \"\n",
    "                f\"GradNorm: {grad_norm:6.4f} | \"\n",
    "                f\"ClipFrac: {clip_fraction:5.3f} | \"\n",
    "                f\"KL: {kl_divergence:7.4f}\"\n",
    "            )\n",
    "    \n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_window_size = min(config[\"window_length\"], len(scores))\n",
    "    final_performance = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    \n",
    "    # Print PPO-specific statistics\n",
    "    ppo_stats = agent.get_ppo_stats()\n",
    "    print(f\"\\n{action_type} PPO training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Final performance: {final_performance:.2f} (last {final_window_size} episodes)\")\n",
    "    print(f\"Total rollouts: {ppo_stats['rollout_count']}\")\n",
    "    print(f\"Total policy updates: {ppo_stats['policy_updates']}\")\n",
    "    print(f\"Steps collected: {ppo_stats['steps_collected']}\")\n",
    "    print(f\"Updates per rollout: {ppo_stats['updates_per_rollout']:.1f}\")\n",
    "    print(f\"Steps per update: {ppo_stats['steps_per_update']:.1f}\")\n",
    "    \n",
    "    return scores, agent.loss_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe95954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ Displaying 1 training videos (episodes: [110]):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;\">\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 110</p>\n",
       "                <video width=\"900\" height=\"675\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/PPO_discrete/ppo_discrete-episode-110.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "        </div>\n",
       "        <style>\n",
       "            video {\n",
       "                border: 2px solid #ccc;\n",
       "                border-radius: 8px;\n",
       "                box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ 1 training videos available in videos/PPO_discrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  215 | Score:  -82.6 | Avg(50):  -91.1 | Rollouts:  12 | Updates: 3840 | ActorLoss: -0.0083 | CriticLoss: 391.5306 | EntLoss: -1.1215 | TotalLoss: 195.7458 | GradNorm: 9.3724 | ClipFrac: 0.076 | KL:  0.0057:  22%|‚ñà‚ñà‚ñè       | 215/1000 [01:56<11:37,  1.13episode/s]"
     ]
    }
   ],
   "source": [
    "# --- DISCRETE ACTION SPACE: PPO ---\n",
    "print(\"Starting PPO training with DISCRETE actions...\")\n",
    "\n",
    "discrete_ppo_scores, discrete_ppo_losses, discrete_ppo_agent = train_ppo(\n",
    "    is_continuous=False, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for discrete PPO\n",
    "plot_ppo_training_results(\n",
    "    discrete_ppo_scores, \n",
    "    discrete_ppo_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Discrete\", \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_ppo_variance_analysis(\n",
    "    discrete_ppo_agent, \n",
    "    discrete_ppo_scores, \n",
    "    \"Discrete\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# PPO-specific analysis\n",
    "variance_stats = discrete_ppo_agent.get_variance_stats()\n",
    "ppo_stats = discrete_ppo_agent.get_ppo_stats()\n",
    "\n",
    "print(f\"\\nüîç DISCRETE PPO ANALYSIS:\")\n",
    "print(f\"Clip fraction: {variance_stats['clip_fraction_mean']:.3f} (target: ~{CONFIG['clip_epsilon']:.1f})\")\n",
    "print(f\"KL divergence: {variance_stats['kl_divergence_mean']:.4f}\")\n",
    "print(f\"Data efficiency: {ppo_stats['steps_per_update']:.1f} env steps per policy update\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_ppo_scores))\n",
    "final_avg = np.mean(discrete_ppo_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE PPO TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_ppo_agent.network.get_param_count():,}\")\n",
    "print(f\"Total rollouts: {ppo_stats['rollout_count']}\")\n",
    "print(f\"Total policy updates: {ppo_stats['policy_updates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b61a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE: PPO ---\n",
    "print(\"Starting PPO training with CONTINUOUS actions...\")\n",
    "\n",
    "continuous_ppo_scores, continuous_ppo_losses, continuous_ppo_agent = train_ppo(\n",
    "    is_continuous=True, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for continuous PPO\n",
    "plot_ppo_training_results(\n",
    "    continuous_ppo_scores, \n",
    "    continuous_ppo_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Continuous\", \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_ppo_variance_analysis(\n",
    "    continuous_ppo_agent, \n",
    "    continuous_ppo_scores, \n",
    "    \"Continuous\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# PPO-specific analysis\n",
    "variance_stats = continuous_ppo_agent.get_variance_stats()\n",
    "ppo_stats = continuous_ppo_agent.get_ppo_stats()\n",
    "\n",
    "print(f\"\\nüîç CONTINUOUS PPO ANALYSIS:\")\n",
    "print(f\"Clip fraction: {variance_stats['clip_fraction_mean']:.3f} (target: ~{CONFIG['clip_epsilon']:.1f})\")\n",
    "print(f\"KL divergence: {variance_stats['kl_divergence_mean']:.4f}\")\n",
    "print(f\"Data efficiency: {ppo_stats['steps_per_update']:.1f} env steps per policy update\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_ppo_scores))\n",
    "final_avg = np.mean(continuous_ppo_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS PPO TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_ppo_agent.network.get_param_count():,}\")\n",
    "print(f\"Total rollouts: {ppo_stats['rollout_count']}\")\n",
    "print(f\"Total policy updates: {ppo_stats['policy_updates']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18022237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPREHENSIVE ANALYSIS: PPO Performance ---\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_utils.visualization import get_moving_average\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE ANALYSIS: PPO Performance and Efficiency\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# PPO-specific comparison\n",
    "experiments = [\n",
    "    (\"Discrete PPO\", discrete_ppo_scores, discrete_ppo_agent),\n",
    "    (\"Continuous PPO\", continuous_ppo_scores, continuous_ppo_agent),\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä PPO PERFORMANCE COMPARISON:\")\n",
    "print(f\"{'Method':<20} {'Final Score':<12} {'Rollouts':<10} {'Updates':<10} {'Efficiency':<15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for name, scores, agent in experiments:\n",
    "    final_window_size = min(CONFIG[\"window_length\"], len(scores))\n",
    "    final_avg = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    ppo_stats = agent.get_ppo_stats()\n",
    "    variance_stats = agent.get_variance_stats()\n",
    "    \n",
    "    print(f\"{name:<20} {final_avg:<12.1f} {ppo_stats['rollout_count']:<10} {ppo_stats['policy_updates']:<10} {ppo_stats['steps_per_update']:<15.1f}\")\n",
    "\n",
    "# Print PPO efficiency analysis\n",
    "print(f\"\\nüîç PPO EFFICIENCY ANALYSIS:\")\n",
    "print(f\"PPO's key advantage: Data reuse through multiple epochs\")\n",
    "print(f\"- Each rollout is trained on {CONFIG['epochs']} times\")\n",
    "print(f\"- Minibatch size: {CONFIG['minibatch_size']} (enables GPU efficiency)\")\n",
    "print(f\"- Rollout length: {CONFIG['rollout_length']} (balances sample efficiency and memory)\")\n",
    "\n",
    "print(f\"\\nüìà PPO STABILITY INDICATORS:\")\n",
    "for name, scores, agent in experiments:\n",
    "    variance_stats = agent.get_variance_stats()\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Clip fraction: {variance_stats['clip_fraction_mean']:.3f} (healthy: ~0.1-0.3)\")\n",
    "    print(f\"  KL divergence: {variance_stats['kl_divergence_mean']:.4f} (healthy: <0.01)\")\n",
    "    print(f\"  Return variance: {variance_stats['recent_return_variance']:.2f}\")\n",
    "\n",
    "# Create PPO-specific analysis plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle('PPO Analysis: Performance and Training Dynamics', fontsize=16)\n",
    "\n",
    "colors = ['blue', 'red']\n",
    "smoothing_window = CONFIG[\"window_length\"]\n",
    "\n",
    "# 1. Performance comparison (episodes)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(scores) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(scores, window=smoothing_window)\n",
    "        episodes = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax1.plot(episodes, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax1.axhline(y=CONFIG[\"target_score\"], color='g', linestyle='--', label=f'Target ({CONFIG[\"target_score\"]})', alpha=0.7)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel(f'Score ({smoothing_window}-episode avg)')\n",
    "ax1.set_title('PPO Performance Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Clip fraction over time (rollouts)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.clip_fraction_history) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.clip_fraction_history, window=smoothing_window)\n",
    "        rollouts = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax2.plot(rollouts, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax2.axhline(y=CONFIG[\"clip_epsilon\"], color='orange', linestyle='--', label=f'Clip Epsilon ({CONFIG[\"clip_epsilon\"]})', alpha=0.7)\n",
    "ax2.set_xlabel('Rollout')\n",
    "ax2.set_ylabel('Clip Fraction')\n",
    "ax2.set_title('Policy Update Clipping Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. KL divergence over time (rollouts)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.kl_divergence_history) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.kl_divergence_history, window=smoothing_window)\n",
    "        rollouts = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax3.plot(rollouts, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Rollout')\n",
    "ax3.set_ylabel('KL Divergence')\n",
    "ax3.set_title('Policy Change Over Time')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Loss components over time (rollouts)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.loss_history[\"total_loss\"]) >= smoothing_window:\n",
    "        # Plot total loss\n",
    "        smoothed, offset = get_moving_average(agent.loss_history[\"total_loss\"], window=smoothing_window)\n",
    "        rollouts = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax4.plot(rollouts, smoothed, label=f'{name} Total', color=colors[i], linewidth=2)\n",
    "        \n",
    "        # Plot actor loss with transparency\n",
    "        if len(agent.loss_history[\"actor_loss\"]) >= smoothing_window:\n",
    "            actor_smoothed, _ = get_moving_average(agent.loss_history[\"actor_loss\"], window=smoothing_window)\n",
    "            ax4.plot(rollouts, actor_smoothed, label=f'{name} Actor', color=colors[i], alpha=0.6, linestyle='--')\n",
    "        \n",
    "        # Plot critic loss with transparency\n",
    "        if len(agent.loss_history[\"critic_loss\"]) >= smoothing_window:\n",
    "            critic_smoothed, _ = get_moving_average(agent.loss_history[\"critic_loss\"], window=smoothing_window)\n",
    "            ax4.plot(rollouts, critic_smoothed, label=f'{name} Critic', color=colors[i], alpha=0.6, linestyle=':')\n",
    "\n",
    "ax4.set_xlabel('Rollout')\n",
    "ax4.set_ylabel('Loss Value')\n",
    "ax4.set_title('Loss Components Over Time')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
