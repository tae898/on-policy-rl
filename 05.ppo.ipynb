{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c475f61b",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO): The Modern Standard\n",
    "\n",
    "## ðŸŽ¯ From Actor-Critic TD to PPO: The Evolution of Policy Gradient Methods\n",
    "\n",
    "Welcome to **Proximal Policy Optimization (PPO)** - the algorithm that has dominated reinforcement learning since 2017 and remains the go-to method even in 2025. PPO represents the culmination of decades of research into stable, sample-efficient policy gradient methods.\n",
    "\n",
    "## ðŸ“ˆ The Historical Journey: REINFORCE â†’ Actor-Critic â†’ TRPO â†’ PPO\n",
    "\n",
    "### The Problem with Vanilla Policy Gradients\n",
    "\n",
    "From our previous notebooks, we've seen the progression:\n",
    "\n",
    "1. **REINFORCE**: High variance, simple implementation\n",
    "2. **Actor-Critic MC**: Reduced variance with baselines, but still episode-based\n",
    "3. **Actor-Critic TD**: Bootstrapping for sample efficiency, but training instability\n",
    "4. **A2C**: Added parallel environments for stability and speed\n",
    "\n",
    "**The Core Challenge**: All these methods suffer from **destructive policy updates** - a single bad gradient step can destroy hours of learning progress.\n",
    "\n",
    "### Trust Region Policy Optimization (TRPO): The Breakthrough\n",
    "\n",
    "**TRPO (2015)** solved the destructive update problem with a brilliant insight:\n",
    "\n",
    "**Core Idea**: Constrain policy updates to stay within a \"trust region\" where our gradient estimates are reliable.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$\\min_\\theta -\\mathbb{E}[L_{\\text{maximize}}^{TRPO}(\\theta)] \\text{, subject to } \\mathbb{E}[KL(\\pi_{\\theta_{old}}, \\pi_\\theta)] \\leq \\delta$$\n",
    "\n",
    "Where:\n",
    "- $L_{\\text{maximize}}^{TRPO}(\\theta)$ is the surrogate objective (importance sampling)\n",
    "- $KL(\\pi_{\\theta_{old}}, \\pi_\\theta)$ is the KL divergence between old and new policies\n",
    "- $\\delta$ is the trust region constraint\n",
    "\n",
    "**TRPO's Innovation**: \n",
    "- **Monotonic improvement**: Guaranteed to never make the policy worse\n",
    "- **Stable learning**: Prevents destructive updates through KL constraint\n",
    "- **Theoretical guarantees**: Provable convergence properties\n",
    "\n",
    "**TRPO's Fatal Flaw**: \n",
    "- **Computational complexity**: Requires second-order optimization (natural gradients)\n",
    "- **Difficult implementation**: Complex conjugate gradient and line search procedures\n",
    "- **Slow**: Expensive computation per update step\n",
    "\n",
    "### PPO: The Practical Solution (Similar to A3C â†’ A2C Evolution)\n",
    "\n",
    "**PPO (2017)** achieved TRPO's benefits with a simple, efficient implementation, following a similar pattern to how **A2C simplified A3C**:\n",
    "\n",
    "**The Simplification Pattern in RL**:\n",
    "- **A3C â†’ A2C**: Asynchronous complexity â†’ Synchronous simplicity\n",
    "- **TRPO â†’ PPO**: Constrained optimization complexity â†’ Clipped objective simplicity\n",
    "\n",
    "**Key Insight**: Instead of constraining KL divergence, **clip the objective function** to prevent large updates.\n",
    "\n",
    "**Why PPO Won (Echoing A2C's Success)**:\n",
    "- **Simple implementation**: First-order optimization only (like A2C's synchronous updates)\n",
    "- **Computational efficiency**: Fast and scalable (like A2C's GPU-friendly batching)\n",
    "- **Robust performance**: Works well across diverse environments\n",
    "- **Stable learning**: Prevents destructive updates like TRPO\n",
    "- **Sample efficiency**: Reuses data through multiple epochs\n",
    "- **Engineering principle**: **Simpler objectives often win in computer science**\n",
    "\n",
    "**PPO's Dominance (2017-2025)**:\n",
    "- **OpenAI's choice**: Used for ChatGPT, GPT-4, and other large-scale RL applications\n",
    "- **Industry standard**: Default choice for most RL practitioners\n",
    "- **Research baseline**: Standard comparison algorithm in RL papers\n",
    "- **Continued relevance**: Still the best general-purpose RL algorithm in 2025\n",
    "\n",
    "## ðŸ”§ PPO's Key Innovations\n",
    "\n",
    "PPO builds upon Actor-Critic TD with parallel environments (like A2C) and adds crucial stability improvements:\n",
    "\n",
    "### 1. ðŸŽ¯ Clipped Surrogate Objective\n",
    "\n",
    "**Problem**: Standard policy gradients can make arbitrarily large updates, destroying learning progress.\n",
    "\n",
    "**Why Do We Need the Probability Ratio? Understanding Importance Sampling**\n",
    "\n",
    "**The Data Reuse Problem**: In standard Actor-Critic TD, we collect data with policy $\\pi_{\\theta_{old}}$ but want to update to $\\pi_\\theta$. When we reuse this \"old\" data for multiple training epochs, we're evaluating a **different policy** than the one that generated the data.\n",
    "\n",
    "**Standard Policy Gradient (On-Policy)**:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho_{\\pi_\\theta}, a \\sim \\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) A^{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "**The Challenge**: This expectation assumes data comes from the **current** policy $\\pi_\\theta$, but our data comes from the **old** policy $\\pi_{\\theta_{old}}$.\n",
    "\n",
    "**Importance Sampling to the Rescue**: We can correct for this mismatch using importance sampling - a technique that lets us estimate expectations under one distribution using samples from another.\n",
    "\n",
    "**Importance Sampling Formula**:\n",
    "$$\\mathbb{E}_{x \\sim p}[f(x)] = \\mathbb{E}_{x \\sim q}\\left[\\frac{p(x)}{q(x)} f(x)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $p(x)$ is the distribution we want to estimate from\n",
    "- $q(x)$ is the distribution we actually have samples from\n",
    "- $\\frac{p(x)}{q(x)}$ is the **importance weight** that corrects the bias\n",
    "\n",
    "**Applied to Policy Gradients**:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim \\rho, a \\sim \\pi_{\\theta_{old}}}\\left[\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_{old}}(a|s)} \\nabla_\\theta \\log \\pi_\\theta(a|s) A(s,a)\\right]$$\n",
    "\n",
    "**The Probability Ratio**:\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "**Intuitive Meaning of the Ratio**:\n",
    "- **$r_t = 1.0$**: New policy assigns same probability as old policy (no change)\n",
    "- **$r_t > 1.0$**: New policy more likely to take this action than old policy\n",
    "- **$r_t < 1.0$**: New policy less likely to take this action than old policy\n",
    "\n",
    "**Why This Works - The Learning Signal Mechanism**: \n",
    "\n",
    "The objective we optimize is: $L = \\mathbb{E}[r_t(\\theta) \\cdot A_t]$\n",
    "\n",
    "**Case 1: Good Action (Positive Advantage)**\n",
    "- $A_t > 0$ means this action was better than expected\n",
    "- We want to **increase** the probability of taking this action\n",
    "- If $r_t > 1$: New policy already favors this action â†’ $r_t \\cdot A_t > A_t$ â†’ **strong positive signal**\n",
    "- If $r_t < 1$: New policy disfavors this action â†’ $r_t \\cdot A_t < A_t$ â†’ **weak positive signal**\n",
    "- **Result**: Gradient pushes to increase probability of good actions\n",
    "\n",
    "**Case 2: Bad Action (Negative Advantage)**\n",
    "- $A_t < 0$ means this action was worse than expected  \n",
    "- We want to **decrease** the probability of taking this action\n",
    "- If $r_t > 1$: New policy favors bad action â†’ $r_t \\cdot A_t <$ (more negative) â†’ **strong negative signal**\n",
    "- If $r_t < 1$: New policy already disfavors bad action â†’ $r_t \\cdot A_t >$ (less negative) â†’ **weak negative signal**\n",
    "- **Result**: Gradient pushes to decrease probability of bad actions\n",
    "\n",
    "**Concrete Example**:\n",
    "- **Action**: Move left in state S\n",
    "- **Old policy**: $\\pi_{\\text{old}}(\\text{left}|S) = 0.2$ (20% probability)\n",
    "- **Advantage**: $A = +5$ (good action!)\n",
    "- **New policy option 1**: $\\pi_{\\text{new}}(\\text{left}|S) = 0.4$ â†’ $r_t = 0.4/0.2 = 2.0$\n",
    "- **New policy option 2**: $\\pi_{\\text{new}}(\\text{left}|S) = 0.1$ â†’ $r_t = 0.1/0.2 = 0.5$\n",
    "\n",
    "**Learning signals**:\n",
    "- **Option 1**: $2.0 \\times (+5) = +10$ (strong positive gradient)\n",
    "- **Option 2**: $0.5 \\times (+5) = +2.5$ (weak positive gradient)\n",
    "\n",
    "**The magic**: Option 1 gets rewarded more because it's **already moving in the right direction** (increasing probability of the good action), while Option 2 gets a weaker signal because it's moving the wrong way.\n",
    "\n",
    "**Standard Policy Gradient with Importance Sampling (as a loss to minimize)**:\n",
    "$$L^{PG}(\\theta) = -\\mathbb{E}\\left[\\sum_{t=0}^{T-1} r_t(\\theta) A_t\\right]$$\n",
    "\n",
    "**The Problem with Unbounded Ratios**: If $r_t(\\theta)$ becomes very large (policy changes dramatically), the gradient can become unstable and destructive.\n",
    "\n",
    "**PPO's Solution: Clip the Ratio**\n",
    "\n",
    "Instead of letting the ratio go to infinity, PPO **clips** it to a safe range:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = -\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)\\right]$$\n",
    "\n",
    "**Step-by-Step Breakdown**:\n",
    "\n",
    "1. **Compute probability ratio**: $r_t = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$\n",
    "\n",
    "2. **Create clipped version**: $r_t^{clipped} = \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)$\n",
    "   - If $r_t < 1-\\epsilon$: $r_t^{clipped} = 1-\\epsilon$\n",
    "   - If $r_t > 1+\\epsilon$: $r_t^{clipped} = 1+\\epsilon$  \n",
    "   - Otherwise: $r_t^{clipped} = r_t$\n",
    "\n",
    "3. **Compute two objectives**:\n",
    "   - **Original**: $r_t \\cdot A_t$ (importance-sampled objective)\n",
    "   - **Clipped**: $r_t^{clipped} \\cdot A_t$ (conservative objective)\n",
    "\n",
    "4. **Take the minimum**: $\\min(r_t A_t, r_t^{clipped} A_t)$\n",
    "\n",
    "**Why Take the Minimum? The Conservative Principle**\n",
    "\n",
    "The $\\min()$ operation implements a **pessimistic** strategy:\n",
    "\n",
    "**Case 1: Positive Advantage ($A_t > 0$)**\n",
    "- **Good action**: We want to increase its probability\n",
    "- **If $r_t > 1+\\epsilon$**: Clipping prevents excessive increase\n",
    "- **Intuition**: \"Don't get too excited about good actions\"\n",
    "\n",
    "**Case 2: Negative Advantage ($A_t < 0$)**  \n",
    "- **Bad action**: We want to decrease its probability\n",
    "- **If $r_t < 1-\\epsilon$**: Clipping prevents excessive decrease\n",
    "- **Intuition**: \"Don't get too harsh on bad actions\"\n",
    "\n",
    "**Concrete Example**:\n",
    "- **Old policy**: $\\pi_{\\theta_{old}}(a|s) = 0.1$ (10% chance)\n",
    "- **New policy**: $\\pi_\\theta(a|s) = 0.5$ (50% chance)\n",
    "- **Ratio**: $r_t = 0.5/0.1 = 5.0$\n",
    "- **Advantage**: $A_t = +10$ (good action)\n",
    "- **Clip epsilon**: $\\epsilon = 0.2$ â†’ clipped ratio = $1.2$\n",
    "\n",
    "**Without clipping**: Objective = $5.0 \\times 10 = 50$ (huge update!)  \n",
    "**With clipping**: Objective = $\\min(50, 1.2 \\times 10) = \\min(50, 12) = 12$ (moderate update)\n",
    "\n",
    "**The Key Insight**: PPO only allows **moderate** policy changes, preventing the destructive updates that plague standard policy gradients while still enabling learning progress.\n",
    "\n",
    "**Benefits of Clipped Objective**:\n",
    "- **Stability**: Prevents destructive policy updates\n",
    "- **Data efficiency**: Enables safe reuse of experience data\n",
    "- **Simplicity**: No complex constrained optimization like TRPO\n",
    "- **Robustness**: Works across diverse environments and hyperparameters\n",
    "\n",
    "### 2. ðŸŒ Parallel Environment Collection\n",
    "\n",
    "**Problem**: Single environment collection is slow and provides limited data diversity.\n",
    "\n",
    "**Solution**: Use multiple parallel environments like A2C, but with PPO's rollout-based collection.\n",
    "\n",
    "**Benefits**:\n",
    "- **Faster data collection**: Multiple environments running simultaneously\n",
    "- **Better batch diversity**: Each environment may be in different states\n",
    "- **More stable gradients**: Averaging across diverse experiences\n",
    "- **GPU efficiency**: Natural batching for neural network updates\n",
    "\n",
    "### 3. ðŸŽ² Generalized Advantage Estimation (GAE)\n",
    "\n",
    "**Problem**: Actor-Critic TD still has high variance in advantage estimates.\n",
    "\n",
    "**Solution**: Blend multiple n-step returns with exponential weighting.\n",
    "\n",
    "**Standard N-Step Advantage**:\n",
    "$$A_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k+1} + \\gamma^n V(s_{t+n}) - V(s_t)$$\n",
    "\n",
    "**GAE Formula**:\n",
    "$$A_t^{GAE(\\lambda)} = \\sum_{k=0}^{\\infty} (\\gamma \\lambda)^k \\delta_{t+k}$$\n",
    "\n",
    "Where $\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "**Effective Horizon Example**: With $\\gamma = 0.99$ and $\\lambda = 0.95$, the weights decay as $(\\gamma \\lambda)^k = 0.9405^k$. After ~15 steps, the weight drops to $0.9405^{15} \\approx 0.1$, meaning GAE effectively considers about 15 future steps while theoretically extending to infinity.\n",
    "\n",
    "**Benefits**:\n",
    "- **Bias-variance tradeoff**: $\\lambda=0$ (low variance, high bias) to $\\lambda=1$ (high variance, low bias)\n",
    "- **Flexible**: Can interpolate between TD and Monte Carlo methods\n",
    "- **Efficient**: Exponential weighting reduces computational cost\n",
    "\n",
    "### 4. ðŸŒŸ Entropy Regularization\n",
    "\n",
    "**Problem**: Policies can converge prematurely to suboptimal solutions.\n",
    "\n",
    "**Solution**: Add entropy regularization to encourage exploration.\n",
    "\n",
    "**Entropy Loss Definition**:\n",
    "$$L^{entropy}(\\theta) = -\\mathbb{E}[H(\\pi_\\theta)]$$\n",
    "\n",
    "Where $H(\\pi_\\theta) = -\\sum_a \\pi_\\theta(a|s_t) \\log \\pi_\\theta(a|s_t)$ is the entropy.\n",
    "\n",
    "**Why Negative Entropy?** \n",
    "- **High entropy** = more exploration = **good** â†’ we want to **minimize** negative entropy\n",
    "- **Low entropy** = less exploration = **bad** â†’ minimizing negative entropy **increases** entropy\n",
    "- This makes entropy loss consistent with our minimization framework\n",
    "\n",
    "**Total PPO Loss (to minimize)**:\n",
    "$$L^{TOTAL}(\\theta) = L^{CLIP}(\\theta) + c_1 L^{V}(\\theta) + c_2 L^{entropy}(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $L^{CLIP}(\\theta)$: Clipped policy loss (already negative, so we minimize it directly)\n",
    "- $L^{V}(\\theta)$: Value function loss\n",
    "- $L^{entropy}(\\theta) = -H(\\pi_\\theta)$: Negative entropy loss\n",
    "- $c_1$: Value function loss coefficient (typically 0.5)\n",
    "- $c_2$: Entropy regularization coefficient (typically 0.01)\n",
    "\n",
    "**Clean Mathematical Form**: Now all three terms are losses we want to minimize, making the total loss a simple sum without mixed signs.\n",
    "\n",
    "**Benefits**:\n",
    "- **Exploration**: Prevents premature convergence to deterministic policies\n",
    "- **Stability**: Maintains policy diversity throughout training\n",
    "- **Adaptability**: Automatic annealing as learning progresses\n",
    "- **Mathematical elegance**: Clean formulation as minimization objective\n",
    "\n",
    "### 5. ðŸ“š Rollout-Based Updates with Multiple Epochs\n",
    "\n",
    "**Problem**: Single-step updates waste valuable environment interaction data.\n",
    "\n",
    "**Solution**: Collect large rollouts from all parallel environments, then train for multiple epochs.\n",
    "\n",
    "**Data Collection Strategy**:\n",
    "1. **Collect rollouts**: Gather `rollout_length` steps from all `num_envs` parallel environments\n",
    "2. **Total transitions**: `rollout_length Ã— num_envs` transitions per update\n",
    "3. **Multiple epochs**: Train on the same rollout data for K epochs (typically 4-10)\n",
    "4. **Minibatch updates**: Split rollout into minibatches for efficient GPU utilization\n",
    "5. **Prevent overfitting**: Clipping and KL penalties prevent over-optimization\n",
    "\n",
    "### 6. âœ‚ï¸ Clipped Value Function Loss\n",
    "\n",
    "**Problem**: Value function updates can also be destructive and unstable.\n",
    "\n",
    "**Solution**: Clip value function updates similar to policy updates.\n",
    "\n",
    "**Standard Value Loss**:\n",
    "$$L^{V}(\\theta) = (V_\\theta(s_t) - V_t^{target})^2$$\n",
    "\n",
    "**PPO Clipped Value Loss**:\n",
    "$$L^{V}(\\theta) = \\mathbb{E}[\\max((V_\\theta(s_t) - V_t^{target})^2, (\\text{clip}(V_\\theta(s_t), V_{old} - \\epsilon_v, V_{old} + \\epsilon_v) - V_t^{target})^2)]$$\n",
    "\n",
    "**Why max() and not min()? A Conservative Approach to Clipping**\n",
    "\n",
    "The maximum operation ensures we **never underestimate the true prediction error** when clipping occurs. Here's the detailed reasoning:\n",
    "\n",
    "**Case 1: Clipping doesn't constrain the update**\n",
    "- If $V_{old} - \\epsilon_v < V_\\theta(s_t) < V_{old} + \\epsilon_v$, then clipping has no effect\n",
    "- The clipped value equals the unclipped value: $\\text{clip}(V_\\theta(s_t), ...) = V_\\theta(s_t)$\n",
    "- Both loss terms are identical: $\\max(\\text{same}, \\text{same}) = \\text{same}$\n",
    "- Result: Normal, unclipped loss computation\n",
    "\n",
    "**Case 2: Clipping constrains the update (the critical case)**\n",
    "- The new value prediction $V_\\theta(s_t)$ would move too far from $V_{old}$\n",
    "- Clipping forces: $V_{clipped} = V_{old} \\pm \\epsilon_v$ (boundary value)\n",
    "- Now we have two different loss values to choose from:\n",
    "  - **Unclipped loss**: $(V_\\theta(s_t) - V_t^{target})^2$ (true error)\n",
    "  - **Clipped loss**: $(V_{clipped} - V_t^{target})^2$ (constrained error)\n",
    "\n",
    "**The Conservative Principle**: We take the **maximum** (larger) of these two losses because:\n",
    "\n",
    "1. **Prevent Loss Hiding**: If clipping makes the prediction artificially closer to the target, we don't want to hide this by using the smaller loss\n",
    "2. **Maintain Learning Signal**: The larger loss preserves the magnitude of the error signal for gradient computation\n",
    "3. **Avoid Underfitting**: Using min() would encourage the optimizer to prefer clipped updates even when they're less accurate\n",
    "4. **Consistency Check**: Only allow clipping if it doesn't make the loss artificially small\n",
    "\n",
    "**Example Scenario**:\n",
    "- Target return: $V_t^{target} = 100$\n",
    "- Old value: $V_{old} = 50$\n",
    "- New prediction: $V_\\theta(s_t) = 90$ (moving toward target)\n",
    "- Clipping bound: $\\epsilon_v = 10$\n",
    "- Clipped value: $V_{clipped} = \\min(90, 50 + 10) = 60$\n",
    "\n",
    "Loss comparison:\n",
    "- Unclipped loss: $(90 - 100)^2 = 100$\n",
    "- Clipped loss: $(60 - 100)^2 = 1600$\n",
    "\n",
    "Using max(): We choose 1600 (the larger loss) because the clipped value is actually **further** from the target. This prevents the clipping from artificially reducing the loss signal.\n",
    "\n",
    "**Benefits**:\n",
    "- **Stable value learning**: Prevents large value function updates\n",
    "- **Conservative clipping**: Only clips when it doesn't hide true error\n",
    "- **Consistent with policy clipping**: Unified approach to stability\n",
    "- **Empirical improvement**: Better performance in practice\n",
    "\n",
    "### 7. ðŸ“Š KL Divergence Monitoring (Not Constraining)\n",
    "\n",
    "**Key Distinction**: Unlike TRPO, PPO doesn't use KL divergence as a **constraint** - it uses it as a **diagnostic tool**.\n",
    "\n",
    "**What is KL Divergence?**\n",
    "The Kullback-Leibler divergence measures how much one probability distribution differs from another:\n",
    "\n",
    "$$KL(\\pi_{\\theta_{old}}, \\pi_\\theta) = \\mathbb{E}_{s \\sim \\rho} \\mathbb{E}_{a \\sim \\pi_{\\theta_{old}}} \\left[ \\log \\frac{\\pi_{\\theta_{old}}(a|s)}{\\pi_\\theta(a|s)} \\right]$$\n",
    "\n",
    "**Intuitive Meaning**:\n",
    "- **KL = 0**: New policy is identical to old policy\n",
    "- **KL > 0**: New policy differs from old policy\n",
    "- **Higher KL**: Larger policy changes\n",
    "\n",
    "**Why Monitor KL in PPO?**\n",
    "\n",
    "1. **Training Health Check**: KL divergence tells us how much the policy is changing each update\n",
    "   - **Healthy range**: 0.001 - 0.01 (modest, stable changes)\n",
    "   - **Too low**: < 0.0001 (learning stagnation)\n",
    "   - **Too high**: > 0.1 (potentially destructive updates)\n",
    "\n",
    "2. **Clipping Effectiveness**: KL helps validate that clipping is working\n",
    "   - If KL is very high despite clipping, something is wrong\n",
    "   - If KL is very low, we might be too conservative\n",
    "\n",
    "3. **Hyperparameter Tuning**: KL guides learning rate and clipping parameter adjustment\n",
    "   - High KL â†’ reduce learning rate or decrease clip epsilon\n",
    "   - Low KL â†’ increase learning rate or increase clip epsilon\n",
    "\n",
    "4. **Early Stopping**: Some implementations use KL divergence for early stopping\n",
    "   - If KL exceeds a threshold, stop training on current batch\n",
    "   - Prevents over-optimization on stale data\n",
    "\n",
    "**PPO's Approach vs TRPO's Approach**:\n",
    "\n",
    "| Aspect | TRPO | PPO |\n",
    "|--------|------|-----|\n",
    "| **KL Usage** | Hard constraint | Diagnostic monitoring |\n",
    "| **Optimization** | Constrained optimization | Unconstrained with clipping |\n",
    "| **Computational Cost** | Expensive (second-order) | Cheap (first-order) |\n",
    "| **Implementation** | Complex conjugate gradient | Simple gradient descent |\n",
    "| **Robustness** | Sensitive to KL threshold | Robust to hyperparameters |\n",
    "\n",
    "**Real-World Example**:\n",
    "In our implementation, you might see:\n",
    "- **Early training**: KL â‰ˆ 0.01 (policy learning quickly)\n",
    "- **Mid training**: KL â‰ˆ 0.005 (policy refining)\n",
    "- **Late training**: KL â‰ˆ 0.001 (policy converging)\n",
    "\n",
    "**Benefits of KL Monitoring**:\n",
    "- **Debugging**: Identifies training instabilities early\n",
    "- **Validation**: Confirms clipping is preventing destructive updates\n",
    "- **Optimization**: Guides hyperparameter tuning\n",
    "- **Research**: Enables comparison with TRPO and other methods\n",
    "- **Zero overhead**: Computed during normal forward pass\n",
    "\n",
    "**The Bottom Line**: PPO gets TRPO's stability benefits through clipping, but keeps KL monitoring as a \"health check\" - giving us the best of both worlds with minimal computational overhead.\n",
    "\n",
    "## ðŸš€ Why PPO Dominates (2017-2025)\n",
    "\n",
    "### âœ… PPO's Advantages\n",
    "\n",
    "1. **Simplicity**: Easy to implement and understand\n",
    "2. **Stability**: Robust across diverse environments and hyperparameters\n",
    "3. **Sample Efficiency**: Reuses data effectively through multiple epochs\n",
    "4. **Computational Efficiency**: First-order optimization, GPU-friendly\n",
    "5. **Generality**: Works well for both discrete and continuous control\n",
    "6. **Theoretical Grounding**: Builds on solid policy gradient theory\n",
    "7. **Empirical Success**: Proven track record in complex domains\n",
    "\n",
    "### ðŸ† PPO's Real-World Impact\n",
    "\n",
    "**OpenAI's Applications**:\n",
    "- **ChatGPT**: RLHF (Reinforcement Learning from Human Feedback) training\n",
    "- **GPT-4**: Large-scale language model alignment\n",
    "- **OpenAI Five**: Dota 2 championship-level performance\n",
    "- **Robotics**: Real-world robot control and manipulation\n",
    "\n",
    "**Industry Adoption**:\n",
    "- **Default choice**: Most RL practitioners start with PPO\n",
    "- **Production systems**: Widely used in recommendation systems, game AI, autonomous vehicles\n",
    "- **Research standard**: Baseline comparison in academic papers\n",
    "\n",
    "### ðŸ“Š PPO vs Alternatives (2025 Perspective)\n",
    "\n",
    "**PPO vs SAC (Soft Actor-Critic)**:\n",
    "- **PPO**: Better for discrete actions, more stable, simpler\n",
    "- **SAC**: Better for continuous control, more sample efficient, more complex\n",
    "\n",
    "**PPO vs TD3 (Twin Delayed Deep Deterministic)**:\n",
    "- **PPO**: General-purpose, works with discrete actions\n",
    "- **TD3**: Continuous control only, more sample efficient in some domains\n",
    "\n",
    "**PPO vs Modern Methods**:\n",
    "- **PPO still competitive**: Remains state-of-the-art for many applications\n",
    "- **Simplicity advantage**: Easier to tune and debug than newer methods\n",
    "- **Proven reliability**: Extensive empirical validation across domains\n",
    "\n",
    "## ðŸ”„ PPO Algorithm Overview\n",
    "\n",
    "**Algorithm: Proximal Policy Optimization (PPO) with Parallel Environments**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Unified Actor-Critic network with parameters $\\theta$\n",
    "- Number of parallel environments $E$\n",
    "- Rollout length $T$ (steps per environment)\n",
    "- Minibatch size $M$\n",
    "- Number of epochs $K$\n",
    "- Clipping parameter $\\epsilon$\n",
    "- GAE parameter $\\lambda$\n",
    "- Learning rate $\\alpha$\n",
    "- $c_1$: Value function loss coefficient (typically 0.5)\n",
    "- $c_2$: Entropy coefficient (typically 0.01)\n",
    "\n",
    "**Output:** \n",
    "- Trained unified network parameters $\\theta$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** network parameters $\\theta$ and $E$ parallel environments\n",
    "2. **For** iteration $i = 1, 2, ...$ **do:**\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;**For** $t = 1, 2, ..., T$ **do:** *(collect rollout from all environments)*\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For each environment** $e = 1, 2, ..., E$ **do:**\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Sample action**: $a_t^{(e)} \\sim \\pi_\\theta(\\cdot|s_t^{(e)})$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Execute**: $s_{t+1}^{(e)}, r_{t+1}^{(e)} \\leftarrow \\text{env}_e.\\text{step}(a_t^{(e)})$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Store**: $(s_t^{(e)}, a_t^{(e)}, r_{t+1}^{(e)}, \\log \\pi_\\theta(a_t^{(e)}|s_t^{(e)}), V_\\theta(s_t^{(e)}))$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "9. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "10. &nbsp;&nbsp;&nbsp;&nbsp;**Compute GAE advantages** for all $T \\times E$ transitions using parameter $\\lambda$\n",
    "11. &nbsp;&nbsp;&nbsp;&nbsp;**For** epoch $k = 1, 2, ..., K$ **do:**\n",
    "12. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Shuffle** all $T \\times E$ transitions randomly\n",
    "13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For** each minibatch $B$ of size $M$ **do:**\n",
    "14. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute probability ratio**: $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$\n",
    "15. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute clipped surrogate objective**: $L^{CLIP}(\\theta) = -\\mathbb{E}_{B}\\left[\\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)\\right]$\n",
    "16. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute clipped value function loss**: $L^{V}(\\theta) = \\mathbb{E}_{B}[\\max((V_\\theta(s_t) - V_t^{target})^2, (\\text{clip}(V_\\theta(s_t), V_{old} - \\epsilon_v, V_{old} + \\epsilon_v) - V_t^{target})^2)]$\n",
    "17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute entropy loss**: $L^{entropy}(\\theta) = -\\mathbb{E}_{B}[H(\\pi_\\theta)]$\n",
    "18. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Monitor** KL divergence $KL(\\pi_{\\theta_{old}}, \\pi_\\theta)$ (diagnostic only)\n",
    "19. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Total loss**: $L^{TOTAL}(\\theta) = L^{CLIP}(\\theta) + c_1 L^{V}(\\theta) + c_2 L^{entropy}(\\theta)$\n",
    "20. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Update** $\\theta$ using gradient descent: $\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L^{TOTAL}(\\theta)$\n",
    "21. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "22. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "23. **End For**\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ PPO Loss Functions Detailed\n",
    "\n",
    "**Clipped Surrogate Objective Loss**:\n",
    "$$L^{CLIP}(\\theta) = -\\mathbb{E}\\left[\\sum_{t=0}^{T-1} \\min(r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $A_t$ is the advantage estimate\n",
    "- $\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)$ constrains the ratio to $[1-\\epsilon, 1+\\epsilon]$\n",
    "- The $\\min()$ operation implements the conservative update principle\n",
    "\n",
    "**Clipped Value Function Loss**:\n",
    "$$L^{V}(\\theta) = \\mathbb{E}[\\max((V_\\theta(s_t) - V_t^{target})^2, (\\text{clip}(V_\\theta(s_t), V_{old} - \\epsilon_v, V_{old} + \\epsilon_v) - V_t^{target})^2)]$$\n",
    "\n",
    "Where:\n",
    "- $V_\\theta(s_t)$ is the current value prediction\n",
    "- $V_t^{target}$ is the target value (GAE-computed return)\n",
    "- $V_{old}$ is the value prediction from the old network\n",
    "- $\\epsilon_v$ is the value clipping parameter\n",
    "- The $\\max()$ operation ensures conservative clipping (prevents loss hiding)\n",
    "\n",
    "**Entropy Loss**:\n",
    "$$L^{entropy}(\\theta) = -\\mathbb{E}[H(\\pi_\\theta)] = -\\mathbb{E}\\left[-\\sum_a \\pi_\\theta(a|s_t) \\log \\pi_\\theta(a|s_t)\\right]$$\n",
    "\n",
    "Where $H(\\pi_\\theta)$ is the entropy of the policy distribution, encouraging exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our RL utilities including the ActorCriticNetwork\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    ActorCriticNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    ")\n",
    "from rl_utils.visualization import (\n",
    "    get_moving_average,\n",
    "    plot_ppo_training_results,\n",
    "    plot_ppo_variance_analysis,\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 5e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"window_length\": 50,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"num_videos\": 9,  # Number of videos to record during training\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"network\": {\n",
    "        \"fc_out_features\": [64, 64],  # Shared features\n",
    "        \"actor_features\": [32],  # Actor-specific layers after shared\n",
    "        \"critic_features\": [32],  # Critic-specific layers after shared\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.0,  # No dropout for stability\n",
    "    },\n",
    "    # PPO-Specific Parameters\n",
    "    \"rollout_length\": 2048,  # Steps per rollout\n",
    "    \"minibatch_size\": 64,    # Minibatch size for updates\n",
    "    \"epochs\": 10,            # Training epochs per rollout\n",
    "    \"clip_epsilon\": 0.2,     # Clipping parameter\n",
    "    \"value_clip_epsilon\": 0.2,  # Value function clipping\n",
    "    \"critic_loss_coeff\": 0.5,   # Weight for critic loss\n",
    "    \"entropy_coeff\": 0.01,      # Weight for entropy bonus\n",
    "    \"gae_lambda\": 0.95,         # GAE parameter\n",
    "    \"max_grad_norm\": 0.5,       # Maximum gradient norm for clipping\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"ðŸŽ² Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"ðŸ“ Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization agent with all key innovations.\"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        \"\"\"\n",
    "        Initialize PPO agent.\n",
    "\n",
    "        Args:\n",
    "            network: ActorCriticNetwork instance\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.network = network.to(config[\"device\"])\n",
    "        self.device = config[\"device\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.rollout_length = config[\"rollout_length\"]\n",
    "        self.minibatch_size = config[\"minibatch_size\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.clip_epsilon = config[\"clip_epsilon\"]\n",
    "        self.value_clip_epsilon = config[\"value_clip_epsilon\"]\n",
    "        self.critic_loss_coeff = config[\"critic_loss_coeff\"]\n",
    "        self.entropy_coeff = config[\"entropy_coeff\"]\n",
    "        self.gae_lambda = config[\"gae_lambda\"]\n",
    "        self.max_grad_norm = config[\"max_grad_norm\"]\n",
    "        self.window_size = config.get(\"window_length\")\n",
    "\n",
    "        # Single optimizer for all network parameters\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "        )\n",
    "\n",
    "        # Print detailed network information\n",
    "        print(f\"ðŸ“Š PPO NETWORK DETAILS:\")\n",
    "        self.network.print_network_info()\n",
    "        print(f\"ðŸŽ¯ Rollout Length: {self.rollout_length}\")\n",
    "        print(f\"ðŸ“¦ Minibatch Size: {self.minibatch_size}\")\n",
    "        print(f\"ðŸ” Training Epochs: {self.epochs}\")\n",
    "        print(f\"âœ‚ï¸ Clip Epsilon: {self.clip_epsilon}\")\n",
    "        print(f\"ðŸŒŸ Entropy Coefficient: {self.entropy_coeff}\")\n",
    "        print(f\"ðŸŽ² GAE Lambda: {self.gae_lambda}\")\n",
    "        print(f\"ðŸŽ“ Learning Rate: {config['lr']}\")\n",
    "        print(f\"âœ‚ï¸ Max Gradient Norm: {self.max_grad_norm}\")\n",
    "        print(f\"âš–ï¸ Critic Loss Coefficient: {self.critic_loss_coeff}\")\n",
    "\n",
    "        # Rollout buffer storage\n",
    "        self.reset_rollout_buffer()\n",
    "\n",
    "        # Variance and performance tracking\n",
    "        self.gradient_norms = []\n",
    "        self.episode_scores = []  # Raw undiscounted episode scores (only thing we can track in PPO)\n",
    "        self.score_variance_history = []\n",
    "\n",
    "        # Update step tracking\n",
    "        self.update_step = 0\n",
    "        self.update_steps_history = []\n",
    "\n",
    "        # Loss component tracking\n",
    "        self.loss_history = {\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"entropy_loss\": [],\n",
    "            \"total_loss\": [],\n",
    "        }\n",
    "\n",
    "        # PPO-specific tracking\n",
    "        self.rollout_count = 0\n",
    "        self.steps_collected = 0\n",
    "        self.policy_updates = 0\n",
    "        self.clip_fraction_history = []\n",
    "        self.kl_divergence_history = []\n",
    "\n",
    "    def reset_rollout_buffer(self):\n",
    "        \"\"\"Reset rollout buffer for new collection.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.advantages = []\n",
    "        self.returns = []\n",
    "        self.current_episode_return = 0.0\n",
    "        self.episode_step_count = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action and store necessary data.\"\"\"\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get policy distribution and value estimate\n",
    "            dist, value = self.network(state)\n",
    "            action = dist.sample()\n",
    "\n",
    "            # Store log probability and value prediction\n",
    "            if self.network.is_continuous:\n",
    "                log_prob = dist.log_prob(action).sum(-1)\n",
    "                action_to_env = self.network.clip_action(action).flatten()\n",
    "            else:\n",
    "                log_prob = dist.log_prob(action)\n",
    "                action_to_env = action.item()\n",
    "\n",
    "        # Store in rollout buffer\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "\n",
    "        self.steps_collected += 1\n",
    "        self.episode_step_count += 1\n",
    "\n",
    "        return action_to_env\n",
    "\n",
    "    def store_transition(self, reward, done):\n",
    "        \"\"\"Store reward and done flag.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.current_episode_return += reward\n",
    "\n",
    "        # If episode ended, store episode score\n",
    "        if done:\n",
    "            self.episode_scores.append(self.current_episode_return)\n",
    "            self.current_episode_return = 0.0\n",
    "            self.episode_step_count = 0\n",
    "\n",
    "    def is_rollout_complete(self):\n",
    "        \"\"\"Check if rollout is complete.\"\"\"\n",
    "        return len(self.states) >= self.rollout_length\n",
    "\n",
    "    def compute_gae_advantages(self, next_value=0.0):\n",
    "        \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
    "        # Convert to tensors\n",
    "        rewards = torch.tensor(self.rewards, dtype=torch.float32, device=self.device)\n",
    "        values = torch.stack(self.values)\n",
    "        dones = torch.tensor(self.dones, dtype=torch.float32, device=self.device)  # Convert to float\n",
    "\n",
    "        # Compute advantages using GAE\n",
    "        advantages = []\n",
    "        gae = 0.0\n",
    "        \n",
    "        # Work backwards through the episode\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                # Last step - use next_value from environment or 0 if terminal\n",
    "                next_value_t = next_value if not self.dones[t] else 0.0\n",
    "            else:\n",
    "                next_value_t = values[t + 1]\n",
    "            \n",
    "            # TD error\n",
    "            delta = rewards[t] + self.gamma * next_value_t * (1 - dones[t]) - values[t]\n",
    "            \n",
    "            # GAE calculation\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "        returns = advantages + values.detach()\n",
    "\n",
    "        return advantages, returns\n",
    "\n",
    "    def update_policy(self, next_state=None):\n",
    "        \"\"\"Update policy using PPO algorithm.\"\"\"\n",
    "        if len(self.states) < self.minibatch_size:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0, \"entropy_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        self.rollout_count += 1\n",
    "\n",
    "        # Get next state value for GAE calculation\n",
    "        if next_state is not None:\n",
    "            next_state_tensor = torch.as_tensor(next_state, dtype=torch.float32, device=self.device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.network(next_state_tensor)\n",
    "                next_value = next_value.item()\n",
    "        else:\n",
    "            next_value = 0.0\n",
    "\n",
    "        # Compute advantages and returns using GAE\n",
    "        advantages, returns = self.compute_gae_advantages(next_value)\n",
    "\n",
    "        # Track score variance\n",
    "        if len(self.episode_scores) >= self.window_size:\n",
    "            recent_scores = self.episode_scores[-self.window_size:]\n",
    "            score_variance = np.var(recent_scores)\n",
    "            self.score_variance_history.append(score_variance)\n",
    "\n",
    "        # Convert rollout data to tensors\n",
    "        states = torch.stack(self.states)\n",
    "        actions = torch.stack(self.actions)\n",
    "        old_log_probs = torch.stack(self.log_probs)\n",
    "        old_values = torch.stack(self.values)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Store for reuse across epochs\n",
    "        rollout_data = {\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'old_log_probs': old_log_probs,\n",
    "            'old_values': old_values,\n",
    "            'advantages': advantages,\n",
    "            'returns': returns,\n",
    "        }\n",
    "\n",
    "        # Train for multiple epochs\n",
    "        total_actor_loss = 0.0\n",
    "        total_critic_loss = 0.0\n",
    "        total_entropy_loss = 0.0\n",
    "        total_loss = 0.0\n",
    "        total_grad_norm = 0.0\n",
    "        clip_fraction = 0.0\n",
    "        kl_divergence = 0.0\n",
    "        num_updates = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle data for each epoch\n",
    "            indices = torch.randperm(len(states))\n",
    "            \n",
    "            for start_idx in range(0, len(states), self.minibatch_size):\n",
    "                end_idx = min(start_idx + self.minibatch_size, len(states))\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                if len(batch_indices) < self.minibatch_size // 2:  # Skip very small batches\n",
    "                    continue\n",
    "\n",
    "                # Extract minibatch\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                batch_old_values = old_values[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                dist, values = self.network(batch_states)\n",
    "                \n",
    "                # Compute new log probabilities\n",
    "                if self.network.is_continuous:\n",
    "                    new_log_probs = dist.log_prob(batch_actions).sum(-1)\n",
    "                else:\n",
    "                    new_log_probs = dist.log_prob(batch_actions)\n",
    "\n",
    "                # Compute probability ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "\n",
    "                # Clipped surrogate objective (already negative, so this is L^CLIP to minimize)\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                # Clipped value function loss\n",
    "                value_pred_clipped = batch_old_values + torch.clamp(\n",
    "                    values - batch_old_values, -self.value_clip_epsilon, self.value_clip_epsilon\n",
    "                )\n",
    "                value_loss_1 = (values - batch_returns).pow(2)\n",
    "                value_loss_2 = (value_pred_clipped - batch_returns).pow(2)\n",
    "                critic_loss = 0.5 * torch.max(value_loss_1, value_loss_2).mean()\n",
    "\n",
    "                # critic loss calculation without clipping. try this.\n",
    "                # critic_loss = 0.5 * (values - batch_returns).pow(2).mean()  \n",
    "\n",
    "                # Entropy loss: L^entropy = -H(Ï€) (negative entropy to minimize)\n",
    "                entropy = dist.entropy()\n",
    "                if entropy.dim() > 1:\n",
    "                    entropy = entropy.sum(-1)\n",
    "                entropy_loss = -entropy.mean()  # This is L^entropy (negative entropy)\n",
    "\n",
    "                # Total loss: L^TOTAL = L^CLIP + c1*L^VF + c2*L^entropy\n",
    "                total_loss_batch = actor_loss + self.critic_loss_coeff * critic_loss + self.entropy_coeff * entropy_loss\n",
    "\n",
    "                # Update network\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss_batch.backward()\n",
    "\n",
    "                # Record gradient norm BEFORE clipping\n",
    "                grad_norm = 0.0\n",
    "                for param in self.network.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param_norm = param.grad.data.norm(2)\n",
    "                        grad_norm += param_norm.item() ** 2\n",
    "                grad_norm = grad_norm ** 0.5\n",
    "\n",
    "                # Apply gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Accumulate statistics\n",
    "                total_actor_loss += actor_loss.item()\n",
    "                total_critic_loss += critic_loss.item()\n",
    "                total_entropy_loss += entropy_loss.item()\n",
    "                total_loss += total_loss_batch.item()\n",
    "                total_grad_norm += grad_norm\n",
    "                \n",
    "                # Track clipping and KL divergence\n",
    "                with torch.no_grad():\n",
    "                    clipped = torch.abs(ratio - 1.0) > self.clip_epsilon\n",
    "                    clip_fraction += clipped.float().mean().item()\n",
    "                    kl_divergence += (batch_old_log_probs - new_log_probs).mean().item()\n",
    "\n",
    "                num_updates += 1\n",
    "\n",
    "        # Average statistics\n",
    "        if num_updates > 0:\n",
    "            avg_actor_loss = total_actor_loss / num_updates\n",
    "            avg_critic_loss = total_critic_loss / num_updates\n",
    "            avg_entropy_loss = total_entropy_loss / num_updates\n",
    "            avg_total_loss = total_loss / num_updates\n",
    "            avg_grad_norm = total_grad_norm / num_updates\n",
    "            avg_clip_fraction = clip_fraction / num_updates\n",
    "            avg_kl_divergence = kl_divergence / num_updates\n",
    "        else:\n",
    "            avg_actor_loss = avg_critic_loss = avg_entropy_loss = avg_total_loss = avg_grad_norm = 0.0\n",
    "            avg_clip_fraction = avg_kl_divergence = 0.0\n",
    "\n",
    "        # Store statistics\n",
    "        self.update_step += 1\n",
    "        self.policy_updates += num_updates\n",
    "        self.gradient_norms.append(avg_grad_norm)\n",
    "        self.loss_history[\"actor_loss\"].append(avg_actor_loss)\n",
    "        self.loss_history[\"critic_loss\"].append(avg_critic_loss)\n",
    "        self.loss_history[\"entropy_loss\"].append(avg_entropy_loss)\n",
    "        self.loss_history[\"total_loss\"].append(avg_total_loss)\n",
    "        self.update_steps_history.append(self.update_step)\n",
    "        self.clip_fraction_history.append(avg_clip_fraction)\n",
    "        self.kl_divergence_history.append(avg_kl_divergence)\n",
    "\n",
    "        # Reset rollout buffer\n",
    "        self.reset_rollout_buffer()\n",
    "\n",
    "        return {\n",
    "            \"actor_loss\": avg_actor_loss,\n",
    "            \"critic_loss\": avg_critic_loss,\n",
    "            \"entropy_loss\": avg_entropy_loss,\n",
    "            \"total_loss\": avg_total_loss,\n",
    "        }, avg_grad_norm\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.episode_scores) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"score_mean\": 0.0,\n",
    "                \"score_std\": 0.0,\n",
    "                \"recent_score_variance\": 0.0,\n",
    "                \"clip_fraction_mean\": 0.0,\n",
    "                \"kl_divergence_mean\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"score_mean\": np.mean(self.episode_scores),\n",
    "            \"score_std\": np.std(self.episode_scores),\n",
    "            \"recent_score_variance\": (\n",
    "                self.score_variance_history[-1]\n",
    "                if self.score_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "            \"clip_fraction_mean\": np.mean(self.clip_fraction_history) if self.clip_fraction_history else 0.0,\n",
    "            \"kl_divergence_mean\": np.mean(self.kl_divergence_history) if self.kl_divergence_history else 0.0,\n",
    "        }\n",
    "\n",
    "    def get_ppo_stats(self):\n",
    "        \"\"\"Get PPO-specific statistics.\"\"\"\n",
    "        return {\n",
    "            \"rollout_count\": self.rollout_count,\n",
    "            \"steps_collected\": self.steps_collected,\n",
    "            \"policy_updates\": self.policy_updates,\n",
    "            \"updates_per_rollout\": self.policy_updates / max(1, self.rollout_count),\n",
    "            \"steps_per_update\": self.steps_collected / max(1, self.policy_updates),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afdb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(is_continuous, config):\n",
    "    \"\"\"Main training loop for the PPO agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"PROXIMAL POLICY OPTIMIZATION (PPO) - {action_type.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate video recording interval\n",
    "    video_record_interval = max(1, config[\"episodes\"] // config[\"num_videos\"])\n",
    "    print(f\"ðŸ“¹ Recording {config['num_videos']} videos every {video_record_interval} episodes\")\n",
    "    \n",
    "    # Create algorithm-specific video folder\n",
    "    video_folder = f\"videos/PPO_{action_type.lower()}\"\n",
    "    config_with_videos = config.copy()\n",
    "    config_with_videos[\"video_folder\"] = video_folder\n",
    "    config_with_videos[\"video_record_interval\"] = video_record_interval\n",
    "    \n",
    "    # Create Environment\n",
    "    env = create_env_with_wrappers(\n",
    "        config_with_videos, \n",
    "        is_continuous, \n",
    "        record_videos=True, \n",
    "        video_prefix=f\"ppo_{action_type.lower()}\",\n",
    "        cleanup_existing=True\n",
    "    )\n",
    "    \n",
    "    # Get observation dimension and space\n",
    "    dummy_obs, _ = env.reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "    \n",
    "    # Create Actor-Critic Network and Agent\n",
    "    print(f\"\\nðŸ—ï¸ CREATING {action_type.upper()} ACTOR-CRITIC NETWORK:\")\n",
    "    network = ActorCriticNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=env.action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"network\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ¤– INITIALIZING {action_type.upper()} PPO AGENT:\")\n",
    "    agent = PPOAgent(network, config)\n",
    "    \n",
    "    # Training Loop\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=config[\"window_length\"])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nðŸš€ STARTING {action_type.upper()} PPO TRAINING...\")\n",
    "    \n",
    "    # Initialize environment\n",
    "    state, _ = env.reset(seed=config[\"seed\"])\n",
    "    episode_count = 0\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    pbar = tqdm(total=config[\"episodes\"], desc=\"Training\", unit=\"episode\")\n",
    "    \n",
    "    while episode_count < config[\"episodes\"]:\n",
    "        # Collect rollout\n",
    "        while not agent.is_rollout_complete():\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_transition(reward, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                # Episode finished\n",
    "                episode_count += 1\n",
    "                episode_reward = agent.episode_scores[-1] if agent.episode_scores else 0.0\n",
    "                scores.append(episode_reward)\n",
    "                scores_window.append(episode_reward)\n",
    "                \n",
    "                # Reset environment\n",
    "                state, _ = env.reset(seed=config[\"seed\"] + episode_count)\n",
    "                \n",
    "                # Update progress bar\n",
    "                avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "                \n",
    "                # Get latest loss values and metrics\n",
    "                actor_loss = agent.loss_history[\"actor_loss\"][-1] if agent.loss_history[\"actor_loss\"] else 0.0\n",
    "                critic_loss = agent.loss_history[\"critic_loss\"][-1] if agent.loss_history[\"critic_loss\"] else 0.0\n",
    "                entropy_loss = agent.loss_history[\"entropy_loss\"][-1] if agent.loss_history[\"entropy_loss\"] else 0.0\n",
    "                total_loss = agent.loss_history[\"total_loss\"][-1] if agent.loss_history[\"total_loss\"] else 0.0\n",
    "                grad_norm = agent.gradient_norms[-1] if agent.gradient_norms else 0.0\n",
    "                \n",
    "                # Get PPO-specific metrics\n",
    "                clip_fraction = agent.clip_fraction_history[-1] if agent.clip_fraction_history else 0.0\n",
    "                kl_divergence = agent.kl_divergence_history[-1] if agent.kl_divergence_history else 0.0\n",
    "                \n",
    "                pbar.set_description(\n",
    "                    f\"Ep {episode_count:4d} | \"\n",
    "                    f\"Score: {episode_reward:6.1f} | \"\n",
    "                    f\"AvgScore({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "                    f\"Rollouts: {agent.rollout_count:3d} | \"\n",
    "                    f\"Updates: {agent.policy_updates:4d} | \"\n",
    "                    f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "                    f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "                    f\"EntLoss: {entropy_loss:7.4f} | \"\n",
    "                    f\"TotalLoss: {total_loss:7.4f} | \"\n",
    "                    f\"GradNorm: {grad_norm:6.4f} | \"\n",
    "                    f\"ClipFrac: {clip_fraction:5.3f} | \"\n",
    "                    f\"KL: {kl_divergence:7.4f}\"\n",
    "                )\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Handle video display\n",
    "                if episode_count % video_record_interval == 0 and config[\"record_videos\"]:\n",
    "                    from rl_utils.environment import display_latest_video\n",
    "                    pbar.write(f\"\\nVideo recorded at episode {episode_count}\")\n",
    "                    display_latest_video(\n",
    "                        config_with_videos[\"video_folder\"], \n",
    "                        f\"ppo_{action_type.lower()}\", \n",
    "                        episode_count\n",
    "                    )\n",
    "                \n",
    "                # Stop if we've reached the target number of episodes\n",
    "                if episode_count >= config[\"episodes\"]:\n",
    "                    break\n",
    "        \n",
    "        # Update policy when rollout is complete\n",
    "        if agent.is_rollout_complete():\n",
    "            loss_dict, grad_norm = agent.update_policy(state)\n",
    "            \n",
    "            # Update progress information with latest metrics\n",
    "            avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "            actor_loss = loss_dict.get('actor_loss', 0.0)\n",
    "            critic_loss = loss_dict.get('critic_loss', 0.0)\n",
    "            entropy_loss = loss_dict.get('entropy_loss', 0.0)\n",
    "            total_loss = loss_dict.get('total_loss', 0.0)\n",
    "            \n",
    "            # Get PPO-specific stats\n",
    "            variance_stats = agent.get_variance_stats()\n",
    "            clip_fraction = variance_stats.get('clip_fraction_mean', 0.0)\n",
    "            kl_divergence = variance_stats.get('kl_divergence_mean', 0.0)\n",
    "            \n",
    "            pbar.set_description(\n",
    "                f\"Ep {episode_count:4d} | \"\n",
    "                f\"Score: {scores[-1] if scores else 0.0:6.1f} | \"\n",
    "                f\"AvgScore({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "                f\"Rollouts: {agent.rollout_count:3d} | \"\n",
    "                f\"Updates: {agent.policy_updates:4d} | \"\n",
    "                f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "                f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "                f\"EntLoss: {entropy_loss:7.4f} | \"\n",
    "                f\"TotalLoss: {total_loss:7.4f} | \"\n",
    "                f\"GradNorm: {grad_norm:6.4f} | \"\n",
    "                f\"ClipFrac: {clip_fraction:5.3f} | \"\n",
    "                f\"KL: {kl_divergence:7.4f}\"\n",
    "            )\n",
    "    \n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_window_size = min(config[\"window_length\"], len(scores))\n",
    "    final_performance = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    \n",
    "    # Print PPO-specific statistics\n",
    "    ppo_stats = agent.get_ppo_stats()\n",
    "    print(f\"\\n{action_type} PPO training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Final performance: {final_performance:.2f} (last {final_window_size} episodes)\")\n",
    "    print(f\"Total rollouts: {ppo_stats['rollout_count']}\")\n",
    "    print(f\"Total policy updates: {ppo_stats['policy_updates']}\")\n",
    "    print(f\"Steps collected: {ppo_stats['steps_collected']}\")\n",
    "    print(f\"Updates per rollout: {ppo_stats['updates_per_rollout']:.1f}\")\n",
    "    print(f\"Steps per update: {ppo_stats['steps_per_update']:.1f}\")\n",
    "    \n",
    "    return scores, agent.loss_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe95954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DISCRETE ACTION SPACE: PPO ---\n",
    "print(\"Starting PPO training with DISCRETE actions...\")\n",
    "\n",
    "discrete_ppo_scores, discrete_ppo_losses, discrete_ppo_agent = train_ppo(\n",
    "    is_continuous=False, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8cee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for discrete PPO\n",
    "plot_ppo_training_results(\n",
    "    discrete_ppo_scores, \n",
    "    discrete_ppo_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Discrete\", \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_ppo_variance_analysis(\n",
    "    discrete_ppo_agent, \n",
    "    discrete_ppo_scores, \n",
    "    \"Discrete\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# PPO-specific analysis\n",
    "variance_stats = discrete_ppo_agent.get_variance_stats()\n",
    "ppo_stats = discrete_ppo_agent.get_ppo_stats()\n",
    "\n",
    "print(f\"\\nðŸ” DISCRETE PPO ANALYSIS:\")\n",
    "print(f\"Clip fraction: {variance_stats['clip_fraction_mean']:.3f} (target: ~{CONFIG['clip_epsilon']:.1f})\")\n",
    "print(f\"KL divergence: {variance_stats['kl_divergence_mean']:.4f}\")\n",
    "print(f\"Data efficiency: {ppo_stats['steps_per_update']:.1f} env steps per policy update (lower the better)\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_ppo_scores))\n",
    "final_avg = np.mean(discrete_ppo_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\nâœ… DISCRETE PPO TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_ppo_agent.network.get_param_count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b61a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE: PPO ---\n",
    "print(\"Starting PPO training with CONTINUOUS actions...\")\n",
    "\n",
    "continuous_ppo_scores, continuous_ppo_losses, continuous_ppo_agent = train_ppo(\n",
    "    is_continuous=True, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8fac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for continuous PPO\n",
    "plot_ppo_training_results(\n",
    "    continuous_ppo_scores, \n",
    "    continuous_ppo_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Continuous\", \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_ppo_variance_analysis(\n",
    "    continuous_ppo_agent, \n",
    "    continuous_ppo_scores, \n",
    "    \"Continuous\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"PPO\"\n",
    ")\n",
    "\n",
    "# PPO-specific analysis\n",
    "variance_stats = continuous_ppo_agent.get_variance_stats()\n",
    "ppo_stats = continuous_ppo_agent.get_ppo_stats()\n",
    "\n",
    "print(f\"\\nðŸ” CONTINUOUS PPO ANALYSIS:\")\n",
    "print(f\"Clip fraction: {variance_stats['clip_fraction_mean']:.3f} (target: ~{CONFIG['clip_epsilon']:.1f})\")\n",
    "print(f\"KL divergence: {variance_stats['kl_divergence_mean']:.4f}\")\n",
    "print(f\"Data efficiency: {ppo_stats['steps_per_update']:.1f} env steps per policy update (lower the better)\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_ppo_scores))\n",
    "final_avg = np.mean(continuous_ppo_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\nâœ… CONTINUOUS PPO TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_ppo_agent.network.get_param_count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18022237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPREHENSIVE ANALYSIS: PPO Performance and Efficiency ---\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_utils.visualization import get_moving_average\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE ANALYSIS: PPO Performance and Efficiency\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Collect experiment results\n",
    "experiments = [\n",
    "    (\"Discrete PPO\", discrete_ppo_scores, discrete_ppo_agent),\n",
    "    (\"Continuous PPO\", continuous_ppo_scores, continuous_ppo_agent),\n",
    "]\n",
    "\n",
    "print(f\"\\nðŸ“Š FINAL PERFORMANCE COMPARISON (last {CONFIG['window_length']} episodes):\")\n",
    "print(f\"{'Method':<20} {'Final Score':<12} {'Score Std':<10} {'Rollouts':<10} {'Updates':<8} {'Up/Ep':<8} {'ClipFrac':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for name, scores, agent in experiments:\n",
    "    final_window_size = min(CONFIG[\"window_length\"], len(scores))\n",
    "    final_score = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    \n",
    "    stats = agent.get_variance_stats()\n",
    "    ppo_stats = agent.get_ppo_stats()\n",
    "    param_count = agent.network.get_param_count()\n",
    "    \n",
    "    # Calculate updates per episode\n",
    "    total_updates = ppo_stats['policy_updates']\n",
    "    updates_per_episode = total_updates / len(scores) if len(scores) > 0 else 0.0\n",
    "    \n",
    "    print(f\"{name:<20} {final_score:<12.1f} {stats['score_std']:<10.1f} {ppo_stats['rollout_count']:<10} {total_updates:<8} {updates_per_episode:<8.1f} {stats['clip_fraction_mean']:<10.3f} {param_count:<12,}\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ PPO-SPECIFIC ANALYSIS:\")\n",
    "for name, scores, agent in experiments:\n",
    "    stats = agent.get_variance_stats()\n",
    "    ppo_stats = agent.get_ppo_stats()\n",
    "    recent_score_var = stats.get('recent_score_variance', 0.0)\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Score variance (last {CONFIG['window_length']} episodes): {recent_score_var:.1f}\")\n",
    "    print(f\"  Clip fraction: {stats['clip_fraction_mean']:.3f} (healthy: 0.1-0.3)\")\n",
    "    print(f\"  KL divergence: {stats['kl_divergence_mean']:.4f} (healthy: <0.01)\")\n",
    "    print(f\"  Data efficiency: {ppo_stats['steps_per_update']:.1f} env steps per policy update (lower the better)\")\n",
    "\n",
    "# Create PPO-specific analysis plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle('PPO Analysis: Performance and Training Dynamics', fontsize=16)\n",
    "\n",
    "colors = ['blue', 'red']\n",
    "smoothing_window = CONFIG[\"window_length\"]\n",
    "\n",
    "# 1. Performance comparison (episodes)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(scores) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(scores, window=smoothing_window)\n",
    "        episodes = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax1.plot(episodes, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax1.axhline(y=CONFIG[\"target_score\"], color='g', linestyle='--', label=f'Target ({CONFIG[\"target_score\"]})', alpha=0.7)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel(f'Score ({smoothing_window}-episode avg)')\n",
    "ax1.set_title('PPO Performance Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Clip fraction over time (rollouts)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.clip_fraction_history) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.clip_fraction_history, window=smoothing_window)\n",
    "        rollouts = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax2.plot(rollouts, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax2.axhline(y=CONFIG[\"clip_epsilon\"], color='orange', linestyle='--', label=f'Clip Epsilon ({CONFIG[\"clip_epsilon\"]})', alpha=0.7)\n",
    "ax2.set_xlabel('Rollout')\n",
    "ax2.set_ylabel('Clip Fraction')\n",
    "ax2.set_title('Policy Update Clipping Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. KL divergence over time (rollouts)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.kl_divergence_history) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.kl_divergence_history, window=smoothing_window)\n",
    "        rollouts = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax3.plot(rollouts, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Rollout')\n",
    "ax3.set_ylabel('KL Divergence')\n",
    "ax3.set_title('Policy Change Over Time')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Loss components over time (rollouts)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.loss_history[\"total_loss\"]) >= smoothing_window:\n",
    "        # Plot total loss\n",
    "        smoothed, offset = get_moving_average(agent.loss_history[\"total_loss\"], window=smoothing_window)\n",
    "        rollouts = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax4.plot(rollouts, smoothed, label=f'{name} Total', color=colors[i], linewidth=2)\n",
    "        \n",
    "        # Plot actor loss with transparency\n",
    "        if len(agent.loss_history[\"actor_loss\"]) >= smoothing_window:\n",
    "            actor_smoothed, _ = get_moving_average(agent.loss_history[\"actor_loss\"], window=smoothing_window)\n",
    "            ax4.plot(rollouts, actor_smoothed, label=f'{name} Actor', color=colors[i], alpha=0.6, linestyle='--')\n",
    "        \n",
    "        # Plot critic loss with transparency\n",
    "        if len(agent.loss_history[\"critic_loss\"]) >= smoothing_window:\n",
    "            critic_smoothed, _ = get_moving_average(agent.loss_history[\"critic_loss\"], window=smoothing_window)\n",
    "            ax4.plot(rollouts, critic_smoothed, label=f'{name} Critic', color=colors[i], alpha=0.6, linestyle=':')\n",
    "\n",
    "ax4.set_xlabel('Rollout')\n",
    "ax4.set_ylabel('Loss Value')\n",
    "ax4.set_title('Loss Components Over Time')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
