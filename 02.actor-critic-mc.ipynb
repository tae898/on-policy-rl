{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcce486",
   "metadata": {},
   "source": [
    "# Actor-Critic (Monte Carlo): Adding Baselines to Reduce Variance\n",
    "\n",
    "## üéØ The Variance Problem in REINFORCE\n",
    "\n",
    "From our REINFORCE experiments, we observed **extremely high variance** in gradient estimates. The same action taken in similar states could receive vastly different return signals depending on random episode outcomes. This makes learning unstable and slow.\n",
    "\n",
    "**The Core Issue**: $\\nabla J(\\theta) = \\mathbb{E}[\\nabla \\log \\pi_\\theta(a_t|s_t) \\cdot G_t]$\n",
    "\n",
    "Where $G_t$ (episode return) has **huge variance** because:\n",
    "- Episode outcomes depend on many future actions and environment randomness\n",
    "- One crash late in the episode ruins the signal for all earlier actions\n",
    "- No consideration of \"typical\" performance - each episode is treated independently\n",
    "\n",
    "## üß† Actor-Critic: The Baseline Solution\n",
    "\n",
    "**Actor-Critic** methods solve the variance problem by introducing a **baseline** - a reference point that doesn't change the expected gradient but reduces variance.\n",
    "\n",
    "### Mathematical Foundation: Baseline Invariance\n",
    "\n",
    "**Key Theorem**: For any baseline $b(s_t)$ that doesn't depend on the action $a_t$:\n",
    "\n",
    "$$\\mathbb{E}[\\nabla \\log \\pi_\\theta(a_t|s_t) \\cdot b(s_t)] = 0$$\n",
    "\n",
    "This means we can subtract any state-dependent baseline from our returns without changing the expected gradient:\n",
    "\n",
    "$$\\nabla J(\\theta) = \\mathbb{E}[\\nabla \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t))]$$\n",
    "\n",
    "**Intuition**: The baseline serves as a \"reference point\" - we only reinforce actions that performed **better than expected**.\n",
    "\n",
    "## üèóÔ∏è Actor-Critic Architecture\n",
    "\n",
    "Actor-Critic methods consist of two components:\n",
    "\n",
    "### üë§ Actor (Policy Network)\n",
    "- **Purpose**: Choose actions based on current state\n",
    "- **Output**: Policy distribution $\\pi_\\theta(a|s)$\n",
    "- **Training**: Updated using policy gradient with baseline-reduced variance\n",
    "- **Same as REINFORCE**: Uses log-probability gradients\n",
    "\n",
    "### üé≠ Critic (Value Network)  \n",
    "- **Purpose**: Estimate state values to serve as baseline\n",
    "- **Output**: State value estimate $V_\\phi(s)$\n",
    "- **Training**: Minimize mean squared error with actual returns\n",
    "- **New Component**: This is what REINFORCE lacks!\n",
    "\n",
    "### üîó Shared Network Architecture\n",
    "\n",
    "Now we have both actor and critic neural networks, but we'll use a **shared architecture** design:\n",
    "\n",
    "1. **Shared Feature Layer**: Both actor and critic share the first layer since they're processing the same observations and learning related representations\n",
    "2. **Separate Heads**: After the shared features, we have separate branches for actor and critic outputs\n",
    "\n",
    "**Architecture Configuration**:\n",
    "```python\n",
    "\"fc_out_features\": [64],    # Shared features (single layer)\n",
    "\"actor_features\": [64],     # Actor-specific layers after shared\n",
    "\"critic_features\": [64],    # Critic-specific layers after shared\n",
    "```\n",
    "\n",
    "**Why This Design?**\n",
    "- **Feature Sharing**: Both networks benefit from shared low-level feature extraction\n",
    "- **Specialization**: Separate heads allow each component to specialize in its task\n",
    "- **Efficiency**: Reduces total parameter count compared to completely separate networks\n",
    "\n",
    "### üìä Fair Parameter Comparison with REINFORCE\n",
    "\n",
    "To make a fair comparison with REINFORCE (which only learns the actor/policy network), we deliberately choose a parameter configuration that results in similar total parameter counts:\n",
    "\n",
    "**REINFORCE Configuration**:\n",
    "```python\n",
    "\"fc_out_features\": [64, 64, 64],  # Three layers for policy network (~9,500 parameters)\n",
    "```\n",
    "\n",
    "**Actor-Critic Configuration**:\n",
    "```python\n",
    "\"fc_out_features\": [64],    # Shared features\n",
    "\"actor_features\": [64],     # Actor branch  \n",
    "\"critic_features\": [64],    # Critic branch\n",
    "# Total: ~9,500 parameters (similar to REINFORCE)\n",
    "```\n",
    "\n",
    "**Parameter Breakdown**:\n",
    "- **Shared layer**: Processes 8D observations ‚Üí 64 features\n",
    "- **Actor head**: 64 features ‚Üí action outputs (4 discrete or 2 continuous)\n",
    "- **Critic head**: 64 features ‚Üí 1 value output\n",
    "- **Total**: Approximately 9,500 parameters (matching REINFORCE for fair comparison)\n",
    "\n",
    "This ensures that performance differences come from the **algorithmic improvements** (baselines, dual learning objectives) rather than simply having more model capacity.\n",
    "\n",
    "## üîÑ Two Baseline Approaches\n",
    "\n",
    "In this notebook, we'll implement **two different baseline strategies** to demonstrate the progression from simple to sophisticated variance reduction:\n",
    "\n",
    "### Approach 1: Global Average Return Baseline ($\\bar{G}$)\n",
    "\n",
    "**Baseline**: $b = \\bar{G} = \\frac{1}{N} \\sum_{i=1}^N G_0^{(i)}$\n",
    "\n",
    "Where:\n",
    "- $N$ = Total number of episodes experienced so far\n",
    "- $i$ = Episode index (superscript denotes episode number)\n",
    "- $G_0^{(i)}$ = Episode return starting from initial state of episode $i$\n",
    "\n",
    "**Why $G_0$ specifically?**\n",
    "- **Episode return**: $G_0^{(i)} = \\sum_{k=0}^{T-1} \\gamma^k r_{k+1}^{(i)}$ is the total discounted return for episode $i$\n",
    "- **Initial state value**: We use $G_0$ (return from time step 0) because it represents the **total episode performance**\n",
    "- **Global metric**: This single number summarizes how well the entire episode went\n",
    "- **State-independent**: Same baseline value used for all states within any episode\n",
    "\n",
    "**Mathematical Details**:\n",
    "- **Running average**: $\\bar{G} = \\frac{1}{N} \\sum_{i=1}^N G_0^{(i)}$ where $N$ increases with each completed episode\n",
    "- **Updated after each episode**: $\\bar{G}_{new} = \\frac{(N-1) \\cdot \\bar{G}_{old} + G_0^{(N)}}{N}$\n",
    "- **Applied to all timesteps**: For episode $i$, all timesteps use the same baseline: $b_t = \\bar{G}$ for all $t$\n",
    "\n",
    "**Characteristics**:\n",
    "- **Simple running average** of all episode returns seen so far\n",
    "- **State-independent**: Same baseline used for all states in all episodes\n",
    "- **Easy to implement**: Just track episode return average\n",
    "- **Limited effectiveness**: Doesn't account for state-specific expectations\n",
    "\n",
    "### Approach 2: State-Dependent Value Function Baseline ($V(s_t)$)\n",
    "\n",
    "**Baseline**: $b(s_t) = V_\\phi(s_t)$\n",
    "\n",
    "- **Learned value function** that estimates expected return from each state\n",
    "- **State-dependent**: Different baseline for different states\n",
    "- **More sophisticated**: Requires neural network and training\n",
    "- **Better variance reduction**: Accounts for state-specific expectations\n",
    "\n",
    "## üìä Expected Learning Progression\n",
    "\n",
    "We expect to see:\n",
    "\n",
    "1. **Global Average Baseline**: Moderate variance reduction compared to REINFORCE\n",
    "2. **Value Function Baseline**: Significant variance reduction and faster convergence\n",
    "3. **Still Monte Carlo**: Both use full episode returns $G_t$ (no bootstrapping yet)\n",
    "\n",
    "## üéØ Normalization in Actor-Critic Methods: The Complete Picture\n",
    "\n",
    "Both baseline approaches use **normalization strategies** to ensure stable learning. Understanding these normalizations is crucial for successful Actor-Critic implementation.\n",
    "\n",
    "### üìê Normalizing Advantages and Returns\n",
    "\n",
    "**1. Advantage Normalization for Actor** (Always Beneficial ‚úÖ):\n",
    "- **Purpose**: Balance updates across actions and stabilize gradient magnitudes\n",
    "- **When**: Applied to policy gradient updates in ALL RL algorithms\n",
    "- **Why Safe**: Preserves relative ordering while preventing extreme updates\n",
    "- **Implementation**: `(advantages - advantages.mean()) / (advantages.std() + 1e-8)`\n",
    "\n",
    "**2. Return Normalization for Critic** (Context-Dependent ‚öñÔ∏è):\n",
    "- **Purpose**: Stabilize value function learning when targets have poor scaling\n",
    "- **When**: Beneficial in MC methods, harmful in TD methods\n",
    "- **Why Context-Dependent**: Depends on the nature of the learning target\n",
    "\n",
    "### üîç When to Normalize Critic Targets\n",
    "\n",
    "**‚úÖ Critic Loss Normalization is Beneficial in Monte Carlo Methods**:\n",
    "\n",
    "In MC methods, the critic target is the raw episode return:\n",
    "$$y_t = G_t = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k}$$\n",
    "\n",
    "**Why normalization helps**:\n",
    "- **High variance**: $G_t$ can vary dramatically (LunarLander: -200 to +300)\n",
    "- **Poor scaling**: Raw returns may not be well-scaled for neural network training\n",
    "- **Stability**: Normalized targets lead to more stable value function learning\n",
    "- **Faster convergence**: MSE loss optimization is more efficient with consistent target scales\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Normalize returns for critic learning in MC methods\n",
    "returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "critic_loss = mse_loss(value_predictions, returns_normalized)\n",
    "```\n",
    "\n",
    "**‚ùå Critic Loss Normalization is Harmful in Temporal Difference Methods**:\n",
    "\n",
    "In TD methods (next notebook), the critic target is:\n",
    "$$y_t = r_t + \\gamma V(s_{t+1})$$\n",
    "\n",
    "**Why normalization breaks TD learning**:\n",
    "- **Breaks value function meaning**: The value function must maintain consistent scale across timesteps\n",
    "- **Inconsistent targets**: Normalizing $r_t + \\gamma V(s_{t+1})$ creates inconsistent value estimates\n",
    "- **Bootstrapping failure**: The recursive relationship $V(s) = r + \\gamma V(s')$ requires consistent scaling\n",
    "\n",
    "**Key Insight**: TD methods **bootstrap** (use $V(s_{t+1})$ to train $V(s_t)$), so the value function scale must be consistent across all states and timesteps.\n",
    "\n",
    "### üìê Approach 1: Global Average Baseline - Single Normalization\n",
    "\n",
    "**Implementation Strategy**:\n",
    "```python\n",
    "# 1. Calculate advantages using global average baseline\n",
    "advantages = returns - global_average_baseline\n",
    "\n",
    "# 2. Normalize advantages for stable actor updates\n",
    "advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# 3. Actor loss uses normalized advantages\n",
    "actor_loss = -(log_probs * advantages_normalized).mean()\n",
    "```\n",
    "\n",
    "**Why This Works**:\n",
    "- **Single normalization step**: Only need to stabilize the actor loss\n",
    "- **No critic learning**: Global average doesn't require neural network training\n",
    "- **Same as REINFORCE**: Uses the same advantage normalization technique from REINFORCE\n",
    "- **Moderate variance reduction**: Better than raw returns but limited by state-independence\n",
    "\n",
    "### üìê Approach 2: Value Function Baseline - Dual Normalization\n",
    "\n",
    "**Implementation Strategy** (Two-Stage Normalization):\n",
    "\n",
    "```python\n",
    "# Stage 1: Normalize returns for critic learning\n",
    "returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "# Stage 2: Train critic with normalized returns\n",
    "critic_loss = mse_loss(value_predictions, returns_normalized)\n",
    "\n",
    "# Stage 3: Calculate advantages using learned baseline\n",
    "advantages = returns_normalized - value_predictions.detach()\n",
    "\n",
    "# Stage 4: Normalize advantages for actor learning  \n",
    "advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# Stage 5: Actor loss uses normalized advantages\n",
    "actor_loss = -(log_probs * advantages_normalized).mean()\n",
    "```\n",
    "\n",
    "#### üéØ Stage 1: Return Normalization for Critic Learning\n",
    "\n",
    "**Why normalize returns for critic?**\n",
    "\n",
    "**The Problem**: Raw returns in LunarLander vary dramatically:\n",
    "- **Successful episodes**: $G_t \\in [200, 300]$\n",
    "- **Failed episodes**: $G_t \\in [-200, -100]$  \n",
    "- **Mediocre episodes**: $G_t \\in [-50, 50]$\n",
    "\n",
    "**Without return normalization**:\n",
    "$$\\begin{align}\n",
    "\\text{Episode 1: } G_t &= [287, 275, 260, 245, \\ldots] \\quad \\text{(Success)} \\\\\n",
    "\\text{Episode 2: } G_t &= [-156, -145, -130, -115, \\ldots] \\quad \\text{(Crash)} \\\\\n",
    "\\text{Episode 3: } G_t &= [23, 15, 8, -5, \\ldots] \\quad \\text{(Mediocre)}\n",
    "\\end{align}$$\n",
    "\n",
    "**With return normalization**:\n",
    "$$\\begin{align}\n",
    "\\text{Episode 1: } G_t^{\\text{norm}} &= [1.5, 1.2, 0.8, 0.4, \\ldots] \\quad \\text{(Success - positive)} \\\\\n",
    "\\text{Episode 2: } G_t^{\\text{norm}} &= [-1.3, -1.0, -0.7, -0.4, \\ldots] \\quad \\text{(Crash - negative)} \\\\\n",
    "\\text{Episode 3: } G_t^{\\text{norm}} &= [0.2, 0.1, -0.1, -0.2, \\ldots] \\quad \\text{(Mediocre - near zero)}\n",
    "\\end{align}$$\n",
    "\n",
    "**Critic learning challenges**:\n",
    "- **Extreme targets**: Value network struggles with targets ranging from -200 to +300\n",
    "- **Slow convergence**: Large target variance makes MSE loss optimization difficult\n",
    "- **Poor generalization**: Network focuses on extreme values, ignores subtle patterns\n",
    "\n",
    "**Benefits for critic learning**:\n",
    "- **Consistent scale**: All targets in similar range for stable neural network training\n",
    "- **Faster convergence**: MSE loss optimization is more efficient\n",
    "- **Better generalization**: Network learns relative performance patterns\n",
    "- **Numerical stability**: Avoids gradient explosion/vanishing issues\n",
    "\n",
    "#### üéØ Stage 2: Advantage Normalization for Actor Learning\n",
    "\n",
    "**Why normalize advantages again?**\n",
    "\n",
    "Even with normalized returns and learned baselines, advantages can still have problematic variance:\n",
    "\n",
    "**Before advantage normalization**:\n",
    "$$\\begin{align}\n",
    "\\text{Episode 1: } A &= [1.2, 0.8, 0.3, -0.1, \\ldots] \\quad \\text{(Mixed signs, varying scale)} \\\\\n",
    "\\text{Episode 2: } A &= [-0.9, -0.5, -0.2, 0.1, \\ldots] \\quad \\text{(Different variance)} \\\\\n",
    "\\text{Episode 3: } A &= [0.5, 0.3, 0.1, -0.2, \\ldots] \\quad \\text{(Another scale)}\n",
    "\\end{align}$$\n",
    "\n",
    "**After advantage normalization**:\n",
    "$$\\begin{align}\n",
    "\\text{Episode 1: } A^{\\text{norm}} &= [1.1, 0.6, 0.1, -0.8, \\ldots] \\quad \\text{(Consistent scale)} \\\\\n",
    "\\text{Episode 2: } A^{\\text{norm}} &= [-1.0, -0.3, 0.2, 0.9, \\ldots] \\quad \\text{(Same variance)} \\\\\n",
    "\\text{Episode 3: } A^{\\text{norm}} &= [0.9, 0.4, 0.1, -0.6, \\ldots] \\quad \\text{(Balanced updates)}\n",
    "\\end{align}$$\n",
    "\n",
    "**Benefits for actor learning**:\n",
    "- **Stable policy updates**: Prevents destructively large gradient steps\n",
    "- **Consistent learning rate**: Same effective learning rate across episodes\n",
    "- **Better convergence**: More predictable optimization dynamics\n",
    "- **Relative importance preserved**: Actions that are relatively better still get stronger signals\n",
    "\n",
    "### üî¨ Mathematical Justification: Why Dual Normalization Works\n",
    "\n",
    "**Critic Learning (Stage 1)**:\n",
    "$$\\mathcal{L}_{\\text{critic}} = \\mathbb{E}\\left[\\left(V_\\phi(s_t) - \\frac{G_t - \\mu_G}{\\sigma_G}\\right)^2\\right]$$\n",
    "\n",
    "Where $\\mu_G$ and $\\sigma_G$ are the episode return mean and standard deviation.\n",
    "\n",
    "**Actor Learning (Stage 2)**:\n",
    "$$\\mathcal{L}_{\\text{actor}} = -\\mathbb{E}\\left[\\nabla \\log \\pi_\\theta(a_t|s_t) \\cdot \\frac{A_t - \\mu_A}{\\sigma_A}\\right]$$\n",
    "\n",
    "Where $A_t = \\frac{G_t - \\mu_G}{\\sigma_G} - V_\\phi(s_t)$ and $\\mu_A, \\sigma_A$ are advantage mean and standard deviation.\n",
    "\n",
    "**Key Insight**: Each component (critic and actor) receives inputs **optimized for its learning dynamics**:\n",
    "- **Critic**: Gets normalized targets that facilitate stable regression\n",
    "- **Actor**: Gets normalized advantages that enable stable policy gradient updates\n",
    "\n",
    "### üéØ Normalization Comparison: REINFORCE vs Actor-Critic\n",
    "\n",
    "**REINFORCE (Single Normalization)**:\n",
    "```python\n",
    "# Only normalize returns for actor learning\n",
    "returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "actor_loss = -(log_probs * returns_normalized).mean()\n",
    "```\n",
    "\n",
    "**Actor-Critic Global Average (Single Normalization)**:\n",
    "```python\n",
    "# Same as REINFORCE - normalize advantages for actor learning\n",
    "advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "actor_loss = -(log_probs * advantages_normalized).mean()\n",
    "```\n",
    "\n",
    "**Actor-Critic Value Function (Dual Normalization)**:\n",
    "```python\n",
    "# Step 1: Normalize returns for critic learning\n",
    "returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "critic_loss = mse_loss(values, returns_normalized)\n",
    "\n",
    "# Step 2: Normalize advantages for actor learning\n",
    "advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "actor_loss = -(log_probs * advantages_normalized).mean()\n",
    "```\n",
    "\n",
    "### üìä Expected Normalization Benefits\n",
    "\n",
    "**Approach 1 (Global Average)**:\n",
    "- ‚úÖ **Stable actor learning**: Advantage normalization prevents destructive updates\n",
    "- ‚úÖ **Simple implementation**: Only one normalization step\n",
    "- ‚ùå **Limited variance reduction**: State-independent baseline has fundamental limits\n",
    "\n",
    "**Approach 2 (Value Function)**:\n",
    "- ‚úÖ **Stable critic learning**: Return normalization enables effective value function learning\n",
    "- ‚úÖ **Stable actor learning**: Advantage normalization maintains policy gradient stability\n",
    "- ‚úÖ **Superior variance reduction**: State-dependent baseline provides better reference point\n",
    "- ‚úÖ **Faster convergence**: Both components learn effectively\n",
    "\n",
    "## üîÑ Actor-Critic Monte Carlo Algorithm\n",
    "\n",
    "**Algorithm: Actor-Critic Monte Carlo**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Unified Actor-Critic network with parameters $\\theta$\n",
    "- Learning rate $\\alpha$\n",
    "- Discount factor $\\gamma$\n",
    "- Number of episodes $N$\n",
    "- **Loss coefficient**:\n",
    "  - $c_V$: Critic loss coefficient (typically 0.5, only for Approach 2)\n",
    "\n",
    "**Output:** \n",
    "- Trained unified network parameters $\\theta$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** network parameters $\\theta$ randomly\n",
    "2. **Initialize** global return average $\\bar{G} = 0$ (for Approach 1)\n",
    "3. **For** $i = 1, 2, ..., N$ **do:**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;**Generate trajectory** $\\tau = (s_0, a_0, r_1, s_1, ..., s_{T-1}, a_{T-1}, r_T)$ using policy $\\pi_\\theta$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;**For** $t = 0, 1, ..., T-1$ **do:**\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Calculate return-to-go:** $G_t \\leftarrow \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;**Update global average:** $\\bar{G} \\leftarrow \\text{running\\_average}(G_0)$\n",
    "9. &nbsp;&nbsp;&nbsp;&nbsp;**For** $t = 0, 1, ..., T-1$ **do:**\n",
    "10. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Choose baseline:** $b_t \\leftarrow \\bar{G}$ (Approach 1) or $b_t \\leftarrow V_\\theta(s_t)$ (Approach 2)\n",
    "11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Calculate advantage:** $A_t \\leftarrow G_t - b_t$\n",
    "12. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Normalize advantages:** $\\hat{A_t} \\leftarrow \\frac{A_t - \\mu_A}{\\sigma_A + \\epsilon}$\n",
    "13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Actor loss:** $L_\\pi(\\theta) \\leftarrow -\\mathbb{E}[\\log \\pi_\\theta(a_t|s_t) \\cdot \\hat{A_t}]$\n",
    "14. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**If** Approach 2 **then:**\n",
    "15. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Normalize returns:** $\\hat{G_t} \\leftarrow \\frac{G_t - \\mu_G}{\\sigma_G + \\epsilon}$\n",
    "16. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Critic loss:** $L_V(\\theta) \\leftarrow \\mathbb{E}[(V_\\theta(s_t) - \\hat{G_t})^2]$\n",
    "17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Total loss:** $L(\\theta) \\leftarrow L_\\pi(\\theta) + c_V L_V(\\theta)$\n",
    "18. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Else** (Approach 1):\n",
    "19. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Total loss:** $L(\\theta) \\leftarrow L_\\pi(\\theta)$ (no critic learning)\n",
    "20. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Update parameters:** $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta L(\\theta)$\n",
    "21. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "22. **End For**\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Understanding the Loss Coefficients\n",
    "\n",
    "**For Approach 1 (Global Average Baseline)**:\n",
    "- **No critic loss coefficient needed**: Only the actor is trained (policy gradients only)\n",
    "- **Simple loss**: $L(\\theta) = L_\\pi(\\theta)$ \n",
    "- **Baseline is fixed**: Global average doesn't require gradient updates\n",
    "\n",
    "**For Approach 2 (Value Function Baseline)**:\n",
    "- **Critic Loss Coefficient ($c_V = 0.5$)**: \n",
    "  - **Purpose**: Balance actor and critic learning in the unified network\n",
    "  - **Why needed**: Both actor and critic share parameters in early layers\n",
    "  - **Effect**: Controls how much the value function learning influences shared features\n",
    "  - **Our choice**: 0.5 provides balanced learning between policy improvement and value estimation\n",
    "\n",
    "**Mathematical Comparison**:\n",
    "\n",
    "**Approach 1**: $L(\\theta) = -\\mathbb{E}[\\log \\pi_\\theta(a_t|s_t) \\cdot \\hat{A_t}]$ where $A_t = G_t - \\bar{G}$\n",
    "\n",
    "**Approach 2**: $L(\\theta) = -\\mathbb{E}[\\log \\pi_\\theta(a_t|s_t) \\cdot \\hat{A_t}] + c_V \\mathbb{E}[(V_\\theta(s_t) - \\hat{G_t})^2]$ where $A_t = \\hat{G_t} - V_\\theta(s_t)$\n",
    "\n",
    "**Why Different Coefficient Strategies?**\n",
    "- **Shared representation learning**: Both actor and critic benefit from shared features\n",
    "- **Unified optimization**: Single optimizer updates all parameters simultaneously\n",
    "- **Balanced gradients**: Coefficient ensures neither component dominates the learning\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Advantages over REINFORCE\n",
    "\n",
    "### ‚úÖ Advantages\n",
    "\n",
    "1. **Reduced Variance**: Baselines significantly reduce gradient variance\n",
    "2. **Faster Convergence**: Lower variance leads to more stable learning\n",
    "3. **Better Sample Efficiency**: More informative gradients per episode\n",
    "4. **Unbiased Estimates**: Baselines don't change expected gradients\n",
    "5. **State-Aware Learning**: Value function baseline considers state-specific expectations\n",
    "\n",
    "### üîÑ Still Maintains\n",
    "\n",
    "1. **Monte Carlo Learning**: Uses full episode returns (no bootstrapping)\n",
    "2. **On-Policy**: Uses only current policy data\n",
    "3. **Unbiased**: No approximation bias in return estimates\n",
    "4. **Action Space Flexibility**: Works with both discrete and continuous actions\n",
    "\n",
    "### ‚ùå Limitations\n",
    "\n",
    "1. **Episode-Based**: Still must wait for complete episodes\n",
    "2. **No Credit Assignment**: All episode steps get same return signal\n",
    "3. **Function Approximation Error**: Value function introduces approximation\n",
    "4. **Still High Variance**: Variance reduction but not elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da8f009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Global random seeds set to 42 for reproducible results\n",
      "üìù Environment episodes will use seeds 42 + episode_number for varied but reproducible episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our RL utilities including the new ActorCriticNetwork\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    ActorCriticNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 5e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"window_length\": 50,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"num_videos\": 9,  # Number of videos to record during training\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"network\": {\n",
    "        \"fc_out_features\": [64],  # Shared features\n",
    "        \"actor_features\": [64],  # Actor-specific layers after shared\n",
    "        \"critic_features\": [64],  # Critic-specific layers after shared\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.0,\n",
    "    },\n",
    "    # Actor-Critic Specific Parameters\n",
    "    \"critic_loss_coeff\": 0.5,  # Weight for critic loss in total loss\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"üé≤ Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"üìù Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0a6892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticMCAgent:\n",
    "    \"\"\"Actor-Critic Monte Carlo agent with two baseline approaches.\"\"\"\n",
    "\n",
    "    def __init__(self, network, config, baseline_type):\n",
    "        \"\"\"\n",
    "        Initialize Actor-Critic agent.\n",
    "\n",
    "        Args:\n",
    "            network: ActorCriticNetwork instance\n",
    "            config: Configuration dictionary\n",
    "            baseline_type: \"global_average\" or \"value_function\"\n",
    "        \"\"\"\n",
    "        self.network = network.to(config[\"device\"])\n",
    "        self.baseline_type = baseline_type\n",
    "        self.device = config[\"device\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.window_size = config.get(\"window_length\")\n",
    "\n",
    "        # Actor-Critic specific parameters\n",
    "        self.critic_loss_coeff = config.get(\n",
    "            \"critic_loss_coeff\",\n",
    "        )\n",
    "\n",
    "        # Single optimizer for all network parameters (simplified approach)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=config[\"lr\"],  # Use single learning rate from config\n",
    "        )\n",
    "\n",
    "        # Print detailed network information\n",
    "        print(f\"üìä ACTOR-CRITIC NETWORK DETAILS:\")\n",
    "        self.network.print_network_info()\n",
    "        print(f\"üéØ Baseline Type: {baseline_type}\")\n",
    "        print(f\"üéì Learning Rate: {config['lr']} (shared for actor & critic)\")\n",
    "        print(f\"‚öñÔ∏è Critic Loss Coefficient: {self.critic_loss_coeff}\")\n",
    "\n",
    "        # Episode-specific storage\n",
    "        self.log_probs = []\n",
    "        self.values = []  # Store value predictions\n",
    "        self.rewards = []\n",
    "        self.states = []  # Store states for value function training\n",
    "\n",
    "        # Global baseline tracking (for global_average mode)\n",
    "        self.global_return_sum = 0.0\n",
    "        self.global_return_count = 0\n",
    "        self.global_return_average = 0.0\n",
    "\n",
    "        # Variance and performance tracking\n",
    "        self.gradient_norms = []\n",
    "        self.episode_scores = []  # Raw undiscounted episode scores\n",
    "        self.episode_returns = []  # Discounted episode returns (G_0)\n",
    "        self.score_variance_history = []\n",
    "        self.return_variance_history = []\n",
    "\n",
    "        # Advantage tracking for analysis\n",
    "        self.advantages = []\n",
    "        self.baselines_used = []  # Track which baseline was used\n",
    "\n",
    "        # Update step tracking\n",
    "        self.update_step = 0\n",
    "        self.update_steps_history = []\n",
    "\n",
    "        # Loss component tracking\n",
    "        self.loss_history = {\n",
    "            \"actor_loss\": [],  # Policy loss\n",
    "            \"critic_loss\": [],  # Value function loss\n",
    "            \"total_loss\": [],  # Combined loss\n",
    "        }\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action and store value prediction.\"\"\"\n",
    "        # Normalize observation before feeding to network\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Get policy distribution and value estimate\n",
    "        dist, value = self.network(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Store log probability and value prediction\n",
    "        if self.network.is_continuous:\n",
    "            log_prob = dist.log_prob(action).sum(-1)\n",
    "            action_to_env = self.network.clip_action(action).flatten()\n",
    "        else:\n",
    "            log_prob = dist.log_prob(action)\n",
    "            action_to_env = action.item()\n",
    "\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.states.append(state)\n",
    "\n",
    "        return action_to_env\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update both actor and critic using collected episode data.\"\"\"\n",
    "        if not self.log_probs:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        self.update_step += 1\n",
    "\n",
    "        # 1. Calculate discounted returns (G_t)\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for r in reversed(self.rewards):\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            returns.insert(0, discounted_reward)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Store both episode score and discounted return for variance tracking\n",
    "        episode_score = sum(self.rewards)  # Raw undiscounted sum\n",
    "        episode_return = returns[0].item() if len(returns) > 0 else 0.0  # Discounted G_0\n",
    "        self.episode_scores.append(episode_score)\n",
    "        self.episode_returns.append(episode_return)\n",
    "\n",
    "        # Update global baseline (for global_average mode)\n",
    "        self.global_return_sum += episode_return\n",
    "        self.global_return_count += 1\n",
    "        self.global_return_average = self.global_return_sum / self.global_return_count\n",
    "\n",
    "        # Track both score and return variance over recent episodes\n",
    "        if len(self.episode_scores) >= self.window_size:\n",
    "            recent_scores = self.episode_scores[-self.window_size:]\n",
    "            score_variance = np.var(recent_scores)\n",
    "            self.score_variance_history.append(score_variance)\n",
    "            \n",
    "            recent_returns = self.episode_returns[-self.window_size:]\n",
    "            return_variance = np.var(recent_returns)\n",
    "            self.return_variance_history.append(return_variance)\n",
    "\n",
    "        # 2. Calculate baselines and advantages\n",
    "        if self.baseline_type == \"global_average\":\n",
    "            baselines = torch.full_like(returns, self.global_return_average)\n",
    "            advantages = returns - baselines\n",
    "            critic_loss = torch.tensor(0.0, device=self.device)\n",
    "\n",
    "        else:\n",
    "            baselines = torch.stack(self.values)\n",
    "            returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            advantages = returns_normalized - baselines.detach()\n",
    "            critic_loss = torch.nn.functional.mse_loss(baselines, returns_normalized)\n",
    "\n",
    "        # Track advantage statistics\n",
    "        self.advantages.extend(advantages.detach().cpu().numpy())\n",
    "        self.baselines_used.extend(baselines.detach().cpu().numpy())\n",
    "\n",
    "        # Normalize advantages for actor loss stability\n",
    "        advantages_normalized = (advantages - advantages.mean()) / (\n",
    "            advantages.std() + 1e-8\n",
    "        )\n",
    "\n",
    "        # 3. Actor loss\n",
    "        log_probs_tensor = torch.stack(self.log_probs)\n",
    "        actor_loss = -(log_probs_tensor * advantages_normalized).mean()\n",
    "\n",
    "        # 4. Total loss\n",
    "        total_loss = actor_loss + self.critic_loss_coeff * critic_loss\n",
    "\n",
    "        # 5. Update network\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Record gradient norm\n",
    "        total_grad_norm = 0.0\n",
    "        for param in self.network.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "        total_grad_norm = total_grad_norm**0.5\n",
    "        self.gradient_norms.append(total_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 6. Log losses\n",
    "        actor_loss_value = actor_loss.item()\n",
    "        critic_loss_value = critic_loss.item()\n",
    "        total_loss_value = total_loss.item()\n",
    "        self.loss_history[\"actor_loss\"].append(actor_loss_value)\n",
    "        self.loss_history[\"critic_loss\"].append(critic_loss_value)\n",
    "        self.loss_history[\"total_loss\"].append(total_loss_value)\n",
    "        self.update_steps_history.append(self.update_step)\n",
    "\n",
    "        # 7. Clear buffers\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "\n",
    "        return {\n",
    "            \"actor_loss\": actor_loss_value,\n",
    "            \"critic_loss\": critic_loss_value,\n",
    "            \"total_loss\": total_loss_value,\n",
    "        }, total_grad_norm\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.episode_scores) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"score_mean\": 0.0,\n",
    "                \"score_std\": 0.0,\n",
    "                \"return_mean\": 0.0,\n",
    "                \"return_std\": 0.0,\n",
    "                \"recent_score_variance\": 0.0,\n",
    "                \"recent_return_variance\": 0.0,\n",
    "                \"advantage_mean\": 0.0,\n",
    "                \"advantage_std\": 0.0,\n",
    "                \"baseline_mean\": 0.0,\n",
    "                \"baseline_std\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"score_mean\": np.mean(self.episode_scores),\n",
    "            \"score_std\": np.std(self.episode_scores),\n",
    "            \"return_mean\": np.mean(self.episode_returns),\n",
    "            \"return_std\": np.std(self.episode_returns),\n",
    "            \"recent_score_variance\": (\n",
    "                self.score_variance_history[-1]\n",
    "                if self.score_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "            \"recent_return_variance\": (\n",
    "                self.return_variance_history[-1]\n",
    "                if self.return_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "            \"advantage_mean\": np.mean(self.advantages) if self.advantages else 0.0,\n",
    "            \"advantage_std\": np.std(self.advantages) if self.advantages else 0.0,\n",
    "            \"baseline_mean\": (\n",
    "                np.mean(self.baselines_used) if self.baselines_used else 0.0\n",
    "            ),\n",
    "            \"baseline_std\": np.std(self.baselines_used) if self.baselines_used else 0.0,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c32cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic_mc(is_continuous, config, baseline_type=\"value_function\"):\n",
    "    \"\"\"Main training loop for the Actor-Critic Monte Carlo agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ACTOR-CRITIC MC ({action_type.upper()}) - {baseline_type.replace('_', ' ').title()} Baseline\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate video recording interval\n",
    "    video_record_interval = max(1, config[\"episodes\"] // config[\"num_videos\"])\n",
    "    print(f\"üìπ Recording {config['num_videos']} videos every {video_record_interval} episodes\")\n",
    "    \n",
    "    # Create algorithm-specific video folder\n",
    "    video_folder = f\"videos/ActorCritic_MC_{action_type.lower()}_{baseline_type}\"\n",
    "    config_with_videos = config.copy()\n",
    "    config_with_videos[\"video_folder\"] = video_folder\n",
    "    config_with_videos[\"video_record_interval\"] = video_record_interval\n",
    "    \n",
    "    # Create Environment\n",
    "    env = create_env_with_wrappers(\n",
    "        config_with_videos, \n",
    "        is_continuous, \n",
    "        record_videos=True, \n",
    "        video_prefix=f\"ac_mc_{action_type.lower()}_{baseline_type}\",\n",
    "        cleanup_existing=True\n",
    "    )\n",
    "    \n",
    "    # Get observation dimension and space\n",
    "    dummy_obs, _ = env.reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "    \n",
    "    # Create Actor-Critic Network and Agent\n",
    "    print(f\"\\nüèóÔ∏è CREATING {action_type.upper()} ACTOR-CRITIC NETWORK:\")\n",
    "    network = ActorCriticNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=env.action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"network\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ INITIALIZING {action_type.upper()} ACTOR-CRITIC AGENT:\")\n",
    "    agent = ActorCriticMCAgent(network, config, baseline_type=baseline_type)\n",
    "    \n",
    "    # Training Loop\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=config[\"window_length\"])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING {action_type.upper()} TRAINING...\")\n",
    "    \n",
    "    # Use tqdm for progress bar with detailed information\n",
    "    pbar = tqdm(range(1, config[\"episodes\"] + 1), desc=\"Training\", unit=\"episode\")\n",
    "    \n",
    "    for i_episode in pbar:\n",
    "        state, _ = env.reset(seed=config[\"seed\"] + i_episode)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        loss_dict, grad_norm = agent.update_policy()\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "        scores_window.append(ep_reward)\n",
    "        \n",
    "        # Update tqdm description with current statistics\n",
    "        avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "        actor_loss = loss_dict['actor_loss']\n",
    "        critic_loss = loss_dict['critic_loss']\n",
    "        total_loss = loss_dict['total_loss']\n",
    "        \n",
    "        pbar.set_description(\n",
    "            f\"Ep {i_episode:4d} | \"\n",
    "            f\"Score: {ep_reward:6.1f} | \"\n",
    "            f\"AvgScore({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "            f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "            f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "            f\"TotalLoss: {total_loss:7.4f} | \"\n",
    "            f\"GradNorm: {grad_norm:6.4f}\"\n",
    "        )\n",
    "        \n",
    "        # Handle video display\n",
    "        if i_episode % video_record_interval == 0 and config[\"record_videos\"]:\n",
    "            from rl_utils.environment import display_latest_video\n",
    "            pbar.write(f\"\\nVideo recorded at episode {i_episode}\")\n",
    "            display_latest_video(\n",
    "                config_with_videos[\"video_folder\"], \n",
    "                f\"ac_mc_{action_type.lower()}_{baseline_type}\", \n",
    "                i_episode\n",
    "            )\n",
    "    \n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_window_size = min(config[\"window_length\"], len(scores))\n",
    "    final_performance = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    print(f\"\\n{action_type} training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Final performance: {final_performance:.2f} (last {final_window_size} episodes)\")\n",
    "    \n",
    "    return scores, agent.loss_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc2655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ Displaying 3 training videos (episodes: [110, 221, 332]):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;\">\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 110</p>\n",
       "                <video width=\"300\" height=\"225\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/ActorCritic_MC_discrete_global_average/ac_mc_discrete_global_average-episode-110.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 221</p>\n",
       "                <video width=\"300\" height=\"225\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/ActorCritic_MC_discrete_global_average/ac_mc_discrete_global_average-episode-221.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 332</p>\n",
       "                <video width=\"300\" height=\"225\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/ActorCritic_MC_discrete_global_average/ac_mc_discrete_global_average-episode-332.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "        </div>\n",
       "        <style>\n",
       "            video {\n",
       "                border: 2px solid #ccc;\n",
       "                border-radius: 8px;\n",
       "                box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  333 | Score:  -49.3 | AvgScore(50):  -87.7 | ActorLoss:  0.0009 | CriticLoss:  0.0000 | TotalLoss:  0.0009 | GradNorm: 0.1161:  33%|‚ñà‚ñà‚ñà‚ñé      | 333/1000 [03:03<23:41,  2.13s/episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ 3 training videos available in videos/ActorCritic_MC_discrete_global_average\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  390 | Score:  -97.4 | AvgScore(50):  -39.6 | ActorLoss: -0.0281 | CriticLoss:  0.0000 | TotalLoss: -0.0281 | GradNorm: 0.2388:  39%|‚ñà‚ñà‚ñà‚ñâ      | 390/1000 [04:09<24:34,  2.42s/episode]"
     ]
    }
   ],
   "source": [
    "# --- DISCRETE ACTION SPACE: GLOBAL AVERAGE BASELINE ---\n",
    "print(\"Starting Actor-Critic MC training with DISCRETE actions and GLOBAL AVERAGE baseline...\")\n",
    "\n",
    "discrete_global_scores, discrete_global_losses, discrete_global_agent = train_actor_critic_mc(\n",
    "    is_continuous=False, \n",
    "    config=CONFIG, \n",
    "    baseline_type=\"global_average\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb3502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for discrete global average baseline\n",
    "plot_training_results(\n",
    "    discrete_global_scores, \n",
    "    discrete_global_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Discrete\", \n",
    "    algorithm_name=\"Actor-Critic MC (Global Avg)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_variance_analysis(\n",
    "    discrete_global_agent, \n",
    "    discrete_global_scores, \n",
    "    \"Discrete\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"Actor-Critic MC (Global Avg)\"\n",
    ")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_global_scores))\n",
    "final_avg = np.mean(discrete_global_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE GLOBAL AVERAGE TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_global_agent.network.get_param_count():,}\")\n",
    "print(f\"Global return average used as baseline: {discrete_global_agent.global_return_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8c08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DISCRETE ACTION SPACE: VALUE FUNCTION BASELINE ---\n",
    "print(\"Starting Actor-Critic MC training with DISCRETE actions and VALUE FUNCTION baseline...\")\n",
    "\n",
    "discrete_value_scores, discrete_value_losses, discrete_value_agent = train_actor_critic_mc(\n",
    "    is_continuous=False, \n",
    "    config=CONFIG, \n",
    "    baseline_type=\"value_function\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for discrete value function baseline\n",
    "plot_training_results(\n",
    "    discrete_value_scores, \n",
    "    discrete_value_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Discrete\", \n",
    "    algorithm_name=\"Actor-Critic MC (Value Function)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_variance_analysis(\n",
    "    discrete_value_agent, \n",
    "    discrete_value_scores, \n",
    "    \"Discrete\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"Actor-Critic MC (Value Function)\"\n",
    ")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_value_scores))\n",
    "final_avg = np.mean(discrete_value_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE VALUE FUNCTION TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_value_agent.network.get_param_count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6793886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE: GLOBAL AVERAGE BASELINE ---\n",
    "print(\"Starting Actor-Critic MC training with CONTINUOUS actions and GLOBAL AVERAGE baseline...\")\n",
    "\n",
    "continuous_global_scores, continuous_global_losses, continuous_global_agent = train_actor_critic_mc(\n",
    "    is_continuous=True, \n",
    "    config=CONFIG, \n",
    "    baseline_type=\"global_average\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e33cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for continuous global average baseline\n",
    "plot_training_results(\n",
    "    continuous_global_scores, \n",
    "    continuous_global_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Continuous\", \n",
    "    algorithm_name=\"Actor-Critic MC (Global Avg)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_variance_analysis(\n",
    "    continuous_global_agent, \n",
    "    continuous_global_scores, \n",
    "    \"Continuous\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"Actor-Critic MC (Global Avg)\"\n",
    ")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_global_scores))\n",
    "final_avg = np.mean(continuous_global_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS GLOBAL AVERAGE TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_global_agent.network.get_param_count():,}\")\n",
    "print(f\"Global return average used as baseline: {continuous_global_agent.global_return_average:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca39dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE: VALUE FUNCTION BASELINE ---\n",
    "print(\"Starting Actor-Critic MC training with CONTINUOUS actions and VALUE FUNCTION baseline...\")\n",
    "\n",
    "continuous_value_scores, continuous_value_losses, continuous_value_agent = train_actor_critic_mc(\n",
    "    is_continuous=True, \n",
    "    config=CONFIG, \n",
    "    baseline_type=\"value_function\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35190b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for continuous value function baseline\n",
    "plot_training_results(\n",
    "    continuous_value_scores, \n",
    "    continuous_value_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Continuous\", \n",
    "    algorithm_name=\"Actor-Critic MC (Value Function)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_variance_analysis(\n",
    "    continuous_value_agent, \n",
    "    continuous_value_scores, \n",
    "    \"Continuous\", \n",
    "    CONFIG, \n",
    "    algorithm_name=\"Actor-Critic MC (Value Function)\"\n",
    ")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_value_scores))\n",
    "final_avg = np.mean(continuous_value_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS VALUE FUNCTION TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_value_agent.network.get_param_count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c22076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPREHENSIVE ANALYSIS: Actor-Critic MC Baseline Comparison ---\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_utils.visualization import get_moving_average\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE ANALYSIS: Actor-Critic MC Baseline Comparison\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Collect all experiment results\n",
    "experiments = [\n",
    "    (\"Discrete Global Avg\", discrete_global_scores, discrete_global_agent),\n",
    "    (\"Discrete Value Func\", discrete_value_scores, discrete_value_agent),\n",
    "    (\"Continuous Global Avg\", continuous_global_scores, continuous_global_agent),\n",
    "    (\"Continuous Value Func\", continuous_value_scores, continuous_value_agent),\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä FINAL PERFORMANCE COMPARISON (last {CONFIG['window_length']} episodes):\")\n",
    "print(f\"{'Method':<25} {'Final Score':<12} {'Score Std':<10} {'Grad Std':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, scores, agent in experiments:\n",
    "    final_window_size = min(CONFIG[\"window_length\"], len(scores))\n",
    "    final_score = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    stats = agent.get_variance_stats()\n",
    "    param_count = agent.network.get_param_count()\n",
    "    \n",
    "    print(f\"{name:<25} {final_score:<12.1f} {stats['score_std']:<10.1f} {stats['gradient_norm_std']:<10.4f} {param_count:<12,}\")\n",
    "\n",
    "print(f\"\\nüìà BASELINE EFFECTIVENESS ANALYSIS:\")\n",
    "for name, scores, agent in experiments:\n",
    "    stats = agent.get_variance_stats()\n",
    "    recent_score_var = stats.get('recent_score_variance', 0.0)\n",
    "    recent_return_var = stats.get('recent_return_variance', 0.0)\n",
    "    \n",
    "    # Calculate baseline statistics\n",
    "    if hasattr(agent, 'advantages') and len(agent.advantages) > 0:\n",
    "        adv_mean = np.mean(agent.advantages)\n",
    "        adv_std = np.std(agent.advantages)\n",
    "        baseline_mean = np.mean(agent.baselines_used) if hasattr(agent, 'baselines_used') else 0.0\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Final score variance (last {CONFIG['window_length']} episodes): {recent_score_var:.1f}\")\n",
    "        print(f\"  Advantage: Œº={adv_mean:6.2f}, œÉ={adv_std:6.2f}\")\n",
    "        print(f\"  Baseline: Œº={baseline_mean:6.1f}\")\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle('Actor-Critic MC: Baseline Method Comparison', fontsize=16)\n",
    "\n",
    "colors = ['blue', 'darkblue', 'red', 'darkred']\n",
    "line_styles = ['-', '--', '-', '--']\n",
    "smoothing_window = CONFIG[\"window_length\"]\n",
    "\n",
    "# 1. Performance comparison\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(scores) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(scores, window=smoothing_window)\n",
    "        episodes = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax1.plot(episodes, smoothed, label=name, color=colors[i], linewidth=2, linestyle=line_styles[i])\n",
    "\n",
    "ax1.axhline(y=CONFIG[\"target_score\"], color='g', linestyle='--', label=f'Target ({CONFIG[\"target_score\"]})', alpha=0.7)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel(f'Score ({smoothing_window}-episode avg)')\n",
    "ax1.set_title('Performance Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Gradient stability comparison\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.gradient_norms) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.gradient_norms, window=smoothing_window)\n",
    "        episodes = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax2.plot(episodes, smoothed, label=name, color=colors[i], linewidth=2, linestyle=line_styles[i])\n",
    "\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel(f'Gradient Norm ({smoothing_window}-episode avg)')\n",
    "ax2.set_title('Gradient Stability Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Score variance over time\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.score_variance_history) > 0:\n",
    "        variance_start = CONFIG[\"window_length\"]\n",
    "        variance_episodes = range(variance_start, variance_start + len(agent.score_variance_history))\n",
    "        ax3.plot(variance_episodes, agent.score_variance_history, label=name, color=colors[i], alpha=0.7, linestyle=line_styles[i])\n",
    "\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel(f'Score Variance (last {CONFIG[\"window_length\"]} episodes)')\n",
    "ax3.set_title('Score Variance Over Time')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Advantage distribution comparison (for value function baselines)\n",
    "value_func_experiments = [(name, scores, agent) for name, scores, agent in experiments if \"Value Func\" in name]\n",
    "if value_func_experiments:\n",
    "    advantage_data = []\n",
    "    labels = []\n",
    "    colors_subset = []\n",
    "    \n",
    "    for i, (name, scores, agent) in enumerate(value_func_experiments):\n",
    "        if hasattr(agent, 'advantages') and len(agent.advantages) > 0:\n",
    "            advantage_data.append(agent.advantages)\n",
    "            labels.append(name.replace(' Value Func', '').replace(' ', '\\n'))\n",
    "            colors_subset.append('blue' if 'Discrete' in name else 'red')\n",
    "    \n",
    "    if advantage_data:\n",
    "        box_plot = ax4.boxplot(advantage_data, tick_labels=labels, patch_artist=True)\n",
    "        for patch, color in zip(box_plot['boxes'], colors_subset):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax4.set_ylabel('Advantage Values')\n",
    "        ax4.set_title('Advantage Distribution (Value Function Baselines)')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
