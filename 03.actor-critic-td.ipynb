{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2d41e9",
   "metadata": {},
   "source": [
    "# Actor-Critic (Temporal Difference): Bootstrapping for Sample Efficiency\n",
    "\n",
    "## üéØ The Leap from Monte Carlo to Temporal Difference\n",
    "\n",
    "Welcome to **Actor-Critic Temporal Difference (TD)** - where we make the crucial transition from using complete episode returns to **bootstrapping** with learned value estimates. This represents a fundamental shift in how we learn from experience.\n",
    "\n",
    "**The Core Innovation**: Instead of waiting for complete episodes, we can update our networks every **N steps** using the **Bellman Expectation Equation** to estimate future returns.\n",
    "\n",
    "## üìê The Bellman Expectation Equation: Foundation of TD Learning\n",
    "\n",
    "### Understanding the Two Bellman Equations\n",
    "\n",
    "There are **two different Bellman equations** in reinforcement learning, each serving different purposes:\n",
    "\n",
    "#### 1. Bellman Optimality Equation (Off-Policy Learning)\n",
    "$$V^*(s) = \\max_a \\mathbb{E}[r + \\gamma V^*(s') | s, a]$$\n",
    "\n",
    "- **Purpose**: Defines the optimal value function\n",
    "- **Used in**: Q-Learning, DQN, off-policy methods\n",
    "- **Characteristic**: Uses **max** operator over actions\n",
    "- **Target**: Find the best possible policy\n",
    "\n",
    "#### 2. Bellman Expectation Equation (On-Policy Learning)\n",
    "$$V^\\pi(s) = \\mathbb{E}_{\\pi}[r + \\gamma V^\\pi(s') | s]$$\n",
    "\n",
    "- **Purpose**: Defines value function for a specific policy $\\pi$\n",
    "- **Used in**: Actor-Critic methods, on-policy learning\n",
    "- **Characteristic**: Uses **expectation** under current policy\n",
    "- **Target**: Evaluate and improve current policy\n",
    "\n",
    "### Why We Use Bellman Expectation in Actor-Critic\n",
    "\n",
    "**Key Insight**: Since we're learning both a policy (actor) and its value function (critic) simultaneously, we need the **expectation equation** that evaluates our **current policy**, not the optimal policy.\n",
    "\n",
    "**Mathematical Foundation**:\n",
    "$$V^\\pi(s_t) = \\mathbb{E}_{\\pi}[r_{t+1} + \\gamma V^\\pi(s_{t+1}) | s_t]$$\n",
    "\n",
    "**Expanded Form**:\n",
    "$$V^\\pi(s_t) = \\sum_a \\pi(a|s_t) \\sum_{s'} P(s'|s_t, a) [r(s_t, a, s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "**Practical Implementation**: Since we don't know the environment dynamics $P(s'|s_t, a)$, we use **sample-based estimates**:\n",
    "\n",
    "$$V^\\pi(s_t) \\approx r_{t+1} + \\gamma V^\\pi(s_{t+1})$$\n",
    "\n",
    "This gives us the **TD target**: $y_t = r_{t+1} + \\gamma V(s_{t+1})$\n",
    "\n",
    "We avoid dropout in TD methods and use **layer normalization** and **gradient clipping** for stability instead.\n",
    "\n",
    "## üîÑ Bootstrapping: The Game Changer\n",
    "\n",
    "**Bootstrapping** means using our own estimates to update our estimates. Instead of waiting for actual returns $G_t$, we use:\n",
    "\n",
    "$$\\text{TD Target} = r_{t+1} + \\gamma V_\\phi(s_{t+1})$$\n",
    "\n",
    "### Monte Carlo vs Temporal Difference Comparison\n",
    "\n",
    "**Monte Carlo (Previous Notebook)**:\n",
    "- **Trajectory**: $s_0 \\xrightarrow{a_0} s_1 \\xrightarrow{a_1} s_2 \\xrightarrow{a_2} \\ldots \\xrightarrow{a_{T-1}} s_T$ (terminal)\n",
    "- **Target**: $G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\ldots + \\gamma^{T-t-1} r_T$\n",
    "- **Update**: After complete episode ($T$ steps)\n",
    "\n",
    "**Temporal Difference (This Notebook)**:\n",
    "- **N-Step Sequence**: $s_t \\xrightarrow{a_t} s_{t+1} \\xrightarrow{a_{t+1}} s_{t+2} \\xrightarrow{a_{t+2}} \\ldots \\xrightarrow{a_{t+N-1}} s_{t+N}$\n",
    "- **Target**: $G_t^{(N)} = r_{t+1} + \\gamma r_{t+2} + \\ldots + \\gamma^{N-1} r_{t+N} + \\gamma^N V_\\phi(s_{t+N})$\n",
    "- **Update**: Every $N$ steps (where $N \\ll T$ typically)\n",
    "\n",
    "### Advantages of Bootstrapping\n",
    "\n",
    "1. **Sample Efficiency**: Learn from incomplete episodes\n",
    "2. **Online Learning**: Update during episode, not just at the end\n",
    "3. **Lower Variance**: Bootstrapped estimates typically have lower variance than MC\n",
    "4. **Faster Learning**: More frequent updates accelerate convergence\n",
    "\n",
    "### The Bias-Variance Tradeoff\n",
    "\n",
    "**Monte Carlo**: \n",
    "- ‚úÖ **Unbiased**: $\\mathbb{E}[G_t] = V^\\pi(s_t)$ exactly\n",
    "- ‚ùå **High Variance**: Episode returns vary dramatically\n",
    "\n",
    "**Temporal Difference**:\n",
    "- ‚ùå **Biased**: $\\mathbb{E}[r + \\gamma V(s')] \\neq V^\\pi(s)$ if $V$ is inaccurate\n",
    "- ‚úÖ **Lower Variance**: Single-step rewards plus learned estimates\n",
    "\n",
    "**The Key Insight**: Early in training, our value estimates $V_\\phi(s_{t+1})$ are wrong, introducing bias. But as training progresses, $V_\\phi$ becomes more accurate, and the bias decreases while maintaining low variance.\n",
    "\n",
    "## üî¢ N-Step Updates: Configurable Bootstrapping\n",
    "\n",
    "**N-Step TD** provides a spectrum between Monte Carlo and 1-step TD:\n",
    "\n",
    "### N-Step Target Calculation\n",
    "\n",
    "For N-step updates, our target becomes:\n",
    "$$G_t^{(N)} = \\sum_{k=0}^{N-1} \\gamma^k r_{t+k+1} + \\gamma^N V_\\phi(s_{t+N})$$\n",
    "\n",
    "**Special Cases**:\n",
    "- **N=1**: Pure TD learning $G_t^{(1)} = r_{t+1} + \\gamma V(s_{t+1})$\n",
    "- **N=T** (episode length): Pure Monte Carlo $G_t^{(T)} = G_t$ (actual return)\n",
    "- **N=5**: Balanced approach (common choice)\n",
    "\n",
    "### Update Frequency Implications\n",
    "\n",
    "This creates an important difference in learning dynamics compared to Monte Carlo methods:\n",
    "\n",
    "**Monte Carlo Methods**:\n",
    "- **Episodes**: 1000 episodes\n",
    "- **Network Updates**: 1000 updates (one per episode)\n",
    "- **Update Frequency**: Low\n",
    "- **Learning Speed**: Slower but stable\n",
    "\n",
    "**N-Step TD Methods** (N=5):\n",
    "- **Episodes**: 1000 episodes\n",
    "- **Network Updates**: ~5000+ updates (multiple per episode)\n",
    "- **Update Frequency**: High\n",
    "- **Learning Speed**: Faster but potentially less stable\n",
    "\n",
    "**‚ö†Ô∏è Comparison Challenge**: Direct comparison between MC and TD methods is complex because:\n",
    "- Different update frequencies\n",
    "- Different learning dynamics\n",
    "- Different bias-variance profiles\n",
    "- Different data utilization patterns\n",
    "\n",
    "## üéØ Normalization Strategy in TD Methods\n",
    "\n",
    "### The Key Difference: Why No Critic Loss Normalization\n",
    "\n",
    "**In Actor-Critic MC** (Previous Notebook):\n",
    "```python\n",
    "# We normalized returns for critic learning\n",
    "returns_normalized = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "critic_loss = mse_loss(value_predictions, returns_normalized)\n",
    "```\n",
    "\n",
    "**In Actor-Critic TD** (This Notebook):\n",
    "```python\n",
    "# NO normalization of TD targets for critic learning\n",
    "td_targets = rewards + gamma * next_values\n",
    "critic_loss = mse_loss(value_predictions, td_targets)  # No normalization!\n",
    "```\n",
    "\n",
    "### Why This Difference Matters\n",
    "\n",
    "**Monte Carlo Targets** (High Variance):\n",
    "- **Nature**: Raw episode returns $G_t$ with extreme variance\n",
    "- **Range**: LunarLander returns span [-200, +300]\n",
    "- **Problem**: Poor scaling for neural network training\n",
    "- **Solution**: Normalization helps stabilize learning\n",
    "\n",
    "**TD Targets** (Lower Variance):\n",
    "- **Nature**: Bootstrapped estimates $r + \\gamma V(s')$\n",
    "- **Range**: Single rewards plus learned estimates (more controlled)\n",
    "- **Stability**: Naturally more stable than raw episode returns\n",
    "- **Risk**: Normalization would break the value function's meaning\n",
    "\n",
    "### Why Normalization Breaks TD Learning\n",
    "\n",
    "**The Recursive Relationship Problem**:\n",
    "```python\n",
    "# TD learning requires consistent scaling across timesteps\n",
    "V(s_t) = r_{t+1} + Œ≥ V(s_{t+1})  # This relationship must hold!\n",
    "\n",
    "# If we normalize targets:\n",
    "V(s_t) = normalize(r_{t+1} + Œ≥ V(s_{t+1}))  # Breaks recursion!\n",
    "```\n",
    "\n",
    "**Bootstrapping Requirement**: Since we use $V(s_{t+1})$ to train $V(s_t)$, the value function must maintain consistent scale across all states and timesteps.\n",
    "\n",
    "### Advantage Normalization: Still Essential\n",
    "\n",
    "While we don't normalize critic targets, we **absolutely still normalize advantages**:\n",
    "\n",
    "```python\n",
    "# Calculate advantages using TD targets\n",
    "advantages = td_targets - value_predictions.detach()\n",
    "\n",
    "# Normalize advantages for stable actor updates\n",
    "advantages_normalized = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# Actor loss uses normalized advantages\n",
    "actor_loss = -(log_probs * advantages_normalized).mean()\n",
    "```\n",
    "\n",
    "**Why Advantage Normalization Remains Important**:\n",
    "- **Stable Policy Updates**: Prevents destructively large gradient steps\n",
    "- **Consistent Learning Rate**: Maintains effective learning rate across different update batches\n",
    "- **Preserved Relative Importance**: Actions that are relatively better still get stronger signals\n",
    "\n",
    "**‚ö†Ô∏è TD-Specific Challenge**: Advantage normalization in TD methods is more prone to NaN issues due to smaller batch sizes (N-steps vs full episodes). With fewer samples, the computed standard deviation can be very small, leading to potential division by zero or near-zero during normalization. Care must be taken to ensure numerical stability in these cases. That's why in our implementation, if we run into a NaN issue, we'll only center the advantages, but not scale them.\n",
    "\n",
    "## ‚ö° Gradient Norm Clipping: Essential for TD Stability\n",
    "\n",
    "### Why TD Methods Need Gradient Clipping\n",
    "\n",
    "**The Instability Challenge**: TD-based Actor-Critic methods face unique stability challenges that make gradient clipping essential:\n",
    "\n",
    "#### 1. Bootstrapping Feedback Loop\n",
    "```bash\n",
    "Current Value Estimate ‚Üí TD Target ‚Üí Critic Update ‚Üí New Value Estimate ‚Üí ...\n",
    "```\n",
    "- **Self-Referential Learning**: Network learns from its own predictions\n",
    "- **Error Amplification**: Mistakes in value estimates compound over time\n",
    "- **Feedback Instability**: Poor estimates create poor targets, leading to worse estimates\n",
    "\n",
    "#### 2. High Variance from Multiple Sources\n",
    "- **Bootstrapped Targets**: $r + \\gamma V(s')$ depends on potentially inaccurate $V(s')$\n",
    "- **Correlated Samples**: Sequential states in episodes are highly correlated\n",
    "- **On-Policy Distribution**: Data distribution constantly shifts as policy improves\n",
    "- **Policy-Value Coupling**: Actor and critic updates affect each other\n",
    "\n",
    "#### 3. Early Training Chaos\n",
    "- **Random Initialization**: Value estimates start completely wrong\n",
    "- **Exploration Noise**: Policy exploration creates additional variance\n",
    "- **Learning Competition**: Actor and critic learning can interfere with each other\n",
    "\n",
    "### Gradient Clipping Implementation\n",
    "\n",
    "**Max Gradient Norm Clipping**:\n",
    "```python\n",
    "# Clip gradients to prevent explosive updates\n",
    "torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_grad_norm)\n",
    "```\n",
    "\n",
    "**How It Works**:\n",
    "1. **Calculate Total Gradient Norm**: $\\|g\\| = \\sqrt{\\sum_i \\|g_i\\|^2}$\n",
    "2. **Check Threshold**: If $\\|g\\| > \\text{max\\_grad\\_norm}$\n",
    "3. **Scale Down**: $g_{clipped} = g \\cdot \\frac{\\text{max\\_grad\\_norm}}{\\|g\\|}$\n",
    "4. **Preserve Direction**: Gradient direction unchanged, only magnitude reduced\n",
    "\n",
    "### Benefits of Gradient Clipping\n",
    "\n",
    "1. **Prevents Explosion**: Stops catastrophically large updates that destroy learning progress\n",
    "2. **Stabilizes Training**: Reduces variance in parameter updates\n",
    "3. **Enables Higher Learning Rates**: Can use more aggressive learning rates safely\n",
    "4. **Improves Convergence**: More stable path to optimal solution\n",
    "5. **Robustness**: Makes training less sensitive to hyperparameter choices\n",
    "\n",
    "### Implementation Note: Display Original Gradient Norms\n",
    "\n",
    "**Important**: In our training progress bar, we display the **original (unclipped) gradient norm** for monitoring purposes:\n",
    "\n",
    "```python\n",
    "# Calculate and store original gradient norm BEFORE clipping\n",
    "original_grad_norm = calculate_gradient_norm(network.parameters())\n",
    "\n",
    "# Apply gradient clipping\n",
    "torch.nn.utils.clip_grad_norm_(network.parameters(), max_grad_norm)\n",
    "\n",
    "# Display original norm in progress bar (not the clipped value)\n",
    "progress_bar.set_description(f\"GradNorm: {original_grad_norm:.4f}\")\n",
    "```\n",
    "\n",
    "**Why Display Original Norms**:\n",
    "- **Training Diagnostics**: Monitor if gradients are frequently being clipped\n",
    "- **Hyperparameter Tuning**: Adjust `max_grad_norm` based on typical gradient magnitudes\n",
    "- **Stability Assessment**: Track training stability over time\n",
    "- **Debug Information**: Identify potential training issues\n",
    "\n",
    "## üèóÔ∏è Actor-Critic TD Architecture\n",
    "\n",
    "The network architecture remains the same as Actor-Critic MC, but the learning algorithm changes fundamentally:\n",
    "\n",
    "### Network Components (Unchanged)\n",
    "- **Shared Features**: Common representation learning for both actor and critic\n",
    "- **Actor Head**: Policy distribution output (discrete or continuous)\n",
    "- **Critic Head**: Single value estimate output\n",
    "\n",
    "### Learning Algorithm (Revolutionized)\n",
    "- **Update Frequency**: Every N steps instead of every episode\n",
    "- **Target Calculation**: Bootstrapped TD targets instead of Monte Carlo returns\n",
    "- **Stability**: Gradient clipping for robust training\n",
    "\n",
    "## üîÑ Actor-Critic TD Algorithm\n",
    "\n",
    "**Algorithm: Actor-Critic Temporal Difference (N-Step)**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Actor-Critic network with parameters $\\theta$ (actor) and $\\phi$ (critic)\n",
    "- Learning rate $\\alpha$\n",
    "- Discount factor $\\gamma$\n",
    "- N-step parameter $N$\n",
    "- Maximum gradient norm $\\text{max\\_grad\\_norm}$\n",
    "- Number of episodes $M$\n",
    "\n",
    "**Output:** \n",
    "- Trained actor parameters $\\theta$\n",
    "- Trained critic parameters $\\phi$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** network parameters $\\theta, \\phi$ randomly\n",
    "2. **For** $i = 1, 2, ..., M$ **do:**\n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;**Initialize** episode: $s_0 \\leftarrow \\text{env.reset()}$\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;**Initialize** buffers: states, actions, rewards, log_probs, values $\\leftarrow$ empty\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;**Set** $t \\leftarrow 0$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;**While** episode not done **do:**\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Sample action**: $a_t \\sim \\pi_\\theta(\\cdot|s_t)$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Execute**: $s_{t+1}, r_{t+1} \\leftarrow \\text{env.step}(a_t)$\n",
    "9. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Store**: $(s_t, a_t, r_{t+1}, \\log \\pi_\\theta(a_t|s_t), V_\\phi(s_t))$\n",
    "10. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**If** buffer size $\\geq N$ or episode done **then:**\n",
    "11. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Calculate N-step targets**: $G_t^{(N)} = \\sum_{k=0}^{N-1} \\gamma^k r_{t+k+1} + \\gamma^N V_\\phi(s_{t+N})$\n",
    "12. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Calculate advantages**: $A_t = G_t^{(N)} - V_\\phi(s_t)$\n",
    "13. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Normalize advantages**: $\\hat{A_t} = \\frac{A_t - \\mu_A}{\\sigma_A + \\epsilon}$\n",
    "14. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Actor loss**: $L_\\pi = -\\mathbb{E}[\\log \\pi_\\theta(a_t|s_t) \\cdot \\hat{A_t}]$\n",
    "15. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Critic loss**: $L_V = \\mathbb{E}[(V_\\phi(s_t) - G_t^{(N)})^2]$ (no normalization!)\n",
    "16. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Total loss**: $L = L_\\pi + c_V L_V$\n",
    "17. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Compute gradients**: $g \\leftarrow \\nabla L$\n",
    "18. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Clip gradients**: $g \\leftarrow \\text{clip}(g, \\text{max\\_grad\\_norm})$\n",
    "19. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Update parameters**: $\\theta, \\phi \\leftarrow \\theta - \\alpha g, \\phi - \\alpha g$\n",
    "20. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Clear buffer** (keep last state for bootstrapping)\n",
    "21. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End If**\n",
    "22. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Increment**: $t \\leftarrow t + 1$\n",
    "23. &nbsp;&nbsp;&nbsp;&nbsp;**End While**\n",
    "24. **End For**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Advantages over Monte Carlo Methods\n",
    "\n",
    "### ‚úÖ Significant Improvements\n",
    "\n",
    "1. **Sample Efficiency**: Learn from partial episodes, not just complete ones\n",
    "2. **Online Learning**: Update during episodes for faster adaptation\n",
    "3. **Lower Variance**: Bootstrapped estimates reduce variance compared to raw returns\n",
    "4. **Faster Convergence**: More frequent updates accelerate learning\n",
    "5. **Scalability**: Works with very long or infinite episodes\n",
    "\n",
    "### üîÑ Maintained Benefits\n",
    "\n",
    "1. **On-Policy Learning**: Still uses current policy data\n",
    "2. **Action Space Flexibility**: Supports both discrete and continuous actions\n",
    "3. **Policy Gradient Foundation**: Built on solid theoretical foundation\n",
    "\n",
    "### ‚ùå New Challenges\n",
    "\n",
    "1. **Bias Introduction**: Bootstrapping introduces bias from inaccurate value estimates\n",
    "2. **Hyperparameter Sensitivity**: More parameters to tune (N, grad clipping, etc.)\n",
    "3. **Training Instability**: Feedback loops can cause training instability\n",
    "4. **Complexity**: More sophisticated algorithm with more failure modes\n",
    "\n",
    "### üîç Bias-Variance Analysis\n",
    "\n",
    "**The Central Tradeoff**:\n",
    "- **Monte Carlo**: High variance, zero bias ‚Üí slow but correct learning\n",
    "- **TD Learning**: Lower variance, some bias ‚Üí faster but potentially incorrect learning\n",
    "\n",
    "**Why TD Often Wins**: In practice, the variance reduction usually outweighs the bias introduction, leading to faster and more stable learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2cc185f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Global random seeds set to 42 for reproducible results\n",
      "üìù Environment episodes will use seeds 42 + episode_number for varied but reproducible episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import our RL utilities including the ActorCriticNetwork\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    ActorCriticNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 5e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"window_length\": 50,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"num_videos\": 9,  # Number of videos to record during training\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"network\": {\n",
    "        \"fc_out_features\": [64],  # Shared features\n",
    "        \"actor_features\": [64],  # Actor-specific layers after shared\n",
    "        \"critic_features\": [64],  # Critic-specific layers after shared\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.0,\n",
    "    },\n",
    "    # Actor-Critic Specific Parameters\n",
    "    \"critic_loss_coeff\": 0.5,  # Weight for critic loss in total loss\n",
    "    # TD-Specific Parameters\n",
    "    \"n_steps\": 5,  # N-step TD parameter\n",
    "    \"max_grad_norm\": 0.5,  # Maximum gradient norm for clipping\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"üé≤ Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"üìù Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61022bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTDAgent:\n",
    "    \"\"\"Actor-Critic Temporal Difference agent with N-step updates.\"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        \"\"\"\n",
    "        Initialize Actor-Critic TD agent.\n",
    "\n",
    "        Args:\n",
    "            network: ActorCriticNetwork instance\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.network = network.to(config[\"device\"])\n",
    "        self.device = config[\"device\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.n_steps = config[\"n_steps\"]\n",
    "        self.max_grad_norm = config[\"max_grad_norm\"]\n",
    "        self.window_size = config.get(\"window_length\")\n",
    "\n",
    "        # Actor-Critic specific parameters\n",
    "        self.critic_loss_coeff = config.get(\"critic_loss_coeff\")\n",
    "\n",
    "        # Single optimizer for all network parameters\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "        )\n",
    "\n",
    "        # Print detailed network information\n",
    "        print(f\"üìä ACTOR-CRITIC TD NETWORK DETAILS:\")\n",
    "        self.network.print_network_info()\n",
    "        print(f\"üéØ N-Steps: {self.n_steps}\")\n",
    "        print(f\"üéì Learning Rate: {config['lr']} (shared for actor & critic)\")\n",
    "        print(f\"‚öñÔ∏è Critic Loss Coefficient: {self.critic_loss_coeff}\")\n",
    "        print(f\"‚úÇÔ∏è Max Gradient Norm: {self.max_grad_norm}\")\n",
    "\n",
    "        # N-step buffer storage\n",
    "        self.reset_buffers()\n",
    "\n",
    "        # Variance and performance tracking\n",
    "        self.gradient_norms = []\n",
    "        self.episode_returns = []\n",
    "        self.return_variance_history = []\n",
    "\n",
    "        # Update step tracking\n",
    "        self.update_step = 0\n",
    "        self.update_steps_history = []\n",
    "\n",
    "        # Loss component tracking\n",
    "        self.loss_history = {\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"total_loss\": [],\n",
    "        }\n",
    "\n",
    "        # Advantage normalization fallback tracking\n",
    "        self.advantage_norm_stats = {\n",
    "            \"total_actor_updates\": 0,\n",
    "            \"fallback_normalizations\": 0,\n",
    "        }\n",
    "\n",
    "    def reset_buffers(self):\n",
    "        \"\"\"Reset N-step buffers for new episode.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.episode_return = 0.0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select an action and store necessary data.\"\"\"\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Get policy distribution and value estimate\n",
    "        dist, value = self.network(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # Store log probability and value prediction\n",
    "        if self.network.is_continuous:\n",
    "            log_prob = dist.log_prob(action).sum(-1)\n",
    "            action_to_env = self.network.clip_action(action).flatten()\n",
    "        else:\n",
    "            log_prob = dist.log_prob(action)\n",
    "            action_to_env = action.item()\n",
    "\n",
    "        # Store in buffers\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "\n",
    "        return action_to_env\n",
    "\n",
    "    def store_reward(self, reward):\n",
    "        \"\"\"Store reward in buffer.\"\"\"\n",
    "        self.rewards.append(reward)\n",
    "        self.episode_return += reward\n",
    "\n",
    "    def should_update(self, done):\n",
    "        \"\"\"Check if we should perform an update.\"\"\"\n",
    "        return len(self.rewards) >= self.n_steps or done\n",
    "\n",
    "    def update_policy(self, next_state=None, done=False):\n",
    "        \"\"\"Update both actor and critic using N-step TD targets.\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        self.update_step += 1\n",
    "        self.advantage_norm_stats[\"total_actor_updates\"] += 1\n",
    "\n",
    "        # Calculate N-step TD targets\n",
    "        td_targets = []\n",
    "        \n",
    "        # Get bootstrap value for the last state\n",
    "        if done:\n",
    "            bootstrap_value = 0.0  # Terminal state has value 0\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                next_state_tensor = torch.as_tensor(next_state, dtype=torch.float32, device=self.device)\n",
    "                _, bootstrap_value = self.network(next_state_tensor)\n",
    "                bootstrap_value = bootstrap_value.item()\n",
    "\n",
    "        # Calculate N-step returns\n",
    "        for i in range(len(self.rewards)):\n",
    "            td_target = 0.0\n",
    "            discount = 1.0\n",
    "            \n",
    "            # Sum discounted rewards for available steps\n",
    "            for j in range(i, len(self.rewards)):\n",
    "                td_target += discount * self.rewards[j]\n",
    "                discount *= self.gamma\n",
    "            \n",
    "            # Add bootstrapped value if not terminal\n",
    "            if not done or i < len(self.rewards) - 1:\n",
    "                td_target += discount * bootstrap_value\n",
    "            \n",
    "            td_targets.append(td_target)\n",
    "\n",
    "        # Convert to tensors\n",
    "        td_targets = torch.tensor(td_targets, dtype=torch.float32, device=self.device)\n",
    "        values_tensor = torch.stack(self.values)\n",
    "        log_probs_tensor = torch.stack(self.log_probs)\n",
    "\n",
    "        # Calculate advantages (no normalization of TD targets for critic!)\n",
    "        advantages = td_targets - values_tensor.detach()\n",
    "\n",
    "        # Robust advantage normalization: only check for NaN after normalization\n",
    "        advantages_mean = advantages.mean()\n",
    "        advantages_std = advantages.std()\n",
    "\n",
    "        # Normalize advantages, fallback to centering if NaN occurs\n",
    "        advantages_normalized = (advantages - advantages_mean) / (advantages_std + 1e-8)\n",
    "        if torch.isnan(advantages_normalized).any():\n",
    "            self.advantage_norm_stats[\"fallback_normalizations\"] += 1\n",
    "            advantages_normalized = advantages - advantages_mean\n",
    "\n",
    "        actor_loss = -(log_probs_tensor * advantages_normalized).mean()\n",
    "\n",
    "        # Critic loss (no normalization of targets!)\n",
    "        critic_loss = torch.nn.functional.mse_loss(values_tensor, td_targets)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = actor_loss + self.critic_loss_coeff * critic_loss\n",
    "\n",
    "        # Update network with gradient clipping\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Record gradient norm BEFORE clipping\n",
    "        total_grad_norm = 0.0\n",
    "        for param in self.network.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "        total_grad_norm = total_grad_norm**0.5\n",
    "        self.gradient_norms.append(total_grad_norm)\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log losses\n",
    "        actor_loss_value = actor_loss.item()\n",
    "        critic_loss_value = critic_loss.item()\n",
    "        total_loss_value = total_loss.item()\n",
    "        self.loss_history[\"actor_loss\"].append(actor_loss_value)\n",
    "        self.loss_history[\"critic_loss\"].append(critic_loss_value)\n",
    "        self.loss_history[\"total_loss\"].append(total_loss_value)\n",
    "        self.update_steps_history.append(self.update_step)\n",
    "\n",
    "        # Reset buffers but keep last state for next update\n",
    "        if not done:\n",
    "            # Keep only the last state as starting point for next N-step sequence\n",
    "            last_state = self.states[-1] if self.states else None\n",
    "            last_action = self.actions[-1] if self.actions else None\n",
    "            self.reset_buffers()\n",
    "            if last_state is not None:\n",
    "                # We don't need to re-add the last state since we'll get it fresh\n",
    "                pass\n",
    "        else:\n",
    "            # Episode ended, store episode return and reset completely\n",
    "            self.episode_returns.append(self.episode_return)\n",
    "            \n",
    "            # Track return variance over recent episodes - only store when we have enough data\n",
    "            if len(self.episode_returns) >= self.window_size:\n",
    "                recent_returns = self.episode_returns[-self.window_size:]\n",
    "                return_variance = np.var(recent_returns)\n",
    "                self.return_variance_history.append(return_variance)\n",
    "            # Don't append 0.0 when we don't have enough data - just skip\n",
    "            \n",
    "            self.reset_buffers()\n",
    "\n",
    "        return {\n",
    "            \"actor_loss\": actor_loss_value,\n",
    "            \"critic_loss\": critic_loss_value,\n",
    "            \"total_loss\": total_loss_value,\n",
    "        }, total_grad_norm\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.episode_returns) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"return_mean\": 0.0,\n",
    "                \"return_std\": 0.0,\n",
    "                \"recent_return_variance\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"return_mean\": np.mean(self.episode_returns),\n",
    "            \"return_std\": np.std(self.episode_returns),\n",
    "            \"recent_return_variance\": (\n",
    "                self.return_variance_history[-1]\n",
    "                if self.return_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def get_advantage_normalization_stats(self):\n",
    "        \"\"\"Get fallback normalization statistics.\"\"\"\n",
    "        stats = self.advantage_norm_stats.copy()\n",
    "        \n",
    "        # Calculate fallback rate\n",
    "        if stats[\"total_actor_updates\"] > 0:\n",
    "            fallback_rate = stats[\"fallback_normalizations\"] / stats[\"total_actor_updates\"]\n",
    "            stats[\"fallback_rate_percent\"] = fallback_rate * 100.0\n",
    "        else:\n",
    "            stats[\"fallback_rate_percent\"] = 0.0\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ff80e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor_critic_td(is_continuous, config):\n",
    "    \"\"\"Main training loop for the Actor-Critic TD agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ACTOR-CRITIC TD ({action_type.upper()}) - {config['n_steps']}-Step Updates\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate video recording interval\n",
    "    video_record_interval = max(1, config[\"episodes\"] // config[\"num_videos\"])\n",
    "    print(f\"üìπ Recording {config['num_videos']} videos every {video_record_interval} episodes\")\n",
    "    \n",
    "    # Create algorithm-specific video folder\n",
    "    video_folder = f\"videos/ActorCritic_TD_{action_type.lower()}\"\n",
    "    config_with_videos = config.copy()\n",
    "    config_with_videos[\"video_folder\"] = video_folder\n",
    "    config_with_videos[\"video_record_interval\"] = video_record_interval\n",
    "    \n",
    "    # Create Environment\n",
    "    env = create_env_with_wrappers(\n",
    "        config_with_videos, \n",
    "        is_continuous, \n",
    "        record_videos=True, \n",
    "        video_prefix=f\"ac_td_{action_type.lower()}\",\n",
    "        cleanup_existing=True\n",
    "    )\n",
    "    \n",
    "    # Get observation dimension and space\n",
    "    dummy_obs, _ = env.reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "    \n",
    "    # Create Actor-Critic Network and Agent\n",
    "    print(f\"\\nüèóÔ∏è CREATING {action_type.upper()} ACTOR-CRITIC NETWORK:\")\n",
    "    network = ActorCriticNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=env.action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"network\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nü§ñ INITIALIZING {action_type.upper()} ACTOR-CRITIC TD AGENT:\")\n",
    "    agent = ActorCriticTDAgent(network, config)\n",
    "    \n",
    "    # Training Loop\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=config[\"window_length\"])\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\nüöÄ STARTING {action_type.upper()} TRAINING...\")\n",
    "    \n",
    "    # Use tqdm for progress bar with detailed information\n",
    "    pbar = tqdm(range(1, config[\"episodes\"] + 1), desc=\"Training\", unit=\"episode\")\n",
    "    \n",
    "    for i_episode in pbar:\n",
    "        state, _ = env.reset(seed=config[\"seed\"] + i_episode)\n",
    "        \n",
    "        ep_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            agent.store_reward(reward)\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # Check if we should update\n",
    "            if agent.should_update(done):\n",
    "                loss_dict, grad_norm = agent.update_policy(next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        scores.append(ep_reward)\n",
    "        scores_window.append(ep_reward)\n",
    "        \n",
    "        # Get latest loss values (may be 0.0 if no update occurred this episode)\n",
    "        if agent.loss_history[\"actor_loss\"]:\n",
    "            actor_loss = agent.loss_history[\"actor_loss\"][-1]\n",
    "            critic_loss = agent.loss_history[\"critic_loss\"][-1]\n",
    "            total_loss = agent.loss_history[\"total_loss\"][-1]\n",
    "        else:\n",
    "            actor_loss = critic_loss = total_loss = 0.0\n",
    "        \n",
    "        # Get latest gradient norm\n",
    "        latest_grad_norm = agent.gradient_norms[-1] if agent.gradient_norms else 0.0\n",
    "        \n",
    "        # Get advantage normalization fallback rate\n",
    "        adv_stats = agent.get_advantage_normalization_stats()\n",
    "        failure_rate = adv_stats.get(\"fallback_rate_percent\", 0.0)\n",
    "        \n",
    "        # Update tqdm description with current statistics\n",
    "        avg_score_window = np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "        \n",
    "        pbar.set_description(\n",
    "            f\"Ep {i_episode:4d} | \"\n",
    "            f\"Score: {ep_reward:6.1f} | \"\n",
    "            f\"Avg({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "            f\"Updates: {agent.update_step:4d} | \"\n",
    "            f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "            f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "            f\"GradNorm: {latest_grad_norm:6.4f} | \"\n",
    "            f\"AdvFail: {failure_rate:4.1f}%\"\n",
    "        )\n",
    "        \n",
    "        # Handle video display\n",
    "        if i_episode % video_record_interval == 0 and config[\"record_videos\"]:\n",
    "            from rl_utils.environment import display_latest_video\n",
    "            pbar.write(f\"\\nVideo recorded at episode {i_episode}\")\n",
    "            display_latest_video(\n",
    "                config_with_videos[\"video_folder\"], \n",
    "                f\"ac_td_{action_type.lower()}\", \n",
    "                i_episode\n",
    "            )\n",
    "    \n",
    "    pbar.close()\n",
    "    env.close()\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_window_size = min(config[\"window_length\"], len(scores))\n",
    "    final_performance = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    \n",
    "    # Print advantage normalization statistics\n",
    "    adv_stats = agent.get_advantage_normalization_stats()\n",
    "    print(f\"\\n{action_type} training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Final performance: {final_performance:.2f} (last {final_window_size} episodes)\")\n",
    "    print(f\"Total updates performed: {agent.update_step}\")\n",
    "    print(f\"Average updates per episode: {agent.update_step / config['episodes']:.1f}\")\n",
    "    \n",
    "    \n",
    "    return scores, agent.loss_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ Displaying 1 training videos (episodes: [110]):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;\">\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 110</p>\n",
       "                <video width=\"900\" height=\"675\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/ActorCritic_TD_discrete/ac_td_discrete-episode-110.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "        </div>\n",
       "        <style>\n",
       "            video {\n",
       "                border: 2px solid #ccc;\n",
       "                border-radius: 8px;\n",
       "                box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  111 | Score:  -52.4 | Avg(50): -118.1 | Updates: 4010 | ActorLoss: -0.0482 | CriticLoss: 10603.4268 | GradNorm: 19636.1705 | AdvFail:  0.5%:  11%|‚ñà         | 111/1000 [01:37<19:07,  1.29s/episode]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ 1 training videos available in videos/ActorCritic_TD_discrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep  112 | Score: -189.2 | Avg(50): -119.3 | Updates: 4090 | ActorLoss: -0.0358 | CriticLoss: 4576.7568 | GradNorm: 4068.2669 | AdvFail:  0.5%:  11%|‚ñà         | 112/1000 [01:39<23:14,  1.57s/episode]  /tmp/ipykernel_3490130/3217766692.py:149: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  advantages_std = advantages.std()\n",
      "Ep  126 | Score: -137.7 | Avg(50): -107.8 | Updates: 5336 | ActorLoss: -0.6898 | CriticLoss: 4349.0684 | GradNorm: 9287.8927 | AdvFail:  0.5%:  13%|‚ñà‚ñé        | 126/1000 [02:10<41:39,  2.86s/episode]  "
     ]
    }
   ],
   "source": [
    "# --- DISCRETE ACTION SPACE: TD LEARNING ---\n",
    "print(\"Starting Actor-Critic TD training with DISCRETE actions...\")\n",
    "\n",
    "discrete_td_scores, discrete_td_losses, discrete_td_agent = train_actor_critic_td(\n",
    "    is_continuous=False, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for discrete TD learning\n",
    "plot_training_results(\n",
    "    discrete_td_scores, \n",
    "    discrete_td_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Discrete\", \n",
    "    algorithm_name=f\"Actor-Critic TD ({CONFIG['n_steps']}-step)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_variance_analysis(\n",
    "    discrete_td_agent, \n",
    "    discrete_td_scores, \n",
    "    \"Discrete\", \n",
    "    CONFIG, \n",
    "    algorithm_name=f\"Actor-Critic TD ({CONFIG['n_steps']}-step)\"\n",
    ")\n",
    "\n",
    "# Show advantage normalization statistics\n",
    "adv_stats = discrete_td_agent.get_advantage_normalization_stats()\n",
    "print(f\"\\nüîç DISCRETE TD ADVANTAGE NORMALIZATION ANALYSIS:\")\n",
    "print(f\"Fallback normalization rate: {adv_stats['fallback_rate_percent']:.2f}% ({adv_stats['fallback_normalizations']} of {adv_stats['total_actor_updates']})\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_td_scores))\n",
    "final_avg = np.mean(discrete_td_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE TD TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_td_agent.network.get_param_count():,}\")\n",
    "print(f\"Total updates: {discrete_td_agent.update_step}\")\n",
    "print(f\"Updates per episode: {discrete_td_agent.update_step / CONFIG['episodes']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE: TD LEARNING ---\n",
    "print(\"Starting Actor-Critic TD training with CONTINUOUS actions...\")\n",
    "\n",
    "continuous_td_scores, continuous_td_losses, continuous_td_agent = train_actor_critic_td(\n",
    "    is_continuous=True, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for continuous TD learning\n",
    "plot_training_results(\n",
    "    continuous_td_scores, \n",
    "    continuous_td_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Continuous\", \n",
    "    algorithm_name=f\"Actor-Critic TD ({CONFIG['n_steps']}-step)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_variance_analysis(\n",
    "    continuous_td_agent, \n",
    "    continuous_td_scores, \n",
    "    \"Continuous\", \n",
    "    CONFIG, \n",
    "    algorithm_name=f\"Actor-Critic TD ({CONFIG['n_steps']}-step)\"\n",
    ")\n",
    "\n",
    "# Show advantage normalization statistics\n",
    "adv_stats = continuous_td_agent.get_advantage_normalization_stats()\n",
    "print(f\"\\nüîç CONTINUOUS TD ADVANTAGE NORMALIZATION ANALYSIS:\")\n",
    "print(f\"Fallback normalization rate: {adv_stats['fallback_rate_percent']:.2f}% ({adv_stats['fallback_normalizations']} of {adv_stats['total_actor_updates']})\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_td_scores))\n",
    "final_avg = np.mean(continuous_td_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS TD TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_td_agent.network.get_param_count():,}\")\n",
    "print(f\"Total updates: {continuous_td_agent.update_step}\")\n",
    "print(f\"Updates per episode: {continuous_td_agent.update_step / CONFIG['episodes']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6bc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPREHENSIVE COMPARISON: TD vs MC vs REINFORCE ---\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_utils.visualization import get_moving_average\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE ANALYSIS: Actor-Critic TD vs Previous Methods\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# For comparison with previous notebooks, let's load some theoretical data\n",
    "# In a real scenario, you would load the actual results from previous runs\n",
    "print(f\"\\nüìä TRAINING DYNAMICS COMPARISON:\")\n",
    "print(f\"{'Method':<30} {'Updates/Episode':<15} {'Variance':<10} {'AdvFail%':<8}\")\n",
    "print(\"-\" * 83)\n",
    "\n",
    "# Calculate actual statistics for TD methods\n",
    "discrete_updates_per_ep = discrete_td_agent.update_step / CONFIG[\"episodes\"]\n",
    "continuous_updates_per_ep = continuous_td_agent.update_step / CONFIG[\"episodes\"]\n",
    "\n",
    "discrete_stats = discrete_td_agent.get_variance_stats()\n",
    "continuous_stats = continuous_td_agent.get_variance_stats()\n",
    "\n",
    "discrete_adv_stats = discrete_td_agent.get_advantage_normalization_stats()\n",
    "continuous_adv_stats = continuous_td_agent.get_advantage_normalization_stats()\n",
    "\n",
    "print(f\"{'Actor-Critic TD (Discrete)':<30} {discrete_updates_per_ep:<15.1f} {discrete_stats['return_std']:<10.1f} {discrete_adv_stats['fallback_rate_percent']:<8.1f}\")\n",
    "print(f\"{'Actor-Critic TD (Continuous)':<30} {continuous_updates_per_ep:<15.1f} {continuous_stats['return_std']:<10.1f} {continuous_adv_stats['fallback_rate_percent']:<8.1f}\")\n",
    "\n",
    "# Comparison plot between discrete and continuous TD\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle(f'Actor-Critic TD ({CONFIG[\"n_steps\"]}-Step): Discrete vs Continuous Analysis', fontsize=16)\n",
    "\n",
    "colors = ['blue', 'red']\n",
    "smoothing_window = CONFIG[\"window_length\"]\n",
    "\n",
    "# 1. Score comparison\n",
    "experiments = [\n",
    "    (\"Discrete TD\", discrete_td_scores, discrete_td_agent),\n",
    "    (\"Continuous TD\", continuous_td_scores, continuous_td_agent),\n",
    "]\n",
    "\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(scores) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(scores, window=smoothing_window)\n",
    "        episodes = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax1.plot(episodes, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax1.axhline(y=CONFIG[\"target_score\"], color='g', linestyle='--', label=f'Target ({CONFIG[\"target_score\"]})', alpha=0.7)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel(f'Score ({smoothing_window}-episode avg)')\n",
    "ax1.set_title('Performance Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Update frequency comparison\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.gradient_norms) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.gradient_norms, window=smoothing_window)\n",
    "        update_steps = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax2.plot(update_steps, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Update Step')\n",
    "ax2.set_ylabel(f'Gradient Norm ({smoothing_window}-step avg)')\n",
    "ax2.set_title('Gradient Stability Over Updates')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss components comparison\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.loss_history[\"total_loss\"]) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.loss_history[\"total_loss\"], window=smoothing_window)\n",
    "        update_steps = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax3.plot(update_steps, smoothed, label=f'{name} Total', color=colors[i], linewidth=2)\n",
    "        \n",
    "        # Also plot actor and critic losses with transparency\n",
    "        if len(agent.loss_history[\"actor_loss\"]) >= smoothing_window:\n",
    "            actor_smoothed, _ = get_moving_average(agent.loss_history[\"actor_loss\"], window=smoothing_window)\n",
    "            ax3.plot(update_steps, actor_smoothed, label=f'{name} Actor', color=colors[i], alpha=0.6, linestyle='--')\n",
    "        \n",
    "        if len(agent.loss_history[\"critic_loss\"]) >= smoothing_window:\n",
    "            critic_smoothed, _ = get_moving_average(agent.loss_history[\"critic_loss\"], window=smoothing_window)\n",
    "            ax3.plot(update_steps, critic_smoothed, label=f'{name} Critic', color=colors[i], alpha=0.6, linestyle=':')\n",
    "\n",
    "ax3.set_xlabel('Update Step')\n",
    "ax3.set_ylabel(f'Loss ({smoothing_window}-step avg)')\n",
    "ax3.set_title('Loss Components Over Updates')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Advantage normalization failure rate over time\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    # Calculate failure rate in sliding windows\n",
    "    window_size = max(10, agent.update_step // 50)  # Adaptive window size\n",
    "    failure_rates = []\n",
    "    update_points = []\n",
    "    \n",
    "    for update_idx in range(window_size, agent.update_step + 1, window_size):\n",
    "        start_idx = max(0, update_idx - window_size)\n",
    "        \n",
    "        # Count failures in this window - we'll approximate since we only have total counts\n",
    "        # For a more accurate implementation, we'd need to track failures per update\n",
    "        adv_stats = agent.get_advantage_normalization_stats()\n",
    "        failure_rate = adv_stats.get(\"fallback_rate_percent\", 0.0)\n",
    "        failure_rates.append(failure_rate)\n",
    "        update_points.append(update_idx)\n",
    "    \n",
    "    if update_points:\n",
    "        ax4.plot(update_points, failure_rates, label=f'{name}', color=colors[i], linewidth=2, marker='o', markersize=4)\n",
    "\n",
    "ax4.set_xlabel('Update Step')\n",
    "ax4.set_ylabel('Advantage Normalization Failure Rate (%)')\n",
    "ax4.set_title('Advantage Normalization Stability')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
