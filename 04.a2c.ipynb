{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7aeaa54",
   "metadata": {},
   "source": [
    "# A2C (Advantage Actor-Critic): Parallel Environments for Stable Learning\n",
    "\n",
    "## üéØ From Actor-Critic TD to A2C: Adding Parallelism\n",
    "\n",
    "Welcome to **Advantage Actor-Critic (A2C)** - the practical solution that takes our Actor-Critic TD method and adds parallel environment collection for significantly more stable learning.\n",
    "\n",
    "**The Core Innovation**: Instead of collecting N-step experience from a single environment, we collect N-step experience from **multiple parallel environments** simultaneously, then perform synchronous batch updates.\n",
    "\n",
    "## üîÑ A3C ‚Üí A2C: From Complexity to Simplicity\n",
    "\n",
    "### The A3C Era (2016): Asynchronous Complexity\n",
    "\n",
    "**A3C (Asynchronous Advantage Actor-Critic)** was a breakthrough algorithm that introduced parallel environment collection:\n",
    "- **Multiple worker threads** collecting experience independently\n",
    "- **Asynchronous updates** to shared network parameters\n",
    "- **No replay buffer needed** - fresh on-policy data from parallel workers\n",
    "- **Massive scalability** - could use dozens of CPU cores effectively\n",
    "\n",
    "**A3C's Problems**:\n",
    "- **Complex implementation**: Asynchronous updates, thread synchronization, shared memory\n",
    "- **Difficult debugging**: Race conditions and non-deterministic behavior\n",
    "- **Hyperparameter sensitivity**: Harder to tune due to asynchronous nature\n",
    "- **Limited GPU utilization**: Asynchronous updates don't batch well for GPU training\n",
    "\n",
    "### A2C (2017): Synchronous Simplification\n",
    "\n",
    "**A2C** keeps A3C's core insight (parallel data collection) but simplifies the execution:\n",
    "- **Synchronous updates**: All workers step in lockstep, then batch update\n",
    "- **Simpler implementation**: No thread synchronization complexity\n",
    "- **Deterministic training**: Reproducible results with proper seeding\n",
    "- **GPU-friendly**: Natural batching for efficient neural network training\n",
    "- **Easier debugging**: Predictable execution flow\n",
    "\n",
    "**Why A2C Won**:\n",
    "1. **Same performance**: A2C achieves similar results to A3C with much simpler code\n",
    "2. **Better engineering**: Easier to implement, debug, and extend\n",
    "3. **Industry adoption**: Became the standard for parallel on-policy RL\n",
    "4. **GPU efficiency**: Better utilization of modern ML hardware\n",
    "\n",
    "## üìä A2C vs Actor-Critic TD: The Key Differences\n",
    "\n",
    "| Aspect | Actor-Critic TD | A2C |\n",
    "|--------|----------------|-----|\n",
    "| **Environments** | Single | Multiple parallel |\n",
    "| **Data Collection** | Sequential N-steps | Parallel N-steps from all envs |\n",
    "| **Batch Size** | N transitions | N √ó num_envs transitions |\n",
    "| **Update Frequency** | Every N steps | Every N steps (all envs) |\n",
    "| **Gradient Variance** | High | Lower (averaged across envs) |\n",
    "| **Implementation** | Simple | Moderate (vectorized envs) |\n",
    "| **Sample Efficiency** | Standard | Same per env, but faster wall-clock |\n",
    "| **Stability** | Moderate | Higher (batch averaging) |\n",
    "\n",
    "## üöÄ A2C Algorithm Overview\n",
    "\n",
    "**Algorithm: Advantage Actor-Critic (A2C)**\n",
    "\n",
    "---\n",
    "**Input:** \n",
    "- Unified Actor-Critic network with parameters $\\theta$\n",
    "- Number of parallel environments $E$\n",
    "- N-step parameter $N$ \n",
    "- Learning rate $\\alpha$\n",
    "- Discount factor $\\gamma$\n",
    "- Maximum gradient norm $\\text{max\\_grad\\_norm}$\n",
    "- Number of episodes $M$\n",
    "- $c_V$: Critic loss coefficient (typically 0.5)\n",
    "\n",
    "**Output:** \n",
    "- Trained unified network parameters $\\theta$\n",
    "\n",
    "---\n",
    "**Procedure:**\n",
    "1. **Initialize** network parameters $\\theta$ randomly\n",
    "2. **Initialize** $E$ parallel environments\n",
    "3. **For** iteration $i = 1, 2, ..., M$ **do:**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;**For** $t = 1, 2, ..., N$ **do:** *(collect N-step experience)*\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**For each environment** $e = 1, 2, ..., E$ **do:**\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Sample action**: $a_t^{(e)} \\sim \\pi_\\theta(\\cdot|s_t^{(e)})$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Execute**: $s_{t+1}^{(e)}, r_{t+1}^{(e)} \\leftarrow \\text{env}_e.\\text{step}(a_t^{(e)})$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Store**: $(s_t^{(e)}, a_t^{(e)}, r_{t+1}^{(e)}, \\log \\pi_\\theta(a_t^{(e)}|s_t^{(e)}), V_\\theta(s_t^{(e)}))$\n",
    "9. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "10. &nbsp;&nbsp;&nbsp;&nbsp;**End For**\n",
    "11. &nbsp;&nbsp;&nbsp;&nbsp;**Compute N-step advantages** for all environments using N-step TD\n",
    "12. &nbsp;&nbsp;&nbsp;&nbsp;**Batch all transitions**: $B = \\{(s,a,A,G)_{t,e} : t \\in [1,N], e \\in [1,E]\\}$\n",
    "13. &nbsp;&nbsp;&nbsp;&nbsp;**Actor loss**: $L^\\pi(\\theta) = -\\mathbb{E}_{(s,a,A,G) \\in B}[\\log \\pi_\\theta(a|s) \\cdot A]$\n",
    "14. &nbsp;&nbsp;&nbsp;&nbsp;**Critic loss**: $L^V(\\theta) = \\mathbb{E}_{(s,a,A,G) \\in B}[(V_\\theta(s) - G)^2]$\n",
    "15. &nbsp;&nbsp;&nbsp;&nbsp;**Total loss**: $L(\\theta) = L^\\pi(\\theta) + c_v L^V(\\theta)$\n",
    "16. &nbsp;&nbsp;&nbsp;&nbsp;**Update parameters**: $\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L(\\theta)$\n",
    "17. **End For**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Key Benefits of A2C\n",
    "\n",
    "### ‚úÖ Significant Advantages over Single-Environment Methods\n",
    "\n",
    "1. **Reduced Gradient Variance**: Averaging gradients across multiple environments dramatically reduces variance\n",
    "2. **Faster Wall-Clock Training**: Parallel data collection speeds up training significantly  \n",
    "3. **More Stable Learning**: Batch updates from diverse environments prevent overfitting to single environment quirks\n",
    "4. **Better Sample Diversity**: Each environment may be in different states, providing richer training signal\n",
    "5. **Natural Regularization**: Averaging across environments acts as implicit regularization\n",
    "\n",
    "### üîÑ Maintained Benefits\n",
    "\n",
    "1. **N-Step Learning**: Still uses efficient N-step bootstrapping from Actor-Critic TD\n",
    "2. **On-Policy**: Uses only fresh data from current policy\n",
    "3. **No Replay Buffer**: Simpler than off-policy methods, no memory management complexity\n",
    "4. **Action Space Flexibility**: Works with both discrete and continuous actions\n",
    "\n",
    "### ‚öñÔ∏è Trade-offs\n",
    "\n",
    "1. **Implementation Complexity**: Requires vectorized environment handling\n",
    "2. **Memory Usage**: Multiple environments increase memory requirements\n",
    "3. **Hyperparameter Sensitivity**: Number of environments becomes additional hyperparameter\n",
    "4. **Debugging Complexity**: Harder to debug issues across multiple environments\n",
    "\n",
    "## üõ†Ô∏è Implementation Notes\n",
    "\n",
    "For this implementation, we maintain the same hyperparameters as Actor-Critic TD (n_steps=5, same learning rates, etc.) to ensure fair comparison, with only the parallel environment collection being different.\n",
    "\n",
    "We use Gymnasium's `SyncVectorEnv` for deterministic, reproducible parallel environment execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d0aae1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé≤ Global random seeds set to 42 for reproducible results\n",
      "üìù Environment episodes will use seeds 42 + episode_number for varied but reproducible episodes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Import our RL utilities including the ActorCriticNetwork\n",
    "from rl_utils import (\n",
    "    set_seeds,\n",
    "    ActorCriticNetwork,\n",
    "    create_env_with_wrappers,\n",
    "    plot_training_results,\n",
    "    plot_variance_analysis,\n",
    "    make_vec_envs\n",
    ")\n",
    "\n",
    "# Create configuration\n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"episodes\": 1000,\n",
    "    \"gamma\": 0.99,\n",
    "    \"lr\": 5e-4,\n",
    "    \"device\": \"cuda\",\n",
    "    \"window_length\": 50,\n",
    "    \"target_score\": 200,  # LunarLander-v3 target score\n",
    "    # Environment: LunarLander-v3 only\n",
    "    \"env_id\": \"LunarLander-v3\",\n",
    "    \"env_kwargs\": {\n",
    "        \"gravity\": -10.0,\n",
    "        \"enable_wind\": False,\n",
    "        \"wind_power\": 15.0,\n",
    "        \"turbulence_power\": 1.5,\n",
    "    },\n",
    "    # Video Recording Config\n",
    "    \"record_videos\": True,\n",
    "    \"video_folder\": \"videos\",\n",
    "    \"num_videos\": 9,  # Number of videos to record during training\n",
    "    \"record_test_videos\": True,\n",
    "    # Neural Network Config\n",
    "    \"network\": {\n",
    "        \"fc_out_features\": [64, 64],  # Shared features\n",
    "        \"actor_features\": [32],  # Actor-specific layers after shared\n",
    "        \"critic_features\": [32],  # Critic-specific layers after shared\n",
    "        \"activation\": \"SiLU\",\n",
    "        \"use_layer_norm\": True,\n",
    "        \"dropout_rate\": 0.0,\n",
    "    },\n",
    "    # Actor-Critic Specific Parameters\n",
    "    \"critic_loss_coeff\": 0.5,  # Weight for critic loss in total loss\n",
    "    # A2C-Specific Parameters\n",
    "    \"num_envs\": 4,  # Number of parallel environments\n",
    "    \"n_steps\": 5,  # N-step TD parameter (same as Actor-Critic TD)\n",
    "    \"max_grad_norm\": 0.5,  # Maximum gradient norm for clipping\n",
    "}\n",
    "\n",
    "set_seeds(CONFIG[\"seed\"])\n",
    "print(f\"üé≤ Global random seeds set to {CONFIG['seed']} for reproducible results\")\n",
    "print(\n",
    "    f\"üìù Environment episodes will use seeds {CONFIG['seed']} + episode_number for varied but reproducible episodes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69e37ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \"\"\"A2C agent with parallel environment collection.\"\"\"\n",
    "\n",
    "    def __init__(self, network, config):\n",
    "        \"\"\"\n",
    "        Initialize A2C agent.\n",
    "\n",
    "        Args:\n",
    "            network: ActorCriticNetwork instance\n",
    "            config: Configuration dictionary\n",
    "        \"\"\"\n",
    "        self.network = network.to(config[\"device\"])\n",
    "        self.device = config[\"device\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.n_steps = config[\"n_steps\"]\n",
    "        self.num_envs = config[\"num_envs\"]\n",
    "        self.max_grad_norm = config[\"max_grad_norm\"]\n",
    "        self.window_size = config.get(\"window_length\")\n",
    "\n",
    "        # Actor-Critic specific parameters\n",
    "        self.critic_loss_coeff = config.get(\"critic_loss_coeff\")\n",
    "\n",
    "        # Single optimizer for all network parameters\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.network.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "        )\n",
    "\n",
    "        # Print detailed network information\n",
    "        print(f\"üìä A2C NETWORK DETAILS:\")\n",
    "        self.network.print_network_info()\n",
    "        print(f\"üåê Number of Parallel Environments: {self.num_envs}\")\n",
    "        print(f\"üéØ N-Steps: {self.n_steps}\")\n",
    "        print(f\"üéì Learning Rate: {config['lr']} (shared for actor & critic)\")\n",
    "        print(f\"‚öñÔ∏è Critic Loss Coefficient: {self.critic_loss_coeff}\")\n",
    "        print(f\"‚úÇÔ∏è Max Gradient Norm: {self.max_grad_norm}\")\n",
    "\n",
    "        # N-step buffer storage for all environments\n",
    "        self.reset_buffers()\n",
    "\n",
    "        # Variance and performance tracking - MODIFIED for vectorized environments\n",
    "        self.gradient_norms = []\n",
    "        self.vectorized_episode_scores = []  # Averaged episode scores across parallel environments\n",
    "        self.score_variance_history = []\n",
    "\n",
    "        # Update step tracking\n",
    "        self.update_step = 0\n",
    "        self.update_steps_history = []\n",
    "\n",
    "        # Loss component tracking\n",
    "        self.loss_history = {\n",
    "            \"actor_loss\": [],\n",
    "            \"critic_loss\": [],\n",
    "            \"total_loss\": [],\n",
    "        }\n",
    "\n",
    "        # Advantage normalization fallback tracking\n",
    "        self.advantage_norm_stats = {\n",
    "            \"total_actor_updates\": 0,\n",
    "            \"fallback_normalizations\": 0,\n",
    "        }\n",
    "\n",
    "        # Episode tracking for all environments - MODIFIED\n",
    "        self.total_episodes = 0  # Total individual episodes completed across all environments\n",
    "        self.vectorized_episodes = 0  # Number of vectorized episodes (averaged groups)\n",
    "        self.env_episode_returns = [0.0] * self.num_envs\n",
    "        self.completed_episode_scores = []  # Store all individual episode scores for averaging\n",
    "\n",
    "    def reset_buffers(self):\n",
    "        \"\"\"Reset N-step buffers for all environments.\"\"\"\n",
    "        self.states = [[] for _ in range(self.num_envs)]\n",
    "        self.actions = [[] for _ in range(self.num_envs)]\n",
    "        self.rewards = [[] for _ in range(self.num_envs)]\n",
    "        self.log_probs = [[] for _ in range(self.num_envs)]\n",
    "        self.values = [[] for _ in range(self.num_envs)]\n",
    "\n",
    "    def select_actions(self, states):\n",
    "        \"\"\"Select actions for all environments.\"\"\"\n",
    "        states = torch.as_tensor(states, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Get policy distribution and value estimates\n",
    "        dist, values = self.network(states)\n",
    "        actions = dist.sample()\n",
    "\n",
    "        # Store log probabilities and value predictions\n",
    "        if self.network.is_continuous:\n",
    "            log_probs = dist.log_prob(actions).sum(-1)\n",
    "            actions_to_env = [self.network.clip_action(actions[i]).flatten() for i in range(self.num_envs)]\n",
    "        else:\n",
    "            log_probs = dist.log_prob(actions)\n",
    "            actions_to_env = actions.cpu().numpy()\n",
    "\n",
    "        # Store in buffers for each environment\n",
    "        for i in range(self.num_envs):\n",
    "            self.states[i].append(states[i])\n",
    "            self.actions[i].append(actions[i])\n",
    "            self.log_probs[i].append(log_probs[i])\n",
    "            self.values[i].append(values[i])\n",
    "\n",
    "        return actions_to_env\n",
    "\n",
    "    def store_rewards(self, rewards, dones):\n",
    "        \"\"\"Store rewards and handle episode completion.\"\"\"\n",
    "        for i in range(self.num_envs):\n",
    "            self.rewards[i].append(rewards[i])\n",
    "            self.env_episode_returns[i] += rewards[i]\n",
    "            \n",
    "            # If episode ended, store episode score\n",
    "            if dones[i]:\n",
    "                self.completed_episode_scores.append(self.env_episode_returns[i])\n",
    "                self.env_episode_returns[i] = 0.0\n",
    "                self.total_episodes += 1\n",
    "\n",
    "    def should_update(self, step_count, dones):\n",
    "        \"\"\"Check if we should perform an update.\"\"\"\n",
    "        # Update every N steps OR when any environment terminates\n",
    "        return step_count >= self.n_steps or any(dones) if dones is not None else False\n",
    "\n",
    "    def update_policy(self, next_states=None, dones=None):\n",
    "        \"\"\"Update both actor and critic using N-step TD targets from all environments.\"\"\"\n",
    "        # Check if we have data to update\n",
    "        if not any(len(env_rewards) > 0 for env_rewards in self.rewards):\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        self.update_step += 1\n",
    "        self.advantage_norm_stats[\"total_actor_updates\"] += 1\n",
    "\n",
    "        # Collect all transitions from all environments\n",
    "        all_td_targets = []\n",
    "        all_values = []\n",
    "        all_log_probs = []\n",
    "\n",
    "        for env_idx in range(self.num_envs):\n",
    "            if len(self.rewards[env_idx]) == 0:\n",
    "                continue\n",
    "\n",
    "            # Calculate N-step TD targets for this environment\n",
    "            td_targets = []\n",
    "            \n",
    "            # Get bootstrap value for the last state\n",
    "            if dones is not None and dones[env_idx]:\n",
    "                bootstrap_value = 0.0  # Terminal state has value 0\n",
    "            elif next_states is not None:\n",
    "                with torch.no_grad():\n",
    "                    next_state_tensor = torch.as_tensor(next_states[env_idx], dtype=torch.float32, device=self.device)\n",
    "                    _, bootstrap_value = self.network(next_state_tensor)\n",
    "                    bootstrap_value = bootstrap_value.item()\n",
    "            else:\n",
    "                bootstrap_value = 0.0\n",
    "\n",
    "            # Calculate N-step returns for this environment\n",
    "            for i in range(len(self.rewards[env_idx])):\n",
    "                td_target = 0.0\n",
    "                discount = 1.0\n",
    "                \n",
    "                # Sum discounted rewards for available steps\n",
    "                for j in range(i, len(self.rewards[env_idx])):\n",
    "                    td_target += discount * self.rewards[env_idx][j]\n",
    "                    discount *= self.gamma\n",
    "                \n",
    "                # Add bootstrapped value if not terminal\n",
    "                if not (dones is not None and dones[env_idx] and i == len(self.rewards[env_idx]) - 1):\n",
    "                    td_target += discount * bootstrap_value\n",
    "                \n",
    "                td_targets.append(td_target)\n",
    "\n",
    "            # Collect data from this environment\n",
    "            if len(td_targets) > 0:\n",
    "                all_td_targets.extend(td_targets)\n",
    "                all_values.extend(self.values[env_idx])\n",
    "                all_log_probs.extend(self.log_probs[env_idx])\n",
    "\n",
    "        if not all_td_targets:\n",
    "            self.reset_buffers()\n",
    "            return {\"actor_loss\": 0.0, \"critic_loss\": 0.0, \"total_loss\": 0.0}, 0.0\n",
    "\n",
    "        # Convert to tensors\n",
    "        td_targets = torch.tensor(all_td_targets, dtype=torch.float32, device=self.device)\n",
    "        values_tensor = torch.stack(all_values)\n",
    "        log_probs_tensor = torch.stack(all_log_probs)\n",
    "\n",
    "        # Calculate advantages (no normalization of TD targets for critic!)\n",
    "        advantages = td_targets - values_tensor.detach()\n",
    "\n",
    "        # NOTE: Score variance tracking is now handled in finalize_vectorized_episode_data()\n",
    "        # to ensure it's properly aligned with vectorized episodes\n",
    "\n",
    "        # Robust advantage normalization: only check for NaN after normalization\n",
    "        advantages_mean = advantages.mean()\n",
    "        advantages_std = advantages.std()\n",
    "\n",
    "        # Normalize advantages, fallback to centering if NaN occurs\n",
    "        advantages_normalized = (advantages - advantages_mean) / (advantages_std + 1e-8)\n",
    "        if torch.isnan(advantages_normalized).any():\n",
    "            self.advantage_norm_stats[\"fallback_normalizations\"] += 1\n",
    "            advantages_normalized = advantages - advantages_mean\n",
    "\n",
    "        actor_loss = -(log_probs_tensor * advantages_normalized).mean()\n",
    "\n",
    "        # Critic loss (no normalization of targets!)\n",
    "        critic_loss = torch.nn.functional.mse_loss(values_tensor, td_targets)\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = actor_loss + self.critic_loss_coeff * critic_loss\n",
    "\n",
    "        # Update network with gradient clipping\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Record gradient norm BEFORE clipping\n",
    "        total_grad_norm = 0.0\n",
    "        for param in self.network.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_grad_norm += param_norm.item() ** 2\n",
    "        total_grad_norm = total_grad_norm**0.5\n",
    "        self.gradient_norms.append(total_grad_norm)\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Log losses\n",
    "        actor_loss_value = actor_loss.item()\n",
    "        critic_loss_value = critic_loss.item()\n",
    "        total_loss_value = total_loss.item()\n",
    "        self.loss_history[\"actor_loss\"].append(actor_loss_value)\n",
    "        self.loss_history[\"critic_loss\"].append(critic_loss_value)\n",
    "        self.loss_history[\"total_loss\"].append(total_loss_value)\n",
    "        self.update_steps_history.append(self.update_step)\n",
    "\n",
    "        # Reset buffers for next collection\n",
    "        self.reset_buffers()\n",
    "\n",
    "        return {\n",
    "            \"actor_loss\": actor_loss_value,\n",
    "            \"critic_loss\": critic_loss_value,\n",
    "            \"total_loss\": total_loss_value,\n",
    "        }, total_grad_norm\n",
    "\n",
    "    def finalize_vectorized_episode_data(self):\n",
    "        \"\"\"\n",
    "        Aggregate and finalize vectorized episode data for plotting.\n",
    "        Called when we have enough completed episodes for averaging.\n",
    "        \"\"\"\n",
    "        if len(self.completed_episode_scores) >= self.num_envs:\n",
    "            # Take the last num_envs completed episodes and average them\n",
    "            recent_episodes = self.completed_episode_scores[-self.num_envs:]\n",
    "            averaged_score = np.mean(recent_episodes)\n",
    "            self.vectorized_episode_scores.append(averaged_score)\n",
    "            self.vectorized_episodes += 1\n",
    "            \n",
    "            # Remove the episodes we just processed\n",
    "            self.completed_episode_scores = self.completed_episode_scores[:-self.num_envs]\n",
    "            \n",
    "            # Track score variance ONLY when we have enough vectorized episodes\n",
    "            if len(self.vectorized_episode_scores) >= self.window_size:\n",
    "                recent_scores = self.vectorized_episode_scores[-self.window_size:]\n",
    "                score_variance = np.var(recent_scores)\n",
    "                self.score_variance_history.append(score_variance)\n",
    "            else:\n",
    "                # For the first few vectorized episodes, we don't have enough data for variance\n",
    "                # but we still want to track something for consistency\n",
    "                self.score_variance_history.append(0.0)\n",
    "\n",
    "    def get_variance_stats(self):\n",
    "        \"\"\"Get variance statistics for analysis.\"\"\"\n",
    "        if len(self.vectorized_episode_scores) < 2:\n",
    "            return {\n",
    "                \"gradient_norm_mean\": 0.0,\n",
    "                \"gradient_norm_std\": 0.0,\n",
    "                \"score_mean\": 0.0,\n",
    "                \"score_std\": 0.0,\n",
    "                \"recent_score_variance\": 0.0,\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"gradient_norm_mean\": np.mean(self.gradient_norms),\n",
    "            \"gradient_norm_std\": np.std(self.gradient_norms),\n",
    "            \"score_mean\": np.mean(self.vectorized_episode_scores),\n",
    "            \"score_std\": np.std(self.vectorized_episode_scores),\n",
    "            \"recent_score_variance\": (\n",
    "                self.score_variance_history[-1]\n",
    "                if self.score_variance_history\n",
    "                else 0.0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def get_advantage_normalization_stats(self):\n",
    "        \"\"\"Get fallback normalization statistics.\"\"\"\n",
    "        stats = self.advantage_norm_stats.copy()\n",
    "        \n",
    "        # Calculate fallback rate\n",
    "        if stats[\"total_actor_updates\"] > 0:\n",
    "            fallback_rate = stats[\"fallback_normalizations\"] / stats[\"total_actor_updates\"]\n",
    "            stats[\"fallback_rate_percent\"] = fallback_rate * 100.0\n",
    "        else:\n",
    "            stats[\"fallback_rate_percent\"] = 0.0\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f7f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_a2c(is_continuous, config):\n",
    "    \"\"\"Main training loop for the A2C agent.\"\"\"\n",
    "    action_type = \"Continuous\" if is_continuous else \"Discrete\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\n",
    "        f\"A2C ({action_type.upper()}) - Parallel {config['num_envs']}-Environment Training\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Target vectorized episodes: {config['episodes']} (averaged across {config['num_envs']} parallel envs)\"\n",
    "    )\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    # Calculate video recording interval BEFORE creating environments\n",
    "    video_record_interval = max(1, config['episodes'] // config['num_videos'])\n",
    "    print(\n",
    "        f\"üìπ Recording {config['num_videos']} videos every {video_record_interval} vectorized episodes\"\n",
    "    )\n",
    "\n",
    "    # Update config with video_record_interval\n",
    "    config_for_envs = config.copy()\n",
    "    config_for_envs[\"video_record_interval\"] = video_record_interval\n",
    "\n",
    "    # Create Vectorized Environment\n",
    "    vec_env, config_with_videos = make_vec_envs(\n",
    "        config_for_envs,\n",
    "        is_continuous,\n",
    "        record_videos=True,\n",
    "        video_prefix=f\"a2c_{action_type.lower()}\",\n",
    "        cleanup_existing=True,\n",
    "    )\n",
    "\n",
    "    # Get observation dimension from a single environment\n",
    "    dummy_obs, _ = vec_env.envs[0].reset()\n",
    "    observation_dim = len(dummy_obs)\n",
    "\n",
    "    # Create Actor-Critic Network and Agent\n",
    "    print(f\"\\nüèóÔ∏è CREATING {action_type.upper()} ACTOR-CRITIC NETWORK:\")\n",
    "    network = ActorCriticNetwork(\n",
    "        observation_dim=observation_dim,\n",
    "        action_space=vec_env.single_action_space,\n",
    "        is_continuous=is_continuous,\n",
    "        network_config=config[\"network\"],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nü§ñ INITIALIZING {action_type.upper()} A2C AGENT:\")\n",
    "    agent = A2CAgent(network, config)\n",
    "\n",
    "    # Training Loop\n",
    "    scores = []  # These will be averaged scores across parallel environments\n",
    "    scores_window = deque(maxlen=config[\"window_length\"])\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\nüöÄ STARTING {action_type.upper()} TRAINING...\")\n",
    "\n",
    "    # Initialize parallel environments\n",
    "    states, _ = vec_env.reset(seed=config[\"seed\"])\n",
    "    step_count = 0\n",
    "    vectorized_episode_count = 0  # Track vectorized episodes (averaged across parallel environments)\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    pbar = tqdm(total=config[\"episodes\"], desc=\"Training\", unit=\"vec-episode\")\n",
    "\n",
    "    # Continue until we reach target vectorized episodes\n",
    "    while vectorized_episode_count < config[\"episodes\"]:\n",
    "        # Collect N-step experience\n",
    "        while step_count < config[\"n_steps\"] and vectorized_episode_count < config[\"episodes\"]:\n",
    "            actions = agent.select_actions(states)\n",
    "\n",
    "            next_states, rewards, terminateds, truncateds, infos = vec_env.step(actions)\n",
    "            dones = np.logical_or(terminateds, truncateds)\n",
    "\n",
    "            agent.store_rewards(rewards, dones)\n",
    "\n",
    "            # Handle episode completion\n",
    "            episodes_completed_this_step = sum(dones)\n",
    "            if episodes_completed_this_step > 0:\n",
    "                # Check if we can create an averaged vectorized episode\n",
    "                agent.finalize_vectorized_episode_data()\n",
    "                \n",
    "                # Update vectorized episode count based on completed averaged episodes\n",
    "                if len(agent.vectorized_episode_scores) > vectorized_episode_count:\n",
    "                    new_vectorized_episodes = len(agent.vectorized_episode_scores) - vectorized_episode_count\n",
    "                    vectorized_episode_count += new_vectorized_episodes\n",
    "                    \n",
    "                    # Update scores tracking\n",
    "                    for i in range(new_vectorized_episodes):\n",
    "                        episode_idx = vectorized_episode_count - new_vectorized_episodes + i\n",
    "                        if episode_idx < len(agent.vectorized_episode_scores):\n",
    "                            score = agent.vectorized_episode_scores[episode_idx]\n",
    "                            scores.append(score)\n",
    "                            scores_window.append(score)\n",
    "\n",
    "                    # Handle video display for vectorized episode intervals\n",
    "                    if vectorized_episode_count % video_record_interval == 0 and config[\"record_videos\"]:\n",
    "                        from rl_utils.environment import display_latest_video\n",
    "                        pbar.write(f\"\\nVideo recorded at vectorized episode {vectorized_episode_count}\")\n",
    "                        display_latest_video(\n",
    "                            config_with_videos[\"video_folder\"],\n",
    "                            f\"a2c_{action_type.lower()}\",\n",
    "                            vectorized_episode_count,\n",
    "                        )\n",
    "\n",
    "            states = next_states\n",
    "            step_count += 1\n",
    "\n",
    "        # Update policy when we have enough steps OR any environment terminated\n",
    "        if agent.should_update(step_count, dones) and vectorized_episode_count < config[\"episodes\"]:\n",
    "            loss_dict, grad_norm = agent.update_policy(next_states, dones)\n",
    "            step_count = 0  # Reset step count after update\n",
    "\n",
    "            # Update progress bar\n",
    "            if scores:\n",
    "                avg_score_window = (\n",
    "                    np.mean(scores_window) if len(scores_window) > 0 else 0.0\n",
    "                )\n",
    "                actor_loss = loss_dict.get(\"actor_loss\", 0.0)\n",
    "                critic_loss = loss_dict.get(\"critic_loss\", 0.0)\n",
    "                total_loss = loss_dict.get(\"total_loss\", 0.0)\n",
    "\n",
    "                # Get advantage normalization fallback rate\n",
    "                adv_stats = agent.get_advantage_normalization_stats()\n",
    "                failure_rate = adv_stats.get(\"fallback_rate_percent\", 0.0)\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f\"VecEp {vectorized_episode_count:4d} | \"\n",
    "                    f\"Score: {scores[-1] if scores else 0.0:6.1f} | \"\n",
    "                    f\"AvgScore({config['window_length']}): {avg_score_window:6.1f} | \"\n",
    "                    f\"Updates: {agent.update_step:4d} | \"\n",
    "                    f\"ActorLoss: {actor_loss:7.4f} | \"\n",
    "                    f\"CriticLoss: {critic_loss:7.4f} | \"\n",
    "                    f\"GradNorm: {grad_norm:6.4f} | \"\n",
    "                    f\"AdvFail: {failure_rate:4.1f}%\"\n",
    "                )\n",
    "                pbar.n = min(vectorized_episode_count, config[\"episodes\"])\n",
    "                pbar.refresh()\n",
    "\n",
    "    pbar.close()\n",
    "    vec_env.close()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    final_window_size = min(config[\"window_length\"], len(scores))\n",
    "    final_performance = (\n",
    "        np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    adv_stats = agent.get_advantage_normalization_stats()\n",
    "    print(f\"\\n{action_type} training completed in {elapsed_time:.1f} seconds!\")\n",
    "    print(f\"Vectorized episodes completed: {vectorized_episode_count} (averaged across {config['num_envs']} parallel envs)\")\n",
    "    print(f\"Total individual episodes: {agent.total_episodes}\")\n",
    "    print(\n",
    "        f\"Final performance: {final_performance:.2f} (last {final_window_size} vectorized episodes)\"\n",
    "    )\n",
    "    print(f\"Total updates performed: {agent.update_step}\")\n",
    "    print(f\"Updates per vectorized episode: {agent.update_step / max(1, vectorized_episode_count):.1f}\")\n",
    "    print(f\"Updates per individual episode: {agent.update_step / max(1, agent.total_episodes):.1f}\")\n",
    "\n",
    "    return scores, agent.loss_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f6b314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìπ Displaying 2 training videos (episodes: [110, 221]):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"display: flex; flex-wrap: wrap; gap: 10px; justify-content: center;\">\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 110</p>\n",
       "                <video width=\"450\" height=\"337\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/a2c_discrete_discrete/a2c_discrete-episode-110.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p style=\"margin: 5px 0; font-weight: bold;\">Episode 221</p>\n",
       "                <video width=\"450\" height=\"337\" controls loop autoplay muted>\n",
       "                    <source src=\"videos/a2c_discrete_discrete/a2c_discrete-episode-221.mp4\" type=\"video/mp4\">\n",
       "                    Your browser does not support the video tag.\n",
       "                </video>\n",
       "            </div>\n",
       "            \n",
       "        </div>\n",
       "        <style>\n",
       "            video {\n",
       "                border: 2px solid #ccc;\n",
       "                border-radius: 8px;\n",
       "                box-shadow: 0 2px 8px rgba(0,0,0,0.1);\n",
       "            }\n",
       "        </style>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VecEp  333 | Score:  263.4 | AvgScore(50):  173.2 | Updates: 32877 | ActorLoss: -0.1997 | CriticLoss:  0.8313 | GradNorm: 30.0163 | AdvFail:  0.0%:  33%|‚ñà‚ñà‚ñà‚ñé      | 333/1000 [30:15<1:00:36,  5.45s/vec-episode]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ 2 training videos available in videos/a2c_discrete_discrete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VecEp  432 | Score:   83.4 | AvgScore(50):  102.9 | Updates: 38981 | ActorLoss:  0.0662 | CriticLoss:  4.3905 | GradNorm: 64.3055 | AdvFail:  0.0%:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/1000 [34:52<45:51,  4.84s/vec-episode]      "
     ]
    }
   ],
   "source": [
    "# --- DISCRETE ACTION SPACE: A2C ---\n",
    "print(\"Starting A2C training with DISCRETE actions...\")\n",
    "\n",
    "discrete_a2c_scores, discrete_a2c_losses, discrete_a2c_agent = train_a2c(\n",
    "    is_continuous=False, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f20cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for discrete A2C using vectorized plotting\n",
    "from rl_utils import plot_vectorized_training_results, plot_vectorized_variance_analysis\n",
    "\n",
    "plot_vectorized_training_results(\n",
    "    discrete_a2c_scores, \n",
    "    discrete_a2c_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Discrete\", \n",
    "    algorithm_name=f\"A2C ({CONFIG['n_steps']}-step, {CONFIG['num_envs']} envs)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_vectorized_variance_analysis(\n",
    "    discrete_a2c_agent, \n",
    "    discrete_a2c_scores, \n",
    "    \"Discrete\", \n",
    "    CONFIG, \n",
    "    algorithm_name=f\"A2C ({CONFIG['n_steps']}-step, {CONFIG['num_envs']} envs)\"\n",
    ")\n",
    "\n",
    "# Show advantage normalization statistics\n",
    "adv_stats = discrete_a2c_agent.get_advantage_normalization_stats()\n",
    "print(f\"\\nüîç DISCRETE A2C ADVANTAGE NORMALIZATION ANALYSIS:\")\n",
    "print(f\"Fallback normalization rate: {adv_stats['fallback_rate_percent']:.2f}% ({adv_stats['fallback_normalizations']} of {adv_stats['total_actor_updates']})\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(discrete_a2c_scores))\n",
    "final_avg = np.mean(discrete_a2c_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ DISCRETE A2C TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} vectorized episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {discrete_a2c_agent.network.get_param_count():,}\")\n",
    "print(f\"Parallel environments: {CONFIG['num_envs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ba7f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONTINUOUS ACTION SPACE: A2C ---\n",
    "print(\"Starting A2C training with CONTINUOUS actions...\")\n",
    "\n",
    "continuous_a2c_scores, continuous_a2c_losses, continuous_a2c_agent = train_a2c(\n",
    "    is_continuous=True, \n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d453fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for continuous A2C using vectorized plotting\n",
    "plot_vectorized_training_results(\n",
    "    continuous_a2c_scores, \n",
    "    continuous_a2c_agent.loss_history, \n",
    "    CONFIG, \n",
    "    \"Continuous\", \n",
    "    algorithm_name=f\"A2C ({CONFIG['n_steps']}-step, {CONFIG['num_envs']} envs)\"\n",
    ")\n",
    "\n",
    "# Show variance analysis\n",
    "plot_vectorized_variance_analysis(\n",
    "    continuous_a2c_agent, \n",
    "    continuous_a2c_scores, \n",
    "    \"Continuous\", \n",
    "    CONFIG, \n",
    "    algorithm_name=f\"A2C ({CONFIG['n_steps']}-step, {CONFIG['num_envs']} envs)\"\n",
    ")\n",
    "\n",
    "# Show advantage normalization statistics\n",
    "adv_stats = continuous_a2c_agent.get_advantage_normalization_stats()\n",
    "print(f\"\\nüîç CONTINUOUS A2C ADVANTAGE NORMALIZATION ANALYSIS:\")\n",
    "print(f\"Fallback normalization rate: {adv_stats['fallback_rate_percent']:.2f}% ({adv_stats['fallback_normalizations']} of {adv_stats['total_actor_updates']})\")\n",
    "\n",
    "# Training completion message\n",
    "final_window_size = min(CONFIG[\"window_length\"], len(continuous_a2c_scores))\n",
    "final_avg = np.mean(continuous_a2c_scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "print(f\"\\n‚úÖ CONTINUOUS A2C TRAINING COMPLETED!\")\n",
    "print(f\"Final average score (last {final_window_size} vectorized episodes): {final_avg:.2f}\")\n",
    "print(f\"Network parameters: {continuous_a2c_agent.network.get_param_count():,}\")\n",
    "print(f\"Parallel environments: {CONFIG['num_envs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPREHENSIVE ANALYSIS: A2C Performance and Parallel Environment Benefits ---\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_utils.visualization import get_moving_average\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"COMPREHENSIVE ANALYSIS: A2C Performance and Parallel Environment Benefits\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Collect experiment results\n",
    "experiments = [\n",
    "    (\"Discrete A2C\", discrete_a2c_scores, discrete_a2c_agent),\n",
    "    (\"Continuous A2C\", continuous_a2c_scores, continuous_a2c_agent),\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä FINAL PERFORMANCE COMPARISON (last {CONFIG['window_length']} vectorized episodes):\")\n",
    "print(f\"{'Method':<20} {'Final Score':<12} {'Score Std':<10} {'Updates':<8} {'Up/VecEp':<8} {'AdvFail%':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for name, scores, agent in experiments:\n",
    "    final_window_size = min(CONFIG[\"window_length\"], len(scores))\n",
    "    final_score = np.mean(scores[-final_window_size:]) if final_window_size > 0 else 0.0\n",
    "    \n",
    "    stats = agent.get_variance_stats()\n",
    "    total_updates = getattr(agent, 'update_step', 0)\n",
    "    updates_per_vectorized_episode = total_updates / max(1, agent.vectorized_episodes) if hasattr(agent, 'vectorized_episodes') else 0.0\n",
    "    adv_stats = agent.get_advantage_normalization_stats()\n",
    "    failure_rate = adv_stats.get(\"fallback_rate_percent\", 0.0)\n",
    "    param_count = agent.network.get_param_count()\n",
    "    \n",
    "    print(f\"{name:<20} {final_score:<12.1f} {stats['score_std']:<10.1f} {total_updates:<8} {updates_per_vectorized_episode:<8.1f} {failure_rate:<10.1f} {param_count:<12,}\")\n",
    "\n",
    "print(f\"\\nüìà A2C-SPECIFIC ANALYSIS:\")\n",
    "for name, scores, agent in experiments:\n",
    "    stats = agent.get_variance_stats()\n",
    "    recent_score_var = stats.get('recent_score_variance', 0.0)\n",
    "    adv_stats = agent.get_advantage_normalization_stats()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Score variance (last {CONFIG['window_length']} vectorized episodes): {recent_score_var:.1f}\")\n",
    "    print(f\"  Advantage norm fallbacks: {adv_stats['fallback_normalizations']} of {adv_stats['total_actor_updates']} ({adv_stats['fallback_rate_percent']:.1f}%)\")\n",
    "    print(f\"  Parallel environments: {CONFIG['num_envs']}\")\n",
    "    print(f\"  N-step parameter: {CONFIG['n_steps']} steps\")\n",
    "\n",
    "# Create A2C-specific analysis plots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle(f'A2C ({CONFIG[\"n_steps\"]}-Step, {CONFIG[\"num_envs\"]} Envs): Performance Analysis', fontsize=16)\n",
    "\n",
    "colors = ['blue', 'red']\n",
    "smoothing_window = CONFIG[\"window_length\"]\n",
    "\n",
    "# 1. Performance comparison\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(scores) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(scores, window=smoothing_window)\n",
    "        episodes = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax1.plot(episodes, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax1.axhline(y=CONFIG[\"target_score\"], color='g', linestyle='--', label=f'Target ({CONFIG[\"target_score\"]})', alpha=0.7)\n",
    "ax1.set_xlabel('Vectorized Episode')\n",
    "ax1.set_ylabel(f'Score ({smoothing_window}-episode avg)')\n",
    "ax1.set_title('Performance Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Update frequency (gradient norms over update steps, not episodes)\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.gradient_norms) >= smoothing_window:\n",
    "        smoothed, offset = get_moving_average(agent.gradient_norms, window=smoothing_window)\n",
    "        update_steps = range(offset + 1, offset + 1 + len(smoothed))\n",
    "        ax2.plot(update_steps, smoothed, label=name, color=colors[i], linewidth=2)\n",
    "\n",
    "ax2.set_xlabel('Update Step')\n",
    "ax2.set_ylabel(f'Gradient Norm ({smoothing_window}-step avg)')\n",
    "ax2.set_title('Gradient Stability Over Updates')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss components over update steps\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.loss_history[\"total_loss\"]) >= smoothing_window:\n",
    "        # Plot total loss\n",
    "        smoothed_total, offset = get_moving_average(agent.loss_history[\"total_loss\"], window=smoothing_window)\n",
    "        update_steps = range(offset + 1, offset + 1 + len(smoothed_total))\n",
    "        ax3.plot(update_steps, smoothed_total, label=f'{name} Total', color=colors[i], linewidth=2)\n",
    "        \n",
    "        # Plot actor and critic losses with transparency\n",
    "        if len(agent.loss_history[\"actor_loss\"]) >= smoothing_window:\n",
    "            smoothed_actor, _ = get_moving_average(agent.loss_history[\"actor_loss\"], window=smoothing_window)\n",
    "            ax3.plot(update_steps, smoothed_actor, label=f'{name} Actor', color=colors[i], alpha=0.6, linestyle='--')\n",
    "        \n",
    "        if len(agent.loss_history[\"critic_loss\"]) >= smoothing_window:\n",
    "            smoothed_critic, _ = get_moving_average(agent.loss_history[\"critic_loss\"], window=smoothing_window)\n",
    "            ax3.plot(update_steps, smoothed_critic, label=f'{name} Critic', color=colors[i], alpha=0.6, linestyle=':')\n",
    "\n",
    "ax3.set_xlabel('Update Step')\n",
    "ax3.set_ylabel(f'Loss Value ({smoothing_window}-step avg)')\n",
    "ax3.set_title('Loss Components Over Updates')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Score variance over time - FIXED to use vectorized episodes\n",
    "for i, (name, scores, agent) in enumerate(experiments):\n",
    "    if len(agent.score_variance_history) > 0:\n",
    "        # Score variance is calculated per vectorized episode, starting from when we have enough data\n",
    "        variance_start_episode = CONFIG[\"window_length\"]  # Start from when we have window_length vectorized episodes\n",
    "        variance_episodes = range(variance_start_episode, variance_start_episode + len(agent.score_variance_history))\n",
    "        ax4.plot(variance_episodes, agent.score_variance_history, label=name, color=colors[i], alpha=0.7)\n",
    "\n",
    "ax4.set_xlabel('Vectorized Episode')\n",
    "ax4.set_ylabel(f'Score Variance (last {CONFIG[\"window_length\"]} vectorized episodes)')\n",
    "ax4.set_title('Score Variance Over Time')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
